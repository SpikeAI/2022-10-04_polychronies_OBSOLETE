% !TeX TS-program = pdfLaTeX
% !TeX encoding = UTF-8 Unicode
% !TeX spellcheck = en-US
% !BIB TS-program = bibtex
% -*- coding: UTF-8; -*-
% vim: set fenc=utf-8
%
% REV1
%
% https://www.overleaf.com/project/633bf90497b5b325a3ff692e
% https://framateam.org/spikeai/channels/review_polychrony
% https://www.zotero.org/groups/4562620/polychronies
%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\ResponseReviewerTwo}{
We would like to thank reviewer 2 for his very detailed response which were essential in providing this new revision. 

> The authors present a relatively thorough review on neural coding based on spike patterns in populations of neurons, from biological, theoretical and neuromorphic engineering perspective. I find the review mostly fine, with some omissions that, when corrected, should enhance it. 

Thanks for the encouraging comment, we hope that in its present form, the result is even more useful to the community.

> My main concern is the overall reliance on spike motifs as a coding mechanism. The authors should list alternatives and compare to spike motifs, at least cursory. A minor concern is the focus on mylenation-based delay learning, ignoring the obvious length-based delay adjustments.

We have more progressively commented different alternatives for coding mechanisms in the beginning of section 2, highlighting their relative benefits.

> I have commented heavily within the manuscript, and am attaching it for the authors to use. I tried to delineate wording suggestions from more significant critiques. I am also listing the bigger concerns below.

Many thanks for providing the PDF of the manuscript with annotations, this was immensely useful to annotate and improve our revision. To respond to all these points, we have constructed a tracked changes manuscript in which we respond (by a change, removal or addition) to the reviewer's comments.

> Specific issues:
> - not referencing much of insect work, with a few exceptions that are put in mammalian context. OK to do just timing in vertebrate systems (as birds are used), but should be stated clearly, and cited work should comport to that constraint (e.g., cite rat olfactory, not locust olfactory). Should the authors wish to include invertebrate work, there is a LOT more to review.

Indeed, our focus was implicitly on vertebrates but we cited some works about insect. As mentioned by the reviewer, adding researches done on insects would increase too much the size of the article. We decide to focus on vertebrates and mention this choice in the new manuscript.

> - some references to original research are missing, while often much older ones are used (and not reviews).
> - missing important lines of work on spike timing. In particular, Aurel Lazar's TEM work. 
> - missing important lines of work on dendritic processing: Mel, Spruston.

These critical references were indeed missing and we have added them now to the manuscript. Thanks for pointing out these omissions.

> - discounting axonal length as delay mechanism, focus on myelin effects too high.

This was corrected in section 4, providing a more balanced review.

> - singular focus on synfire chain/polychronicity-like mechanisms. Alternative options, like Bayesian spiking (Deneve), probabilistic ensemble motifs (Ballard) and probabilistic population coding (Pouget and Latham), which do not rely on exact motifs of spikes, are not discussed.

Indeed, we had made that choice to study precise temporal motifs. In the new manuscript, we mention these alternative options briefly even if we keep the focus on deterministic spiking motifs for this review.

> - there are also some discrepancies - are SNN-s differentiable or not.

We have corrected these discrepancies. Thanks again for all these useful comments.

}

% cite or die
% ❯ ipython analyze_bibs.py

% In recognition of your well-respected work, we are inviting you to submit one feature contribution (i.e., a long review or research paper) to an MDPI journal free of charge. If you wish to take advantage of this opportunity, please provide your information and the name of the journal you would like to submit to: https://www.surveymonkey.com/r/XQZ5FP9 (If you cannot find the journal through the link, please feel free to reply to us about your choice).

% The submission deadline for this special offer is 31 October 2022. The journal editorial office will contact you soon once you choose the journal. Please contact the revelant journal’s Editorial Office if you require an extension.


%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[brainsci, %arts,
               review,accept,pdftex,moreauthors
               %,  draft
               ]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.
%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2023}
\copyrightyear{2022}
%\externaleditor{Academic Editor: Firstname Lastname}
\datereceived{16 November 2022} 
\daterevised{20 December 2022 } % Only for the journal Acoustics
\dateaccepted{23 December 2022 } 
\datepublished{} 
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{siunitx}%The siunitx package provides a  set  of  tools  for  authors  to  typeset  quantities  in  aconsistent  way.
\newcommand{\ms}{\si{\milli\second}}%

% a simpler annotation system:
\usepackage{soulutf8}
\usepackage{color}
\newcommand{\mynote}[1]{{\sethlcolor{yellow}\hl{#1}}}
% \usepackage{trackchanges}
\usepackage[finalnew]{trackchanges}
\addeditor{MDPI}

%\newcommand{\note}[1]{}
%
%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Precise Spiking Motifs in Neurobiological and Neuromorphic~Data}
%
% MDPI internal command: Title for citation in the left column
\TitleCitation{Precise Spiking Motifs in Neurobiological and Neuromorphic Data}
%
% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0002-3107-4788} % AnG % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorB}{0000-0003-3916-0514} % AmG
\newcommand{\orcidauthorC}{0000-0002-0054-2729} % CB
\newcommand{\orcidauthorD}{0000-0002-9238-6654} % JN
\newcommand{\orcidauthorE}{0000-0001-8821-5556} % JM
\newcommand{\orcidauthorF}{0000-0002-9536-010X} % LP
%
% Authors, for the paper (add full first names)
\Author{Antoine Grimaldi $^{1}$% $^{1,\dagger,\ddagger}$
\orcidA{}, 
Amélie Gruel $^{2}$\orcidB{}, % $^{2,\dagger,\ddagger}$,
Camille Besnainou $^{1}$\orcidC{},
Jean-Nicolas Jérémie $^{1}$\orcidD{},
Jean Martinet $^{2}$\orcidE{}
and~Laurent U Perrinet $^{1,}$*\orcidF{}}
%
%\longauthorlist{yes}
%
% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Antoine Grimaldi, Amélie Gruel, Camille Besnainou, Jean-Nicolas Jérémie, Jean Martinet and Laurent U Perrinet}
%
% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Grimaldi, A.; Gruel, A.; Besnainou, C.; Jérémie, J.-N.; Martinet, J.; Perrinet, L.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.
%
% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad INT UMR 7289, Aix Marseille Univ, CNRS, 27 Bd Jean Moulin, 13005 Marseille, France; antoine.grimaldi@univ-amu.fr (A.G.); camille.besnainou@etu.univ-amu.fr (C.B.); jean-nicolas.jeremie@univ-amu.fr (J.-N.J.), laurent.perrinet@univ-amu.fr (L.U.P.)
 \\
$^{2}$ \quad SPARKS,  Côte d'Azur, CNRS, I3S, 2000 Rte des Lucioles,
06900 Sophia-Antipolis, France; amelie.gruel@i3s.unice.fr (A.G.); jean.martinet@univ-cotedazur.fr (J.M.)
}
%
% Contact information of the corresponding author
\corres{Correspondence: laurent.perrinet@univ-amu.fr}
%
% Current address and/or shared authorship
% \firstnote{Current address: Affiliation 3.} 
% \secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes
%
%\simplesumm{} % Simple summary
%
%\conference{} % An extended version of a conference paper
%
% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Why do neurons communicate through spikes? By definition, spikes are all-or-none neural events which occur at continuous times. In other words, spikes are on one side binary, existing or not without further details, and on the other, can occur at any asynchronous time, without the need for a centralized clock. This stands in stark contrast to the analog representation of values and the discretized timing classically used in digital processing and at the base of modern-day neural networks. As neural systems almost systematically use this so-called event-based representation in the living world, a better understanding of this phenomenon remains a fundamental challenge in neurobiology in order to better interpret the profusion of recorded data. With the growing need for intelligent embedded systems, it also emerges as a new computing paradigm to enable the efficient operation of a new class of sensors and event-based computers, called neuromorphic, which could enable significant gains in computation time and energy consumption---a major societal issue in the era of the digital economy and global warming. In this review paper, we provide evidence from biology, theory and engineering that the precise timing of spikes plays a crucial role in our understanding of the efficiency of neural networks.
}
%
% Keywords
\keyword{\textls[-15]{spikes; asynchronous computing; neurobiology; computational neuroscience; neuromorphic} engineering; heterogeneous delays; spiking motifs; polychronization} 
%
% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction: Importance of Precise Spike Timings in the Brain}\label{sec:time}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Is There a Neural Code?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONTEXT
Neural activity is directly influenced by our immediate environment and by internal states and is structured in order to generate motor actions. The efficiency of these actions is key for survival, which is the sole determinant in the light of natural selection. A central question of modern neuroscience is to better understand the essence of neural activity, as exemplified by the recordings observed in neurobiological experiments. One sometimes uses the expression ``decoding the neural code'', although this implies the existence of a code, i.e., an explicit representation \add[Antoine]{of cognitive processes} within the neural activity. Nevertheless, we will use this terminology in all generality to denote the existence of a structure in neural activity. In this respect, it is \change[Antoine]{known}{reasonable to declare} that neural activity may be related to specific measurable variables\change[Antoine]{, and due to Galvani's}{. Since Galvani's} experiments, we know that an electrical activity applied on muscular nerves can cause the stretching of a frog's limb (for a review, see~\citep{piccolino_luigi_1997}). A central and well-studied way of communication between neurons is specific electrochemical events called action potentials, or spikes, which were first discovered \change[Antoine]{by Lord Adrian}{at the beginning of the $XX^{th}$ century}~\citep{adrian_impulses_1926}. In this study, the frequency at which these spikes are emitted has been shown to be roughly commensurate with the stretch of the frog's limb.
In the scope of this article, we focus mainly on these spikes~\change[Antoine]{which are brief (about one millisecond) impulses that propagate along the axons of neurons. First, these have the particular property of being essentially binary in their amplitude, i.e., they are prototypical, ``all-or-none''}{in vertebrate systems. They can be described as brief (about one millisecond) and prototypical, i.e., ``all-or-none'', impulses that propagate along the axons of neurons.}
 \change[Antoine]{Typically, action potentials are received by a neuron from other afferent neurons on the arborized ``input'' dendrite, integrated in its soma, and possibly further propagated along its ``output'' axon to reach efferent neurons.}{Typically, a postsynaptic neuron receives incoming spikes from other afferent neurons on the arborized ``input'' dendrite. The integration of these spikes by the dendritic tree and the soma of the postsynaptic neuron results in the modification of its membrane potential that possibly leads to the emission of an action potential along its ``output'' axon to reach efferent neurons.} \add[Antoine]{Except notably in the retina where neurons communicate with graded potentials}~\citep{gouras_graded_1960}\add[Antoine]{, it is assumed that spike trains are the main component of the neural code.}
\remove[Antoine]{The frequency at which these spikes are emitted has been shown to be roughly commensurate with the stretch of the frog's limb(Adrian citation), and this variable (measured in spikes per second, or Hertz) is as of today considered to be the main constituent of the neural code.} \change[Antoine]{Indded, neurophysiologists typically}{Until recently, most neurophysiologists} used the temporal evolution of the firing rate (for instance, as computed as the average occurrence of spikes in small temporal windows of about $100~\ms$) in order to characterize the dynamical activity of neurons. This may be extended by computing different statistics on each neuron's sequence of spikes~\citep{perkel_neuronal_1967} but also the dependence across neurons~\citep{perkel_neuronal_1967-1}. 

% GAP
However, computational neuroscience models have suggested that the precise timing within a sequence of spikes may play a crucial role and that neurons \change[Antoine]{may be as well integrators than synchrony detectors}{may be synchrony detectors as well as integrators}~\citep{abeles_role_1982}. \add[Laurent]{In particular, it is possible that the minute arrangement of temporal delays between neurons may provide a computational advantage.} We will investigate this very hypothesis in this review. %Indeed, the response of one biological neuron largely depends on the precise timing of the sequence of incoming spikes that reach its soma~\citep{paugam-moisy_computing_2012}. 
In comparison to a classical \change[Antoine]{analogical}{analog} vector of inputs, this event-based representation observed in the neural code is essential in understanding information processing~\citep{carr_processing_1993}. For instance, it expands the capabilities of representations \change[Antoine]{(which rely on the firing rate)}{of the rate coding hypothesis that relies only on the firing rate} by considering \change[Antoine]{a representation based on repetitions of spiking motifs at precise times of occurrence.}{representations based on the precise timing of single spikes.} \remove[Antoine]{This hypothesis is directly inspired by neurobiological observations and expands the capabilities of analog representations (which are based on spiking firing rates) by considering a representation based on instances of spiking motifs at precise times of occurrence.} Additionally, numerous studies have demonstrated the importance of precise timing in neural population activity~\citep{davis_spontaneous_2021}, efficient encoding thanks to the use of spike latencies~\citep{perrinet_coding_2004,gollisch_rapid_2008} or precise timing in the auditory system~\citep{deweese_binary_2003,carr_circuit_1990}. All these findings, and more~\citep{bohte_evidence_2004,dilorenzo_spike_2013}, highlight the importance of the temporal aspect of the neural code and further suggest the existence of precise spatio-temporal spiking motifs in the input which excites neurons. A mathematical formalization would be particularly well-suited to neuromorphic computing~\citep{roy_towards_2019}, and would allow for the supervised or self-supervised learning of such motifs in any event-driven data.  Crucially, validating this hypothesis would also be crucial in our understanding of neural processes. %
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dynamics of Vision and Consequences on the Neural Code}%Precise timing in the neural code enables efficient vision}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Let us start with a focus on the state-of-the-art of the role of dynamics in vision. Broadly speaking, vision is the set of processes that allow us \change[Antoine]{to make sense of the luminous world}{to make sense of the world through luminous signals}, and is an intensively studied field in neuroscience, particularly with respect to deciphering the neural code. In most mammals, light enters the eye to induce neural activity on the retina, which maintains a certain similitude between the topology of external visual space and its representation on the retina, called retinotopy. The origins of this neuroscientific question can be found in the first experiments from Pierre Flourens which, using lesions in animals, demonstrated the relationship between visual sensations and activity in the cerebral cortex~\citep{flourens_recherches_1842,pearce_marie-jean-pierre_2009}. This was also observed \remove[Antoine]{by Lord Adrian} when recording the activity of the frog's visual system~\citep{adrian_impulses_1926}. In a series of seminal studies,~\citet{hubel_receptive_1968} showed that this activity could be selective to different features, such as the visual orientation or motion of elementary contours. For a large proportion of neurons, there is \change[Antoine]{remarkably a monotonic}{a remarkably monotonic} relationship between the contrast of visual features and the firing frequency of neurons. However, there is no consensus to explain the multiple nonlinear mechanisms that transform the visual scene into retinotopic neural activity maps, even though these processes seem to constitute essential pieces to this puzzle~\citep{carandini_normalization_2012}. 

%----------------------------%

%----------------------------%
In particular, there have been some remarkable findings when studying the dynamics of vision. For instance, Simon Thorpe's group has shown during the last decades numerous examples demonstrating that humans can categorize briefly presented images in a fraction of a second. \change[Antoine]{His}{Their} experiments consisted in asking subjects to categorize images that do or do not contain animals~\citep{thorpe_speed_1996}. The results showed that humans were able to perform this task very well (with a success rate of more than 95\%) but above all that, a differential activity for the two categories of images could be observed by electroencephalography, showing that this differentiation emerges with a very short latency in neural activity. These results have been extended to several species, including primates. Different experimental protocols have shown, for example, that the motor response could be extremely fast (of the order of $120~\ms$) when the task was to perform a saccade~\citep{kirchner_ultra-rapid_2006}. This fast processing correlates with the surprising experiments of fast serial detection, which consists in presenting a fast succession of different images and decoding via the EEG if the observer can detect, for example, the presence of an animal~\citep{keysers_speed_2001}. As expected, the performances decrease progressively as the frequency of presentation of the images increases. However, it has been shown in the macaque that a significant performance could be maintained with an image presentation time of only $14~\ms$ per image.

Although surprising, this speed of the visual cortex in primates is compatible with the latencies that are recorded at the neuro-physiological level. The rapid propagation of the visual information in the thalamus, then in the primary visual cortex takes about $45~\ms$ in the macaque~\citep{schmolesky_signal_1998} and about $60~\ms$ in humans~\citep{vanni_coinciding_2001}. This functioning of visual processing as a forward pass is most prominent in fast processing (see Figure~\ref{fig:thorpe}), and can be complemented with feedback loops from the higher areas to the sensory areas~\citep{lamme_distinct_2000}. An important consequence of this speed of processing of vision is that it implies that processing is carried out using only very few spikes per layer. As a comparison, the latencies in macaque monkeys are approximately as follows: Retina, 20--40~$\ms$; V1, 40--60~$\ms$; IT, 80--100~$\ms$; MC, 140--190~$\ms$; and to finger muscles, 180--260~$\ms$. \add[Laurent]{Note that, since maximal conduction speeds are roughly constant, theses latencies are comparable to that found in humans, with a ratio given by the physical size of the whole system.} It follows that if we consider that a behavioral response occurrs in only $200~\ms$, it would involve about ten processing stages along the ``forward'' pathways of the visual system. Such processes were indeed efficiently reproduced in feed-forward models trained with back-propagation~\citep{serre_feedforward_2007,jeremie_ultrafast_2022}. At the same time, it was demonstrated that one spike requires a significant amount of time (about $10~\ms$) to be conducted from one layer to the next~\citep{nowak_timing_1997,thorpe_seeking_2001}.  This figure is inspired by similar schematics performed for monkeys in~\citep{thorpe_seeking_2001}. As a consequence, these results suggest that ``like other senses, vision relies heavily on temporal strategies and temporal neural codes to extract and represent spatial information''~\citep{rucci_temporal_2018}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{How Precise Spike Timing May Encode Vectors of Real Values}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\add[Laurent]{Let us now focus on one processing step along a cortical pathway. }
% reliability of spike timings
Sensory data are most often represented by continuous values, such as the energy produced by a flow of photons that hits the different photoreceptors of the retina. How may such information be encoded in neuronal activity? The analysis of generic raster plots reveals particular traits that hint at the role of precise timing. For instance, the firing rate of cortical cells in awake monkeys is highly irregular~\citep{softky_highly_1993}, which makes it, at first sight, inconsistent with the temporal integration of firing rate. Remarkably, it was observed that the response of a neuron in a cortical slice to a current step could be highly non-reproducible: while the first spike is aligned to stimulus' time, the subsequent spike times tend to diffuse for independent repetitions of the stimulation~\citep{bryant_spike_1976,mainen_reliability_1995} (see Figure~\ref{fig:mainen}). However, if that same neuron is now driven by a \emph{frozen} noise, that is, a highly dynamic signal which is repeatedly replayed from trial to trial, then the output spikes are highly reproducible (for a review, see~\citep{ermentrout_reliability_2008}). This is consistent with the differential role of different stimulus frequencies (for instance, the gamma range around $80~\si{\Hz}$) on the reliability of the spike timing reported in~\citep{nowak_influence_1997}: ``we found that, as expected given the resistive and capacitive properties of cortical neurons, low frequencies have a larger effect on the membrane potential of cortical neurons than do higher frequencies. However, increasing the amount of gamma range fluctuations in a stimulus leads to more precise timing of action potentials.'' 

\begin{figure}[H]
%\centering
\includegraphics[width=.75\textwidth]{figures/visual-latency.pdf}
% thorpe
% \includegraphics[width=.7\textwidth]{figures/visual-latency-estimate.jpg}%-estimate.jpg}
\caption{
  Latency of the different processing steps along the human visual pathway.Though the visual system is highly inter-connected, one can follow the sequence of activations whenever an image (here a yellow star) is flashed in front of the eyes. Different areas are schematically represented by ellipses, and arrows denote the fastest feed-forward activation, ordered with respect to their activation latency in $\ms$. In order, the retina is first activated (20--40~$\ms$), then the thalamus and the primary visual cortex (V1, 60--90~$\ms$). This visual information  projects to the temporal lobe to reach the infero-temporal area (IT, 150~$\ms$) for object recognition. It then reaches the prefrontal cortex (PFC, $180~\ms$), which modulates decision making to evoke the motor cortex (MC, $220~\ms$) which may mediate an action. This is eventually relayed through the spinal cord to trigger finger muscles, with latencies of about 280--400~$\ms$.}\label{fig:thorpe}
\end{figure}
\vspace{-6pt}
\begin{figure}[H]
%\centering
\includegraphics[width=\textwidth]{figures/replicating_MainenSejnowski1995.pdf} 
\caption{
  Reproducibility of the spiking response of a neuron. 
The timing of the spikes produced following the repetition of a step stimulus is less reproducible than that to a noisy stimulus. The stimulus current value over time for a step stimulus (top left) and for a noisy one (top right). Trial repetitions of a leaky integrate-and-fire neuron stimulated by the stimulus on the upper row (middle row). Membrane potential is represented by dark blue color when light yellow colors when depolarized) and quantified by the average firing rate across trials (lower row). While this seems paradoxical at first sight, it highlights the consequence of using the same \emph{\hl{frozen}} noise at each repetition and highlights the highly reproducible pattern of spikes when it is driven by a highly dynamic input. See this~\href{https://github.com/laurentperrinet/2022_UE-neurosciences-computationnelles/blob/master/C_MainenSejnowski1995_Perrinet.ipynb}{\hl{notebook}} %MDPI: Please confirm if the link is unnecessary and can be removed. The following highlights are the same.
for a replication of the results from~\citep{mainen_reliability_1995} using a simple LIF model.}\label{fig:mainen}
\end{figure}

% retina
\remove[Antoine]{How may information about the sensory world be encoded in the spiking pattern?} At the level of the retina, it has been shown that a coding of luminance values in the image using the timing of the spikes may be at work~\citep{gollisch_rapid_2008}. In particular, these results show that the response of ganglion cells to the visual gratings projected on the retina could be encoded in the latency of the response and not only in the frequency of the discharge, as it is often assumed. These results have been extended to natural images and show a qualitatively similar behavior. The authors' conclusion is that the precise spiking latency of the neurons encodes the spatial features of the image. Interestingly, such a precise latency mechanism may underlie some visual illusions, e.g., the false color illusion in the \href{https://michaelbach.de/ot/col-Benham/index.html}{\hl{Benham Top}} based on center--surround interactions in the parvocellular pathway~\citep{kenyon_theory_2004}. This evidence found in the retina can be extended to other areas, such as the visual cortex.
%----------------------------%
% C_MainenSejnowski1995_Perrinet.ipynb

%----------------------------%

% %----------------------------%
% \begin{figure}
% \centering
% \mynote{redo a figure similar to the thorpe code}%\includegraphics{images/roc.jpg}
% \caption{Latency coding. An input analog profile is encoded in latencies: the higher the contrast, the shorter the latency. In this example, one generates at most one spike per neuron.
% }\label{fig:roc}
% \end{figure}
% %----------------------------%
In fact, similar results have been demonstrated through neurophysiological recordings in the primary visual cortex and show that different levels of visual activity will induce different levels of neuronal discharge latency in the primary visual area~\citep{celebrini_dynamics_1993}. First-spike latency codes are a feasible mechanism for information transfer, even when biologically plausible estimates of the stimulus onset are considered, for instance, for sound localization~\citep{chase_first-spike_2007}.  Note also that timing is not entirely sensory or internal, but can be used as a general neural coding principle. In~\citep{safaie_turning_2020}, for instance, they found that the ``timing accuracy was improved when the environment afforded cues that rats can incorporate into motor routines. Timing, at least in animals, may thus be fundamentally embodied and situated.''  Many models have used these properties in temporal coding to build fast image categorization networks~\citep{gautrais_rate_1998,delorme_spikenet_1999, perrinet_coding_2004}. These models take the form of artificial spiking neural networks (SNNs) and have been able to demonstrate their practical applications for image categorization~\citep{delorme_ultra-rapid_2000}. One of these is the SpikeNet algorithm, which uses a purely temporal approach by encoding information using one spike per neuron by using the rank of neurons' activation~\citep{delorme_spikenet_1999,bonilla_analyzing_2022}. Another class of artificial SNNs uses precise spike timing as a metric in order to determine the structure of the network in order to minimize a cost function. This was implemented in the SpikeProp algorithm~\citep{bohte_error-backpropagation_2002} and has been extended in novel gradient-based methods. The subsequent surrogate gradient method is now widely used in methods that attempt to transfer performance from analog (CNNs) to spike-based (SNNs) architectures~\citep{zenke_remarkable_2021}. This type of modeling often uses the classical task of categorizing images developed in deep learning, while adapting it to the specificity of the event-based representation~\citep{goltz_fast_2021}. For instance,~\citep{kheradpisheh_stdp-based_2018} implements a STDP-based spiking deep convolutional neural networks for object recognition or~\citep{tavanaei_representation_2018} develops a form of spike-based, competitive learning applied for unsupervised learning. 
However, the performance of SNNs is still lagging compared to that of analog networks, and the question of the advantage of using spikes in machine learning and computer vision remains open. \change[Antoine]{Efficiently implementing this type of modeling is extremely important with respect to the development of a new generation of event-based cameras}{Improvements in this new generation of Artificial Neural networks (ANN) would bring major advances in terms of efficient computations in machine learning. They would benefit in particular to a new generation of cameras called Silicon Retinas}~\citep{gallego_event-based_2022} (see Section~\ref{sec:neuromorphic}). \remove{These represent visual information as events, similarly to the biologically-inspired representation described above and in contrast with a basic frame-based representation}%~\citep{lichtsteiner_128x128_2008} (for a review, see~\citep{rasetto_challenges_2022}, this will be developed in Section~\ref{sec:neuromorphic}).This work has been extended to include unsupervised learning capabilities, and we have recently developed a SNN architecture that allows to categorize images of different classes in only a few spikes~\citep{grimaldi_homeostatic_2021,grimaldi_robust_2022}. 

\change[Antoine]{To alleviate this gap in computational neuroscience, we will further review the role of  precise spike timing in neural assemblies from a neuroscientific perspective in the next section.}{~Even if technology lags far behind biology, this introduction demonstrates the importance of timing in neural processes, and we will further review the role of precise spike timing in neural assemblies. We start by reviewing the different hypotheses that aim at deciphering the neural code with spatio-temporal spiking motifs. After listing some biological evidence for the use of precise spike timing, we review some computational models and neuromorphic technics that add this temporal dimension to their computations.}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Role of Precise Spike Timing in Neural Assemblies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\add[Antoine]{In this first section, we have introduced the notion of rate coding and demonstrated that spike timing can also carry information. There are multiple experimental evidences that neurons use different strategies to communicate.}~\cite{maunsell_functional_1983} \add[Antoine]{demonstrate that neurons of visual area MT in macaque monkeys encode motion direction through population activity, offering robustness to single neuron activity fluctuations. For rapid image processing, it has been observed that neurons in the salamander retina represent the spatial structure of an image with the relative timing of their first spikes}~\citep{gollisch_rapid_2008}\add[Antoine]{. In the primary visual area of macaque monkeys, the phase of firing convey information about naturalistic stimuli that cannot be described by firing rates alone}~\citep{montemurro_phase-of-firing_2008}\add[Antoine]{. Among a variety of strategies to communicate, so far we have also evidence that neurons can use correlation coding}~\citep{decharms_primary_1996}\add[Antoine]{ or sparse coding}~\citep{vinje_sparse_2000}. 
\add[Antoine]{In this first section, we introduced the notion of rate coding and demonstrated that spike timing can also carry information. Scientists found experimental evidence for various hypotheses of neural representations such as population coding}~\citep{maunsell_functional_1983}\add[Antoine]{, time-to-first-spike coding}~\citep{gollisch_rapid_2008}\add[Antoine]{, phase-of-firing coding}~\citep{montemurro_phase--firing_2008}\add[Antoine]{, correlation coding}~\citep{decharms_primary_1996}\add[Antoine]{ or sparse coding}~\citep{vinje_sparse_2000}\add[Antoine]{. In the scope of this review, we infer that spike trains are composed of repeating spiking motifs and we focus on precise spatio-temporal representations composed of a motif of spikes defined precisely in time and in the presynaptic address space. In all generality, this representation can encompass all the previous ones, except for rate coding, which is not defined locally in time. In this section, we choose to describe different hypotheses, making use of spatio-temporal patterns of spikes that can be propagated among neural assemblies.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{One First Hypothesis: Synchronous Firing in Cell Assemblies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
In his book \emph{Corticonics},~\citet{abeles_corticonics_1991} queried as to whether the role of cortical neurons is to integrate synaptic inputs or rather to detect coincidences in temporal spiking patterns. The book gradually leads the reader from the macroscopic cortical anatomy and standard electro-physiological properties of single neurons to neural network models. While the first hypothesis favors the rate coding theory, the second possibility highlights the need for temporal precision in the neural code~\citep{abeles_role_1982,paugam-moisy_computing_2012}. The book then demonstrates that neural assemblies could form so-called ``synfire chains,'' that is, showing the emergence of synchronous activity on subsets of neurons, which could be propagated in a stable fashion. More broadly, the idea of using the synchrony of co-activation in a cell assembly is reminiscent of the hypothesis that was formalized by~\citet{hebb_organization_1949}: \add[Antoine]{``cells that fire together wire together''}. Since this date, multiple experimental observations have suggested the existence of this precise zero-phase-lag spike synchronization in a defined subset of neurons~\citep{harris_organization_2003}. One possible function of this synchronization may serve the binding of information distributed in the brain~\citep{singer_visual_1995, roelfsema_visuomotor_1997}.

Some experimental results show the emergence of synchrony, for instance in motor cortical function~\citep{riehle_spike_1997}. Interestingly, these authors showed that ``accurate spike synchronization occurred in relation to external events (stimuli, movements) and was commonly accompanied by discharge rate modulations but without precise time locking of the spikes to these external events. Spike synchronization also occurred in relation to purely internal events (stimulus expectancy), where firing rate modulations were distinctly absent. These findings indicate that internally generated synchronization of individual spike discharges may subserve the cortical organization of cognitive motor processes.'' Moreover, such emergence could change over the learning period involved in learning a task~\citep{kilavik_long-term_2009} and showed some tuning to movement direction and reaction time~\citep{grammont_spike_2003}. It is important to note that synchronous events tend to lock to spatio-temporal patterns of neural activity called \href{https://en.wikipedia.org/wiki/Local_field_potential}{\hl{LFP}} beta waves~\citep{denker_lfp_2018} and were also extended to larger assemblies using statistical methods~\citep{torre_asset_2016} (see Section~\ref{sec:detection} for further details). \change[Antoine]{Note also that synchronicity may explain some unintuitive results. Indeed, it has been shown that thalamo-cortical synapses are relatively weak compared to the amount of intracortical activity. However, this pathway is sufficient to drive the cortex, as this input is more often synchronously active, allowing it to more efficiently drive cortical neurons}{Synchronicity is also an interesting proposition to explain how the relatively weak thalamo-cortical synapses are able to drive cortical neurons. Among different explanations, including travelling waves}~\citep{ben-yishai_traveling_1997}\add[Antoine]{, a synchronous activity at the synaptic level may be sufficient to elicit activity in the cortex}~\citep{bruno_cortex_2006}. 

Theoretically\add[Antoine]{, even if the vertebrate's neural system is not likely to be modeled only by such deterministic connectivity}~\citep{deneve_bayesian_2004, ballard_dual_2011}, it was shown that a simple model may allow the propagation of such synfire chains~\citep{gewaltig_propagation_2001} by considering the dynamics of leaky integrate-and-fire (LIF) neurons in different groups of a similar size. Each neuron of one group is connected by an excitatory synapse to the next. When a pulse is elicited in the first group, this may generate a spike in the next group. Depending on the concentration of synaptic weight values, this new activity may become more or less synchronized with respect to that of the previous pulse (as measured by the standard deviation of spike times within the pulse). Recursively applying this to a sequence of groups within a chain generates either a synfire propagation or not. A simple simulation of synfire propagation is shown in Figure~\ref{fig:diesman}. A crucial aspect of this emergence is explained by the dynamics of the spiking neuron model~\citep{gerstner_time_1995} and, in particular, the balance between excitation and inhibition~\citep{azouz_stimulus-selective_2008}. This balance was, for instance, modeled by feed-forward inhibition, a fine-scaled latency mechanism that is an essential ingredient in modelling so-called push--pull effects in the primary visual cortex~\citep{kremkow_push-pull_2016}. Further models have shown that such synfire chains could be embedded in topographies~\citep{aviel_embedding_2003}, while others used conductance-based neurons with feed-forward inhibition to improve the robustness of the propagation~\citep{kremkow_functional_2010}. In particular, this was implemented as a computational neuroscience benchmark model using the pyNN language~\citep{davison_pynn_2008} both in CPU-based and neuromorphic hardware~\citep{pfeil_six_2013}. % (see Figure~\ref{fig:pynn}).
%
%----------------------------%
%\begin{figure}
%\centering
%\includegraphics[width=.7\textwidth]{figures/fnins-07-00011-g004.jpg}
%\caption{A pyNN implementation of a Synfire chain with feedforward inhibition. The background noise  is implemented as random Gaussian current. The state space gives %the duration of synfire chains as a function of the protocol's parameters.}\label{fig:pynn}
%\end{figure}
%----------------------------%
%----------------------------%
\begin{figure}[H]
%\centering
\includegraphics[width=\textwidth]{figures/Diesmann_et_al_1999.pdf}
\caption{
  Simulation of a synfire propagation \href{https://brian2.readthedocs.io/en/stable/examples/frompapers.Diesmann_et_al_1999.html}{\hl{using Brian}}.} 
  The model consists of $10$ groups (arranged with the first group represented in the lowest row) of $100$ neurons each. Five pulses with decreasing jitter are generated in the first group around times $10$, $30$, $50$, $70$ and $90$~\ms~(with jitters given by a standard deviation which linearly decreases from $5$ to $1$~\ms). This generates a pulse after a certain processing delay in the second group with a different jitter. While the first two pulses progressively vanish in the following groups, starting from the third input pulse (with a jitter of $3~\ms$), it is propagated to the following groups. This allows the propagation of the synchronous activity along the chain of the neural groups.}\label{fig:diesman}
\end{figure}
%----------------------------%

Attempts have been made to detect such synfire chains in neurobiological data. \citet{schrader_detecting_2008} envisioned that ``sensitivity is high enough to detect synfire chain activity in simultaneous single-unit recordings of 100 to 200 neurons from such data, enabling application to experimental data in the near future.'' Indeed, simultaneously recorded activities of neurons in the primary motor cortex of monkeys exhibited context-dependent, rapid changes in the patterns of coincident action potentials~\citep{riehle_spike_1997,grammont_precise_1999}. It is now commonly accepted that the planning and execution of movements are based on distributed processing by neural populations in motor cortical areas, yet it is less clear \remove[Antoine]{, though,} how these populations organize dynamically to cope with the momentary computational demands.  In~\citep{brette_computing_2012}, the author proposed a simple spike-based computational framework, based on the idea that stimulus-induced synchrony can be used to extract sensory invariants (for example, the location of a sound source), which is a difficult task for classical ANNs. It relies on the simple remark that a serie of repeated coincidences is in itself an invariant. Many aspects of perception rely on extracting invariant features, such as the spatial location of a time-varying sound, the identity of an odor with fluctuating intensity, or the pitch of a musical note.

This is also expressed in the idea that different cortical areas could achieve binding by synchrony~\citep{fries_mechanism_2005}. The synchronicity will generate rhythms at different ranges of frequencies, with spikes arriving at peak susceptibility (top of a cycle) or down. Such a theory has surprisingly been validated in EEG recordings to explain for example the continuous wagon-wheel illusion, i.e., the perceived reversal of the rotational movements of the spikes in a rotating wheel~\citep{vanrullen_continuous_2006}. More generally, it can be shown that the phase of alpha oscillations (about $10$ Hz) is causally linked with modulations of cortical excitability and with visual perception~\citep{dugue_phase_2011}. The question still remains open as to know if this is an epiphenomenon or a working mechanism of the neural code.
%
\subsection{A Further Hypothesis: Travelling Waves}
%
To further investigate the role of precise timing, let us also focus on the role of differential timings in an assembly of neurons. As we have seen, a visual feature will induce the firing of different cells at different latencies~\citep{celebrini_dynamics_1993}. Using intracellular recordings, it was shown that the response to a focal visual activation would elicit a latency basin, that is, a graded onset of the neural response from the most activated to neighboring neurons~\citep{bringuier_horizontal_1999}. In particular, it was shown that the network of the so-called horizontal connections within a cortical area is typically not myelinated, and that this latency basin would be determined by the propagation speed within that area. For generic visual scenes, these processes would generate a complex interplay between the dynamics of the sensory signal and the spatio-temporal determinants of these interactions. Such interactions may underlay anticipatory mechanisms in the primary visual cortex~\citep{benvenuti_anticipatory_2020,le_bec_horizontal_2022}. The underlying process could be the emergence of propagating waves on the surface of the cortex.

Propagating waves in the neuronal response occur in many excitable media and were found in neural systems, for instance, in the retina\note[Antoine]{added ref to Feller}~\citep{feller_dynamic_1997} or the neocortex~\citep{bienenstock_model_1995}. While propagating waves are clearly present under anesthesia, whether they also appear during awake and conscious states remained unclear until recent discoveries. One possibility is that these waves were systematically missed in trial-averaged data, due to variability. A recent work~\citep{muller_stimulus-evoked_2014} presents a method for detecting propagating waves in noisy multichannel recordings. Applying this method to single-trial voltage-sensitive dye imaging data\note{The reviewer is right to point that calcium imaging may not be the best tool to capture precise spiking motif as both spatial and temporal resolution are not that great. We argue that few imaging techniques can capture the precise spike location, even good spike sorting algorithms. Also, here we study a propagation of activity defining a spatio-temporal pattern that may not have the same precision as some spiking motifs we describe later.}, the authors show that the stimulus-evoked population response in the primary visual cortex of the awake monkey propagates as a travelling wave, with consistent dynamics across trials. A network model suggests that this reliability is the hallmark of the horizontal fiber network of superficial cortical layers. Propagating waves with similar properties occur independently in the secondary visual cortex, but maintain precise phase relations with the waves in the primary visual cortex. These results show that, in response to a visual stimulus, propagating waves are systematically evoked in several visual areas, generating a consistent spatio-temporal frame for further neural interactions.

% \begin{figure}
% \centering
% %\includegraphics[width=1\textwidth,height=\textheight]{https://media.springernature.com/lw685/springer-static/image/art\%3A10.1038\%2Fs41467-021-26175-1/MediaObjects/41467_2021_26175_Fig1_HTML.png}
% \caption{a Spike raster for repeated presentations (%N = 40) of a high-contrast (10\% Michelson contrast) drifting Gabor recorded from area MT of a fixating marmoset (stimulus-onset, red line; mean response, blue line). A single-trial LFP trace is plotted in gray, and the average LFP response is plotted in black. The relative power between baseline (−200 ms to stimulus-onset) and evoked fluctuations (stimulus-onset + 50--250 ms) significantly favored the evoked response (right panel; N = 110 trials; median = 1.89 dB, p = 0.000019, two-tailed Wilcoxon's ranked-sum test). b Same as in (a), but for a low contrast stimulus (2\% Michelson contrast). The relative power between baseline and evoked LFP fluctuations was not statistically different from parity (median = 1.23 dB, p = 0.087 two-tailed Wilcoxon's ranked-sum test). c An example of spontaneous LFP fluctuations structured as a travelling wave recorded from a spatially distributed multielectrode array in marmoset area MT. d Histogram of spontaneous spike probability as a function of the generalized phase of the LFP during fixation. e The average evoked response to low contrast stimuli was stronger when a more excitable phase (+ / - \pi rad) of a spontaneous travelling wave aligned with the retinotopic location of the target (aligned, green line) as compared to when a less excitable phase (0 rad) was aligned (unaligned, purple line; N = 43 wave and non-wave trials; shaded region SEM; p = 0.0000015 two-tailed Wilcoxon's rank-sum test). Data for panels c--e modified from Davis et al.17 with permission.
% }\label{fig:davis}
% \end{figure}

More recently, novel multi-unit recording techniques have enabled the identification of travelling waves of neural activity in different areas of the cortex~\citep{muller_cortical_2018}. The authors reviewed these findings by considering the mechanisms by which travelling waves are generated, and evaluated their possible roles in cortical function. In particular, spontaneous travelling waves naturally emerge from horizontal fiber time delays and travel through locally asynchronous-irregular states~\citep{davis_spontaneous_2021}. Studies of sensory-evoked neuronal responses often focus on mean spike rates, with fluctuations treated as internally generated noise. However, fluctuations of spontaneous activity, often organized as travelling waves, shape stimulus-evoked responses and perceptual sensitivity. The mechanisms underlying these waves are unknown. Further, it is unclear whether waves are consistent with the low rate and weakly correlated ``asynchronous-irregular'' dynamics observed in cortical recordings. In that paper, the authors describe a large-scale computational model with topographically organized connectivity and conduction delays relevant to biological scales. They find that spontaneous travelling waves are a general property of these networks. The travelling waves that occur in the model are sparse, with only a small fraction of neurons participating in any individual wave. Consequently, they do not induce measurable spike correlations and remain consistent with locally asynchronous irregular states. Further, by modulating the state of the local network, they can shape responses to incoming inputs as observed {\hl{in vivo}}. %MDPI: we remove italic, please confirm. The following highlights are the same.
Such waves also occur in motor areas and~\citet{linden_movement_2022} have recently presented ensemble recordings of neurons in the lumbar spinal cord that indicate that, rather than alternating, the population is performing a low-dimensional ``rotation'' in neural space, in which the neural activity is cycling through all phases continuously during the rhythmic~behavior.

Interestingly, it can be shown that these travelling waves could have a measurable impact on the activity of the visual cortex. This was illustrated in a recent study investigating the long-range apparent motion effect (lrAM)~\citep{chemla_suppressive_2019}. The lrAM is the simple phenomenon of perceiving a smooth motion when showing two dots in a temporal sequence and in relatively close visual proximity. The lrAM is the core building block underlying the use of sequences of images to induce the perception of smooth, realistic visual scenes, which is at the base of movies seen in cinema theaters. In this study, the authors used voltage-sensitive dye imaging to record the activity of the primary visual cortex of macaque monkeys to the presentation of the pair of dots presented independently or in conjunction. A probabilistic modelling showed that the activity of the joint presentation induced a suppressive wave in the direction opposed to the perceived direction, shaping the formation of a wave of propagation travelling at a speed compatible with the perceived motion. A computational model validated the hypothesis that this process could be mediated by diffusion in the horizontal layers connecting the different locations within this cortical area. In summary, the study by~\citet{chemla_suppressive_2019} gave a multi-disciplinary account to demonstrate the effect of travelling waves in the visual cortex.
%
% Robust computation with rhythmic spike patterns.~Proceedings of the National Academy of Sciences of the United States of America~116(36), 18050 - 18059.~ https://dx.doi.org/10.1073/pnas.1902653116
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Rediscovered Hypothesis: Precise Spiking Motifs in Cell Assemblies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Travelling waves indicate that spatio-temporal correlations could play an important role in shaping neural activity. For instance, statistical dependencies in the responses of sensory neurons govern both the amount of stimulus information conveyed and the means by which downstream neurons can extract it. In particular, this was put in evidence by analyzing the functional significance of correlated firing in a complete population of macaque parasol retinal ganglion cells using a model of multi-neuron spike responses~\citep{pillow_spatio-temporal_2008}, which shows the precise spatio-temporal differences in this recurrently connected assembly. The different aspects of information in the data are evaluated by a decoding strategy, highlighting the role of correlations. Note that a similar dataset used in~\citep{schneidman_weak_2006} is available from Michael Berry's lab~\citep{berry_spike_2022} and allows \change[Antoine]{performing and validate}{testing in order to validate or falsify} these hypotheses. However, \remove[Antoine]{a limit of explaining such data with travelling waves is that} in theory, a \add[Antoine]{cortical} travelling wave would be stationary, which is incompatible with the limits in space and time of a neural system. Recent observations may suggest that neural groups or ensembles, rather than individual neurons, are emergent functional units of cortical activity. ~\citet{miller_visual_2014} showed that whereas intrinsic ensembles recur at random time intervals, visually evoked ensembles are time-locked to stimuli. Experiments are performed using two-photon calcium imaging of populations of neurons from the primary visual cortex of awake mice during visual stimulation and spontaneous activity. The study proposes that visual stimuli recruit endogenously generated ensembles to represent visual attributes. Note that evoked ensembles in response to a natural movie played in a loop were precisely timed across repetitions. % (Fig. 7).

From another viewpoint, there is a substantial literature in neurobiology indicating that brain dynamics often organize into stereotyped sequences, such as synfire chains~\citep{ikegaya_synfire_2004}, packets~\citep{luczak_sequential_2007} or hippocampal sequences~\citep{pastalkova_internally_2008}. %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Explictly decoding spiking motifs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%We have previously seen that neurobiologists have described stereotyped sequences of neural activation, notably in the hippocampus~\citep{pastalkova_internally_2008,villette_internally_2015,malvache_awake_2016}, and that neurons can be activated in the same order across days~\citep{haimerl_internal_2019}. 
Going further, researchers found precise repetitions of spontaneous patterns of synaptic inputs in neocortical neurons, {\hl{in vivo} %MDPI: We removed the italics. Please confirm this revision.
} and {\hl{in vitro}}. These patterns repeat after minutes, maintaining millisecond accuracy. Indeed,~\citet{ikegaya_synfire_2004} demonstrated that in cortical activity, one can find a repetition of several motifs in spike activity (duration around 1 s +/$-$ 0.5 s, some events in motifs are of similar size but sometimes absent). These sequences can be specific to a particular layer or column, can be synchronized with network activity oscillation, and can involve several cells. They also demonstrated that these sequences can form super sequences, so-called \emph{\hl{cortical songs}}. It consists of the assembly of several sequences which repeat in a specific order with a compressed timing. {\hl{In vivo}} spontaneous activity also reveals repeating sequences: about 3000 sequences, each involving 3--10 cells out (of about 900), and lasting up to 3 s. Sequences have specific topographic structures, in some cases involving only a particular layer or a vertical column of cells or cells located in a cluster, and are associated with a structured spatial organization of the neurons that formed them. %In cortical songs, there is a ``compressing timing'' which may be taken into account by a similar mechanism as maxpooling in CNNs for space, but in time. Or there may be a mechanism for controlling the replay speed (pulvinar, \ldots{} , ?)
% %----------------------------%
% \begin{figure}
% \centering
% \includegraphics[width=.6\linewidth]{figures/Ikegaya2004zse0150424620001.jpeg}%pcbi.1006283.g001.PNG_L.png}
% \caption{Fig. 1. from~\citep{ikegaya_synfire_2004} repeated motifs of spontaneous synaptic activity \textit{in vitro} and \textit{in vivo}. (A) Repeated motifs of intracellular activity from layer 5 pyramidal neurons in slices. Panels show segments (red) of the same voltage-clamp recording from the same cell, repeating seconds or minutes after the initial occurrence (blue). Arrows indicate timings of repeated PSCs. (i) Upper trace: low--temporal resolution display of spontaneous activity of a neuron. Lower traces: higher resolution display of the repeated motif at indicated regions of the trace (a to c). (ii) Example of a longer motif. (B) Three repetitions of a motif. The top traces show the motifs superimposed on each other (blue, green, and red), the middle traces show these same traces individually, and the bottom traces show temporally magnified regions of the motifs (a to c). (C) Repeated sequences of intracellular current-clamp recordings \textit{in vivo}. Two (i) and three (ii) repetitions of motifs are shown. Shuffle tests were performed on traces (i), a to c, yielding significantly fewer repeats (fig.~S2, P  0.02). In (i), the blue trace is shifted --2.75 mV; in (ii), the blue trace is shifted --1.58 mV, and the green +0.79 mV.}\label{fig:Ikegaya2004}
% \end{figure}
% %----------------------------%

Additional studies detail the role of such precise spike timing in downstream information transfer and coding~\citep{villette_internally_2015,branco_dendritic_2010,luczak_packet-based_2015}. This is, for instance, relevant in sensory pathways in vision~\citep{meister_concerted_1995}, audition~\citep{decharms_primary_1996}, olfaction~\citep{cleland_construction_2014, kashiwadani_synchronized_1999, rinberg_speed-accuracy_2006} or touch~\citep{johansson_first_2004}.  In particular, stereotyped sequences of neural activation have been described in the adult hippocampus and related to its function in mental travel in time and space~\citep{buzsaki_space_2018}. These sequences can be internally generated~\citep{pastalkova_internally_2008,villette_internally_2015} and may be formed by the chained activation of orthogonal assemblies, themselves organized as sequence packets~\citep{malvache_awake_2016}. In that protocol, hippocampal sequences are formed by the ordered activation of smaller sequence motifs. They are stereotyped and robust since neurons can be activated in the same order across days (see Figure~\ref{fig:haimerl} from~\citep{haimerl_internal_2019}). As a consequence, hippocampal sequences may rely on an internally hardwired structure and form the functional building blocks for encoding, storing and retrieving experience.
%
%----------------------------%
\begin{figure}[H]
%\centering
\includegraphics[width=0.98\linewidth]{figures/haimerl2019.jpg}
\caption{
        An example of a precise temporal motifs seen in cortical slices. 
        In this study by~\citep{haimerl_internal_2019}, an analysis of the raster plot shows repetitions of precise spiking motifs with a time scale of the order of seconds. (\textbf{A}) Calcium fluorescence (heatmap) of CA1 neurons participating to run sequences in consecutive imaging sessions. Cells were selected and ordered with respect to their activity in the first imaging session (Top). The black line represents the speed of the mouse (modified from Figure 1 from~\citep{haimerl_internal_2019} under the CC-BY \href{https://www.pnas.org/doi/full/10.1073/pnas.1718518116}{\hl{PNAS License}}).}\label{fig:haimerl}  %MDPI: Please add the explanation for B--D in the figure.

\end{figure}
%----------------------------%

It is interesting to make a parallel with the ``Rapid Formation of Robust Auditory Memories'' reported in~\citep{agus_rapid_2010}, which uses noise patterns to observe if listeners could learn to detect repeated occurrences of some frozen noise patterns. In particular, they used random waveforms to probe the formation of new memories for arbitrary complex sounds. A behavioral measure was designed, based on the detection of repetitions embedded in noises up to $4~ \si{\second}$ long. %The task is to detect the repetition of the same (frozen) noise within a trial.
Unbeknownst to listeners, some noise samples reoccurred randomly throughout an experimental block. They showed that the ``repeated exposure induced learning for otherwise totally unpredictable and meaningless sounds'' by showing that the sensitivity increases in that case. Note also that ``acoustical analyses failed to reveal any obvious differences between good and bad noises'' and that ``time reversal had no significant effect on the detection accuracy (which is quite surprising). The learning is unsupervised (statistical, automatic), fast-acting (phase transition, ``insight''), and long-lasting (memorization). 

That results suggest that precise spiking motifs are not necessarily grouped on a topography and can be forming apparently randomly arranged connections. Interestingly, one theoretical viewpoint considers synfire braids~\citep{bienenstock_model_1995}, where a precise sequential motif of spikes will synchronize as it reaches the soma of a neuron for which synaptic delays are adequately tuned. Furthermore, computational modeling shows that at the scale of neurons, an efficient neural code can emerge where spike times are organized in prototypical, precise temporal motifs~\citep{izhikevich_polychronization_2006}, which this author defined as \emph{\hl{polychronous groups}}. The rest of this review will be devoted to present evidence for the use of such precise spiking motifs in computational neuroscience, neurobiology and neuromorphic engineering. As a summary, it seems that such precise structural information is essential to the neural code and that it seems imperative to include this information in the decoding algorithm for a better understanding of neural activity.
%
% TODO: re-read~\citep{luczak_packet-based_2015} Luczak A, McNaughton BL, Harris KD. Packet-based communication in the cortex. Nat Rev Neurosci. 2015;16(12):745--55.
% \subsubsection{Neural Variability and Sampling-Based Probabilistic Representations in the Visual Cortex~\citep{orban_neural_2016}}
% \begin{itemize}
%  \item
%   Stochastic sampling links perceptual uncertainty to neural response variability
% \item
%   Model accounts for independent changes in strength and variability of responses
% \item
%   Model predicts relationship between noise, signal, and spontaneous correlations
% \item
%   Stimulus statistics dependence of response statistics is explained
% \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Understanding Precise Spiking Motifs in Neurobiology}\label{sec:detection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decoding Neural Activity from Firing Rates}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we will review current evidence on how we may take advantage of spiking motifs in neurobiology, that is, in an effort to understand actual recordings from biological neural tissues. In most generic computational models, the neural activity is assumed to be encoded in the firing rate. For instance, the output of the so-called linear non-linear (L-NL) models is assumed to model the response of a biological neuron as the sequence of a linear integration followed by a non-linear spiking response generating spikes according to a Poisson point process\note[Laurent]{adding reference}~\citep{simoncelli_characterization_2003}. As such, a simple decoding strategy is to infer the input knowing the neuron's tuning curves, that is, its selectivity to a range of features~\citep{jazayeri_optimal_2006} or simply by a simple regression~\citep{berens_fast_2012}. This latter model assumes a Bernoulli model for the generation of spikes, such that the decoding amounts to a single-layer logistic regression. An important perspective of these methods used to decipher the recorded activity is that it could be ultimately used to fit neural network models to the recorded activity~\citep{bellec_fitting_2021}. In this particular paper, the authors fit the summary statistics of neural data with a (differentiable) spiking network model. The loss function is the cross entropy (following the Bernouilli hypothesis with a GLM, where each unit is modelled with an SRM neuron~\citep{gerstner_time_1995}) and embedded with recurrent dynamics. In particular, it comes with \href{https://github.com/EPFL-LCN/pub-bellec-wang-2021-sample-and-measure}{\hl{code}} and uses the \note[Laurent]{now citing the URL}publicly available V1 dataset~\citep{kohn_utah_2016}, which allows supervising the model, with the input being the movie and the output the spikes recorded. These type of model may infer sparse activity in a set of binary latent variables, each describing the activity of a cell assembly~\citep{warner_probabilistic_2022}.
\change[Antoine]{Using the right metric, as implemented in that paper by the corresponding neural models, is essential to better understanding neural data.}{Carefully picking the more appropriate metric, as implemented in that paper by the corresponding neural models, is essential in better understanding neural data.} Importantly, these models are dependent on a core definition of spike measures, and we will review here how precise spiking motifs are taken into account by such spike distances.
%
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Decoding Neural Activity Using Spike Distances}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
There are different solutions to provide with a distance between two given spike trains. A known measure is the Victor--Purpura distance, which overcomes inconsistencies experienced with a \remove[JN]{simple} firing rate (Poisson model) of spike trains~\citep{victor_nature_1996}. \change[JN]{In a later study, Van Rossum (2001) defined a distance that uses a time constant as a parameter}{Then a study tries to solve the problem by including a time constant as a parameter}~\citep{van_rossum_novel_2001}. \change[JN]{Depending on this parameter, the distance interpolates between a coincidence detector and a rate difference counter.}{This parameter will then be used to interpolate the distance between a coincidence detector and a rate difference counter.} Such distances were extended to non-Euclidean metrics and use morphological manipulations to compute spike train dissimilarity~\citep{kreuz_measuring_2007}. Mathematically, the stability of distance measures induced by level-crossing sampling can be evaluated~\citep{moser_stability_2014}, notably in light of the so-called Weyl's discrepancy measure~\citep{weyl_ueber_1916}, which may lead to the definition of a cross-correlation measure\change[JN]{which}{, an interesting conclusion since the cross-correlation measure is} that which is adapted to the event-based nature of spiking signals. These observations lead to the intuition that each distance may be as good as the optimal solution of a generative model for these measures, possibly through non-linear relations~\citep{aronov_non-euclidean_2004}. % ~\citep{} 

%Let us now focus on distances defined based on a generative model formed by using precise spiking motifs. 
Concerning spike timings,~\citet{levakova_review_2015} \change[JN]{reviewed the methods for neuronal response latency estimation exists and includes Bayesian binning}{reviewed existing methods for estimating the latency of neural responses that include Bayesian binning.} \change[JN]{More broadly }{Alternatively }, unitary event analysis can be performed by a statistical model of coincidence detection~\citep{grun_unitary_2002-1}. This was extensively used in detecting above chance significant synchronous patterns~\citep{grun_unitary_2010}, particularly in recordings of pairs of neurons (see~\citep{riehle_spike_1997} for instance), and has been extended to non-stationary data~\citep{grun_unitary_2002}. A method to detect significant patterns of synchronous spiking in a subset of massively parallel spike trains in the presence of background activity can be defined using the statistical evaluation of synchronous spike patterns extracted by frequent item set mining~\citep{torre_statistical_2013}. By the same group, the SPADE, CAD or ASSET algorithms are methods for identification of spike patterns in massively parallel spike trains (the spiking activity of tens to hundred(s) of neurons recorded in parallel) by identifying fine temporal correlations in the$~\ms$ precision range~\citep{quaglio_methods_2018}. This was recently extended in~\citep{stella_3d-spade_2019} in order to find re-occurring patterns in parallel spike train data, and to determine their statistical significance. The extension improves the performance in the presence of patterns with different durations, as demonstrated by application to various synthetic data, \add[Laurent]{such as the synthetic data for synfire chains (see Figure}~\ref{fig:Diesmann_et_al_1999_spade}), such as surrogates generated to evaluate precisely timed higher-order spike correlations~\citep{stella_comparing_2022}.

\begin{figure}[H]
%\centering
\includegraphics[width=\linewidth]{figures/Diesmann_et_al_1999_spade.pdf}
\caption{
  Detecting motifs using SPADE. 
  We used the SPADE algorithm~\citep{stella_3d-spade_2019} by adapting their \href{https://elephant.readthedocs.io/en/latest/tutorials/spade.html}{\hl{tutorial}} on the data generated in  Figure~\ref{fig:diesman}. This allowed to label different precise spike motifs which are denoted by different colors. Spikes belonging to the same motif have the same color. %As the synfire chain gets more precise, the detected patterns grow in size.
}\label{fig:Diesmann_et_al_1999_spade}
\end{figure}

Another important algorithm, called SPOTDisClust, is based on the detection of structured temporal patterns~\citep{grossberger_unsupervised_2018}. They introduced an unsupervised method based on their detection from high-dimensional neural ensembles. The algorithm measures similarity between two ensemble spike patterns by determining the minimum transport cost of transforming their corresponding normalized cross-correlation matrices into each other. Many approaches to this problem are supervised. In other words, they take patterns occurring concurrently with a known event, such as the delivery of a stimulus for sensory neurons or the traversal of a running track for determining hippocampal place fields, as a ``template'' and then search for repetitions of the same template in spiking activity~\citep{nadasdy_replay_1999,lee_combinatorial_2004}. In SPOTDisClust, the learning is unsupervised. It uses the prior that there is only one spike per pattern. Using a so-called t-SNE projection (that allows to project this high-dimensional representation to a lower-dimension map) validated that this clustering method can retrieve all patterns from the data. % (see Figure~\ref{fig:SPOTDisClust}). 
The limits of this method are that it is computationally complex, block-based and strictly specialized for the task at hand. To overcome these difficulties, a novel method was recently developed~\citep{sotomayor-gomez_spikeship_2021}.
%   Davidson TJ, Kloosterman F, Wilson MA. Hippocampal replay of extended experience. Neuron. 2009;63(4):497--507. pmid:19709631
\add[JN]{Whether it is the distance between two given spike trains or a comparison of the spike timings, the complexity and the diversity of the methods used to measure them are witnesses of the growing interest of the integration of these measures in the understanding of the neural code. One of the steps to test their potential usefulness is then to scale these methods to larger amounts of data.} 

%----------------------------%

%----------------------------%

% %%----------------------------%
% \remove[LP]{
% \begin{figure}
% \centering
% \includegraphics[width=\linewidth]{figures/pcbi.1006283.g001.png}%pcbi.1006283.g001.PNG_L.png}
% \caption{\textbf{The SPOTDisClust algorithm.} Fig 1 of~\citep{grossberger_unsupervised_2018}, showing simulated example illustrating the steps in the algorihtm. \textbf{(A)} Structure of five ``ground-truth'' patterns (\ldots). For each pattern and each neuron, a random position was chosen for the activation pulse. \textbf{(B)} Neuronal output is generated according to an inhomogeneous Poisson process, with rates dictated by the patterns in (A). (© Authors under a \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006283}{CC licence})}\label{fig:SPOTDisClust}
% \end{figure}
% %----------------------------%
% }
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scaling Up to Very Large Scale Data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Over the past decade, tremendous technological advances across several disciplines have dramatically expanded the frontiers of experimentally accessible neuroscientific facts. Bridging across different spatial and temporal scales, combination of {in vivo} two photon imaging, large population recording-array technologies, optogenetic circuit control tools, transgenic manipulations as well as large volume circuit reconstructions are now used to examine the function, structure and dynamics of neural networks at an unprecedented level of detail and precision. The daunting complexity of the biological reality revealed by these technologies highlights the importance of neurobiological knowledge to provide a conceptual bridge between abstract principles of brain function and their biological implementations within neural circuits. As a consequence, there is a growing need to scale these methods to larger amounts of data. 

There are multiple \change[Antoine]{solutions}{approaches} which aim at tackling this problem. One algorithm capable of achieving such a daunting task is the Rastermap algorithm~\citep{pachitariu_robustness_2018}. Basically, it rearranges neurons in the raster plot based on the similarity of their activity and applies a deconvolution strategy based on a linear model. \add[Laurent]{Yet this method was mainly tested on calcium imaging data, which are known to add some imprecision to the timing of the original spiking activity.} The model is \note[Laurent]{now citing the URL}openly accessible~\citep{stringer_mouselandrastermap_2020} and has led to important discoveries. In~\citep{stringer_spontaneous_2019} for instance, it was shown that a neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Then, in~\citep{stringer_high-precision_2021}, the authors analyzed spontaneous neural firing, finding that neurons in the primary visual cortex encoded both visual information and motor activity related to facial movements. In~\citep{russo_cell_2017}, the authors developed novel machine learning tools and statistical tests for unsupervised spatio-temporal pattern detection in non-stationary environments, which were applied to simultaneous electro-physiological recordings from tens to hundreds of neurons for decoding cognitive processes from neural activity. Altogether, this provides evidence for the importance of such machine-learning-based tools to provide with breakthroughs in~neuroscience.
%   Data availability: All of the processed deconvolved calcium traces are available on \href{https://figshare.com/articles/Recordings_of_ten_thousand_neurons_in_visual_cortex_in_response_to_2_800_natural_images/6845348}{figshare}, together with the image stimuli.

% \subsection{Extending the decoding at multiple time scales}
In the paper by~\citet{russo_cell_2017}, the authors present a unifying methodological and conceptual framework which detects assembly structure at many different time scales, levels of precision, and with arbitrary internal organization. It uses  sliding window as in~\citep{grun_unitary_2002} and the reliable and efficient analysis of an excess or deficiency of joint-spike events~\citep{pipa_neuroxidence_2008}. They extend the measure to multiple lags~\citep{torre_synchronous_2016}. The core measure is based on a non-stationarity corrected parametric statistical test for assessing the independence of pairs and an agglomerative, heuristic clustering algorithm for fusing significant pairs into higher-order assemblies.  To overcome the limits of models which require spike times to be discretized, utilize a suboptimal least-squares criterion, or do not provide uncertainty estimates for model predictions or estimated parameters,~\citep{williams_point_2020} addressed each of these shortcomings by developing a point process model that characterizes fine-scale sequences at the level of individual spikes and represents sequence occurrences as a few marked events in continuous time. \change[Laurent]{They also introduce}{As originally introduced by}~\citep{kass_statistical_2005}, they used learnable time-warping parameters to model sequences of varying duration, which were experimentally observed in neural circuits, and demonstrated these advantages on experimental recordings from the songbird higher vocal center and rodent hippocampus.
At a larger scale, in~\citep{van_kempen_top-down_2021}, it was shown that attentional information from V4 or arousal can change the timings of groups of events in V1. They develop a hidden Markov model for quantifying the transitions. In particular, they showed that fluctuations in neural excitability are coordinated between visual areas with retinotopic precision. Top-down attention drives inter-areal coordination along the reverse cortical hierarchy, predicting better behavioral performance with increased coordination. Building such models for predicting changes of timings based on context, such as using a so-called change point model for blocked-based experimental protocols~\citep{pasturel_humans_2020}, could therefore provide useful prior information to enhance the decoding from neural activity.
% sparse in time and space : AL Barth and JF Poulet Trends in Neurosciences 35.6 (2012), pp.~345-355.  CC Petersen and S Crochet, Neuron 78.1 (2013), pp.~28-48.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{What Biological Mechanism Could Allow Learning Spiking Motifs?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Despite the evidence for the effectiveness of precise spiking detection we presented above, doubts may remain as to the reliability of this learning mechanism and whether there is a real need for further research on this subject. The discovery of the existence of an equivalent biological mechanism in vertebrates as well as the demonstration of the importance of its role in various developed behaviors allow us to put these doubts to rest. In the following paragraphs, we will successively describe the first biological observations of delay learning, identify myelinization as an important actor, and finally study this phenomenon at the molecular level. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Biological Observations of Delay Adaptation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First biological observations

One of the first significant pieces of evidence of any neuronal delay in the information propagation within the animal neural system came from Hermann von Helmholtz's study of a frog's sciatic nerve in 1850~\citep{von_helmholz_messungen_1850,peyrard_how_2020}, and was later confirmed with Young's study of the squid giant axon~\citep{young_functioning_1938}. Dendritic propagation delays vary from sub-milliseconds to a few milliseconds, while axonal propagation delays range from a few milliseconds to tens of milliseconds, depending on the neuronal population studied~\citep{madadi_asl_dendritic_2018}. Extensive measures of nerve conduction velocities were performed in different animals, including humans, and significant variations related to age, sex and other factors were identified~\citep{stetson_effects_1992}.

However, it was not until the study of the interaural time difference (ITD) by \remove[Amélie]{gerstnerneuronal1996}~\citet{carr_circuit_1990} that it was discovered that this delay is not homogeneous for all neurons of the same type and species, but adapts according to their function. This ITD is a biological mechanism which allows for the azimuthal localization of sound by barn owls, by organically computing the difference in arrival time of a sound between their two ears. \add[Amélie]{It was first theorized in the Jeffress ``ITD-versus-place model''}~\citep{jeffress_place_1948}. \add[Amélie]{As hypothesized by this model, the \textit{\hl{nucleus laminaris}} of the avian brain contains coincidence detectors and, associated with the \textit{\hl{nucleus magnocellularis}} axons, forms circuits for processing ITD}~\citep{konishi_coding_2003}. According to~\citep{gerstner_neuronal_1996}, there is a true paradox in auditory neural systems since ``neural networks encode behaviorally relevant signals in the range of a few $\mu s$ with neurons that are at least one order of magnitude slower'' and therefore necessarily need to play on the response time to do so. \add[Amélie]{This assertion confused the mean interspike interval, i.e., how often a neuron can fire, and the specific spike time, i.e., how precisely a spike can be generated. However, it has nevertheless contributed to the recognition of the importance of time in various biological mechanisms.} \remove[Amélie]{penacochlear2001 conducted an extended study on ITD in order to confirm this theory by identifying the coincidence detector involved, which the authors situated in the nuclear laminaris. In the mouse somatosensory cortex, camontiming2019 found a strong correlation between the delay of the mouse behavioral response and the timing of multiunit activity evoked by a trained whisker.} A first hypothesis suggested that the sound coincidence was detected using stereausis, i.e., the temporal disparity between the left and right cochlear loci in the owl's brain. However, it was quickly set aside, as the predictions did not match the measured disparities in the loci, and no variation was perceived in the nuclear laminaris for a similar sound intensity in both owl ears. The authors supported a second hypothesis, that of different axonal delays in the ipsi- and contra-lateral cochlear nucleus magnocellularis.~\citet{seidl_mechanisms_2010} experimentally seconds this hypothesis of a ``coarse'' regulation of delay, as the authors concluded that regulations at different sites within individual axons of at least two parameters, namely, the axon diameter and internode distances, might be responsible for precise adjustments of physiological delays, thus allowing the ITD detection. \add[Amélie]{The authors also noted that the barn owl's axons seem to change in length, thus implementing a ``pure delay line''.}

\add[Amélie]{The experiments described above thus conclude on the important role of physiological delays in the avian sound localization behaviour. However, the relevance of precise timing in spikes is not limited to birds; for example, in the mouse somatosensory cortex,}~\citep{camon_timing_2019} \add[Amélie]{found a strong correlation between the delay of the mouse behavioral response and the timing of multiunit activity evoked by a trained whisker.} These experiments also confirmed previous studies stating that the conduction velocity of a spike in a neuron (in other words, its delay) depends strongly on the axon diameter~\citep{gasser_axon_1939} and the internode distance between Ranvier nodes~\citep{brill_conduction_1977}\change[Amélie]{--- instead of solely the axon's length as was previously thought.}{. This mechanism adds to the axonal length delay, which was previously thought to be the sole influence on the conduction velocity due to its anatomical soundness---as}~\citet{seidl_mechanisms_2010}\add[Amélie]{ experimentally demonstrated, this mechanism by itself is not sufficient to explain the biological functionality but should be added to the one of axonal length delay.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Importance of Myelination}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The importance of myelination
\citet{gasser_axon_1939} experimentally confirmed with homogeneously selected neurons that the axonal delay is positively proportional to \change[Amélie]{both the axon diameter by itself and the diameter of the axon wrapped in myelin}{the axon diameter, i.e., the amount of myelin wrapped around the axon}. Indeed, the oligodendrocytes, one of the many glial cells present in the vertebrates' nervous system identified in 1924 by Pío del Rigo Hortega~\citep{perez-cerda_pio_2015}, produces thin protein sheets interspersed with lipid layers wrapped concentrically around the axon, called myelin~\citep{schmitt_ultrastructure_1939}. Myelinization consists in ``two motions: the wrapping of the leading edge of the inner tongue around the axon underneath the previously deposited membrane and the lateral extension of myelin membrane layers toward the nodal regions''~\citep{simons_oligodendrocytes_2016}. Multiple myelin regions can appear on one neuron axon and form the following subdomains: ``the internode (corresponding to the compacted region of myelin), the paranodes (where the outer loops of the myelin contact the axon), the juxtaparanode (the interface between the paranode and compact myelin, rich in potassium channels) and the node of Ranvier (the approx $1~\upmu$m gap between adjacent myelin internodes [allowing for] the saltatory conduction)''~\citep{duncan_neuron-oligodendrocyte_2021}. \change[Antoine]{In}{On} average, each oligodendrocyte produces 20 to 60 myelinization processes and each myelin sheath is 20 to 200 $\upmu$m long~\citep{simons_oligodendrocytes_2016}. This demonstrates an additional impact of myelinization on the conduction velocity, as the number of segments is positively correlated to the axonal delay~\citep{brill_conduction_1977}. Thus, in several occasions, myelin has been identified as \change[Antoine]{a main}{an important} actor in the regulation of conduction velocity in neurons, i.e., axonal delay regulation.

% role of myelination and neuronal delay
\citet{fields_new_2015, fields_myelin_2020} state that myelin facilitates both the neural circuit function and the behavioral performance: experiments on mammals show that myelinization is activity-dependent and directly related to learning and memory consolidation, especially sensory or motor training and in enriched environments. This biological phenomenon takes place both at an early age, where the amount of oligodendrocites is particularly high in the central nervous system~\citep{reynolds_study_1928}, and in older animals, due to its involvement in coupling the activity of distant neuron populations. Myelination helps memory consolidation by coupling the activity of distant neuron populations and generating nearly synchronous responses in postsynaptic neurons involved among others in path integration~\citep{madadi_asl_dendritic_2018}, as was experimentally demonstrated on mice using a Morris water maze~\citep{steadman_disruption_2020}, contextual fear conditioning~\citep{pan_preservation_2020} or oligodendrocyte precursor cells (OPCs) knock-out~\citep{wan_impaired_2020}. Myelin also inhibits axon sprouting and synapse formation, especially in pyramidal neurons~\citep{fields_new_2015}, and is involved in axonal energy saving through a reduced axonal capacitance and a shift of the metabolic load from axons onto oligodendrocytes~\citep{duncan_neuron-oligodendrocyte_2021}.

It is worth highlighting that myelinization becomes increasingly important in larger brains where conductance delays are substantial and brainwave rhythms are critical; synchrony errors can lead to neuropsychiatric and neurological dysfunctions~\citep{fields_new_2015}, such as Parkinson's disease, epilepsy or multiple sclerosis~\citep{madadi_asl_dendritic_2018}. Additionally, a recent study suggests that demyelination of the optic nerve could be an underlying factor in glaucoma~\citep{xue_demyelination_2021}. ~\citet{duncan_neuron-oligodendrocyte_2021} states that ``the loss of myelin and oligodendrocytes fundamentally alters the neuron, [which are then] susceptible to energetic failure [and] subsequent degeneration''.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interplay of Delay Adaptation and Neural Activity}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How is the neuronal activity detected ?
However, one question remains: how do oligodendrocytes detect neuronal activity and regulate the myelinization accordingly? To answer this, we must first study the myelinization process. The OPCs first proliferate in the white matter \textit{via} a self-repulsive process, thus allowing for an evenly spaced network, and identify target axons. Most OPCs then differentiate into oligodendrocytes and immediately initiate myelinization, with no further migration~\citep{simons_oligodendrocytes_2016}. Not much is known about how oligodendrocytes select the axons to myelinate, but it seems that myelination only takes place on large enough axons and is strongly regulated by several factors~\citep{kuhn_oligodendrocytes_2019}, such as Ca$^{2+}$ activity~\citep{baraban_ca2_2018} of the neuregulin 1 growth factor~\citep{nave_axonal_2006}. The important role of myelination on delay learning and biological behaviors, as highlighted in the previous paragraph, suggests that the identification of target neurons as well as the myelin production is also regulated by neuronal activity. Indeed,~\citet{cullen_periaxonal_2021} experimentally demonstrated that learning and associated neuronal activity modify the Ranvier nodes' length and the periaxonal space width in the adult mouse brain. They also confirmed that the delay correlates with the level of skill acquisition.~\citet{gibson_neuronal_2014} suggests that neuronal activity does not solely promote adaptive myelination in the mammalian brain, but also OPC differentiation and oligodendrogenesis. Some further studies show that oligodendrocytes may detect neuronal activity thanks to growth factors or neurotransmitters released through ion channels or {\hl{via}} exocytosis, but does not require any axo-glial synaptic communication~\citep{fields_new_2015}. 

A potential scenario for selective myelination on electrically active axons using non-synaptic junctions between an axon and an oligodendrocyte is as follows: the axon releases glutamate in the extracellular environment by vesicle fusion, which activate the oligodendrocyte's NMDA and metabotropic glutamate receptors. This triggers the axo-glial signalling complex, involving the phosphorylation of the SRC family kinase FYN followed by the translation of heterogeneous nuclear ribonucleoprotein A2 into local myelin basic proteins~\citep{fields_new_2015}.

% myelination for axonal delay, but what about dendritic delay ? 
The previous paragraphs present the biological mechanisms behind the axonal delay, regulated by myelination. However, the dendritic delay \add[Amélie]{as well as the axonal delay (see}~\citep{seidl_mechanisms_2010}) seems to play an equally important role in the precise timing within a sequence of spikes. Dendritic delay is involved in the performance, structure and function of the nervous system, the modulation of spatio-temporal properties of pre- and post-synaptic activity patterns and the functional limitations of sensory feedback control efficiency~\citep{madadi_asl_dendritic_2018}. Its role has been specifically identified in the compensation of input asynchrony in the mammalian auditory brain stem~\citep{spencer_compensation_2018}. \citet{mel_synaptic_2017} \add[Amélie]{highlighted the dendrites' impact on neuronal plasticity, which is caused by the wide variation of numerous parameters: plasticity rules applied to different dendritic subtrees or dendritic subregions, local passive cable properties, distance travelled by remote dendritic inputs, branching structures, dendritic diameters, the relative timing of back-propagating somatic action potentials, etc. Dendritic spiking involvement in synaptic potentiation following active backpropagation into dendrites was experimentally uncovered using calcium imaging to highlight dendritic calcium entry allowing for long-tem potentiation}~\citep{golding_dendritic_2002}. ~\citet{branco_dendritic_2010} demonstrated the dendritic sensitivity to a sequence of synaptic activation in cortical pyramid neurons, encoded by ``both local dendritic calcium signals and somatic depolarization, leading to sequence-selective spike output''. The dendritic mechanism described can identify patterns delivered to a single dendrite or randomly distributed across the dendritic tree and relies on the dendritic calcium influx moderation by NMDA receptors.

All in all, learning spike motifs requires significantly complex pathways and biological mechanisms, whether in the dendrites or the axon of the neuron. More and more is becoming known about the non-trivial research topic that is biological delay learning, and extensive experimental data help develop ANNs, whose learning rules would be more neuromorphic. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Modeling Precise Spiking Motifs in Theoretical and Computational Neuroscience}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now that we have reviewed biological foundations for the role of delays in neural computations, we review, in the following section, theoretical models which directly take advantage of using precise spiking motifs. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Spiking neural networks (SNNs)~\citep{maass_networks_1997} are natural candidates to use these precise temporal patterns in the brain. The approach which is currently most prominent in the SNN community is to use existing algorithms from machine learning and to adapt them to the specificity of spiking architectures~\citep{goltz_fast_2021}. One such example is to adapt the successes of deep learning algorithms and to transfer the back-propagation algorithm to SNNs, as it is the most widely used to tune the weights of a classical (non-spiking) neural network. In a nutshell, it considers the system as implementing an input / output function and iteratively updates the weights according to the direction and magnitude of the error's gradient. \change[Antoine]{Since the activation function of a spiking neuron is not differentiable,}{In deep learning, the gradient is computed on the activation function and since spikes are not differentiable,} a recent popular approach consists in using a surrogate gradient~\citep{neftci_surrogate_2019} to ``cross-compile'' a classical neural network to a spiking architecture~\citep{rueckauer_conversion_2017}. SNNs reach in some case a similar performance as their non-spiking equivalent, for instance on the \remove[Antoine]{N-}MNIST dataset for categorizing digits in a stream of events~\citep{susi_nmnsd-spiking_2021}. \change[Antoine]{The main reason for adopting this approach instead of classical architectures is the possibility to benefit from dedicated neuromorphic low-energy hardware. 
However, most biological neural systems are obviously more efficient than current state-of-the-art vision systems, both in terms of efficiency (accuracy), in speed (latency), and energy consumption.}{So far, this approach does not outperform classical architectures both in term of training efficiency and performances}~\citep{davies_loihi_2018}\add[Antoine]{. However, they remain the best candidates to reproduce biological neural systems and their capacities in terms of accuracy, speed and energy consumption.} There is, therefore, an immense gap in the way we understand biology to translate it to the efficiency of SNNs. To go beyond the state-of-the-art, we will focus here on one core computation of a spiking neuron, that is, its ability to switch from the classical integrator mode (summing analog currents on its \change[Antoine]{synapses}{dendrites}) \change[Antoine]{to a synchrony detector where it emits a spike whenever presynaptic spiking inputs are synchronized}{to a detector of precise spiking motifs}~\citep{paugam-moisy_computing_2012}. In particular, we will explore different existing architectures which are able to overcome the diversity of input presynaptic patterns and learn to detect stable spiking motifs, that is, volleys of spikes which are stable up to a certain onset time (see Figure~\ref{fig:THC}). These models will be compared in light of neuroscientific and computational perspectives. %We review in this section theoretical and computational foundations of spiking motif detection. %In particular, our objective is to describe methods which fully exploit the capacity of spiking neurons to detect synchronous patterns.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Izhikevich's Polychronization Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% polychronization
As we saw, most SNN, and in particular those adapted from analogous deep-learning-like architectures, rely on an encoding of information based on a continuously varying firing rate. Notable exceptions of SNNs using precise spike timings are the \add[Antoine]{time-encoding machine by}~\citet{lazar_time_2004} and \textit{\hl{polychronization}} model of~\citet{izhikevich_polychronization_2006}. \add[Antoine]{In this section, we focus on the polychronization model} based on a random recurrent model of spiking neurons, including synaptic delays chosen from a range of biologically realistic delays (from $0$ to $20~\ms$) and whose weights evolved with a spike-time-dependent plasticity (STDP) learning rule~\citep{markram_regulation_1997}. \change[Antoine]{It is consensus that spike timing (STDP) plays a crucial role in the development of synaptic efficacy for many different kinds of neurons}{It was shown that spike timing (STDP) has an impact on the development of synaptic efficacy for many kinds of neurons}~\citep{caporale_spike_2008}. Delays are defined as the total time taken for a spike to be conducted from one presynaptic neuron's soma to the efferent postsynaptic neuron's soma. It is worth mentioning that only the weights are changed using the STDP rule and that the set of delays is set randomly at initialization and that delays are then ``frozen'' for the rest of the simulation. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity, i.e.,~exhibit reproducible time-locked firing patterns which  the author defined as ``polychronous groups'' (PGs). One core ingredient of this model is the fact that the neurons composing a group fire at different times, but due to the heterogeneous delays, the spikes reach the postsynaptic neuron at the same time. This synchrony of arrival at the soma of the neuron leads to the summation of the excitatory post-synaptic potentials evoked by each spike, and thus to the crossing of the voltage threshold and to the discharge of a spike (see Figure~\ref{fig:izhikevich}). According to the STDP rule, the group of neurons involved in this polychronous activity will see their synaptic weight increase and, thus, may consolidate the formation of a polychronous group. 
%----------------------------%
%

% 
%----------------------------%

% follow-ups
Interestingly, the paper by~\citep{izhikevich_polychronization_2006} stirred a lively debate in the field of computational neuroscience, with a general positive acceptance, but relatively few works extended this seminal paper. Indeed, there were already existing models of synaptic delay learning in spiking neural networks, see for instance~\citep{huning_synaptic_1998} or~\citep{eurich_dynamics_1999}, yet they had not shown potential applications to the detection of spiking motifs. A popular model for the detection of latency patterns is the \emph{tempotron}~\citep{gutig_tempotron_2006}, particularly reviewed in~\citep{gutig_spike_2014}. The \emph{tempotron} is a supervised synaptic learning algorithm, which classifies a distractor from a target motif, in order to extend the perceptron, which does not incorporate a spike timing framework. The \emph{tempotron} learning rule is derived by an optimization process and takes the form of a supervised STDP rule. The limits of this model are that its output is only binary and that its storage capacities are limited. An extension of~\citep{izhikevich_polychronization_2006} was made in a very detailed work aiming at reproducing the polychronization model~\citep{pauli_reproducing_2018}. Indeed, while the original paper contained material within the text to reproduce the whole model (using MATLAB), it was not complete such as to allow for the reproduction of all results presented in that manuscript. This more recent work details how this code could be slightly corrected. It comes with a Python code and a version control system detailing the whole process used to give provenance to the different steps in this scientific process. Another recent work gives a Bayesian account in a similar model~\citep{guise_bayesian_2014}. In that work, based on the fact that previous methods for studying polychronous groups' activation response to stimuli have been limited by the template-based methods used to identify PG activation, the authors outline a new method that overcomes these difficulties by establishing a probabilistic interpretation of PG activation. They demonstrate the use of this method by investigating the claim that PGs might provide the foundation of a representational system. Stimulation of the trained network produces the activation of a PG, i.e., the propagation of firing activity through multiple layers due to convergent patterns of firing. While extending the original method, these methods reveal shortcomings that we will try to analyze in the rest of this~section.

Strikingly, thanks to the fact that a neuron can be involved in different polychronous groups, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system (see Figure~\ref{fig:THC}). In other neuronal models, an efficient use or detection of these spatio-temporal patterns embedded in the spike train comes with the integration of heterogeneous delays~\citep{guise_bayesian_2014,zhang_supervised_2020}. The recent ``multi-neuronal spike sequence detector'' architecture integrates the weight- and delay-adjustment methods by combining plasticity with the modulation of spike latency emission~\citep{susi_nmnsd-spiking_2021}. Additional models for the detection of latency patterns are presented in the extensive (graph-centric) review on synchronization in time-varying networks~\citep{ghosh_synchronization_2021,ghosh_synchronized_2022}. This representation has potentially a much greater information capacity in comparison to other neural coding approaches through their connectivity and the possible coexistence of numerous superposed PGs~\citep{izhikevich_polychronous_2009}. Recently, by using a logistic regression model coupled with a temporal convolution, a model with heterogeneous delays was implemented to test the detection of the spiking motifs embedded in an event stream~\citep{grimaldi_learning_2022}. This allowed to detect a high number of superposed polychronous motifs in synthetic data, illustrating the computational benefit of such representations compared to that with a unique delay (see Figure~\ref{fig:THC}). As such, these models use the neural dynamics to handle input signals with different delays but do not explicitly take full advantage of the representation capacity offered by heterogeneous delays. 

\begin{figure}[H]%[t!][]
%  \centering
  \includegraphics[width=0.490\linewidth]{figures/THC_1a_k.pdf}
  \includegraphics[width=0.490\linewidth]{figures/THC_1c.pdf}
  \includegraphics[width=0.490\linewidth]{figures/THC_1b.pdf}
  \includegraphics[width=0.490\linewidth]{figures/THC_1a.pdf}
    \caption{
      Detecting event-based motifs using spiking neurons with heterogeneous delays.
    { (\textbf{a})}~Given a generic raster plot defined by a set of spikes occurring on specific addresses and at specific times, one may consider that this information consists of the repeated occurrence of groups of precise motifs of spikes that define ``polychronous groups'' (PGs). 
    { (\textbf{b})}~The generative model is defined by this set of motifs (here $4$ PGs) each defined by  synapses defined by weights (increasing with the radius of the black dots) with heterogeneous delay. 
    { (\textbf{c})}~Generalizing the core polychrony detection model (see Figure~\ref{fig:izhikevich}), one can define a layer of neurons that detect the identity and timing of these PGs~\citep{grimaldi_learning_2022}. % such that the addresses are the synapses of these neurons (which we therefore call here synapses).
    %Note that each afferent may be connected with multiple weights at different delays. The propagation of the afferent information through these delay may generate at each time step a synchronous motif on a subset of synapses. %These PG motifs were efficiently learned by a logistic regression.
    %{\bf (c)}~The output of the model provides with the predicted probability of occurrence of each PG motif at any time, which may be used to generate a spike as a Bernoulli trial, providing in this particular case with an exact identification of PGs occurrences.
    { (\textbf{d})}~\hl{Knowing the} %MDPI: Please change the (a) into (d) in picture.
 results of this detection, one may for illustration purposes highlight them by different colors in the raster plots, showing that in this synthetic example, all spikes are now associated with a PG. 
     }
  \label{fig:THC} % TODO: citer biol cyb fig:THC
\end{figure}

\begin{figure}[H]%[t!]
%  \centering
  \includegraphics[width=0.980\linewidth]{figures/izhikevich.pdf}%png}% https://www.overleaf.com/5625872443qpcwrkssgbsf
    \caption{
      Core mechanism of polychrony detection~\citep{izhikevich_polychronization_2006}.
       {(\textbf{Left})}~In this example, three presynaptic neurons denoted \textit{b}, \textit{c} and, \textit{d} are fully connected to two post-synaptic neurons \textit{a} and \textit{e}, with different delays of respectively $1$, $5$, and $9~\ms$ for \textit{a} and  $8$, $5$, and $1~\ms$ for \textit{e}. {(\textbf{Middle})}~If three synchronous pulses are emitted from presynaptic neurons, this will generate post-synaptic potentials that will reach \textit{a} and \textit{e} asynchronously because of the heterogeneous delays, and they may not be sufficient to reach the membrane threshold in either of the post-synaptic neurons; therefore, no spike will be emitted, as this is not sufficient to reach the membrane threshold of the post synaptic neuron, so no output spike is emitted.
    %at these different delays, and these may not be sufficient to generate a spike in either neuron.
    {(\textbf{Right})}~If the pulses are emitted from presynaptic neurons such that, taking into account the delays, they reach the post-synaptic neuron \textit{a} at the same time (here, at $t=10~\ms$),  the post-synaptic potentials evoked by the three pre-synaptic neurons sum up, causing the voltage threshold to be crossed and thus to the emission of an output spike (red color), while none is emitted from post-synaptic neuron \textit{e}.
     }
  \label{fig:izhikevich}
\end{figure}
%
%----------------------------%
%

% 
%----------------------------%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Learning Synaptic Delays}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Training delays ~\citep{state_training_2019}
First, the original model by Izhikevich uses a simple STDP rule while a whole range of STDP-based learning rules may implement precise spiking motifs detection. For instance, to address how transmission delays and STDP can jointly determine these emergent pairwise activity--connectivity patterns, a recent study analyzed phase synchronization properties and coupling symmetry between two bidirectionally coupled neurons using both phase oscillator and conductance-based neuron models~\citep{madadi_asl_delay-dependent_2022}. Moreover, modified STDP rules have been used for synchronous coherence detection~\citep{perrinet_coherence_2002}, for the learning of specific receptive fields~\citep{perrinet_networks_2001}. They were also extended to recurrent neuronal networks~\citep{gilson_stdp_2010} or delay selection~\citep{datadien_right_2011}. In particular, this has been applied for recurrent networks of spiking neurons receiving oscillatory inputs~\citep{kerr_delay_2013} which targets for the selective potentiation of recurrent connections with different axonal and dendritic delays during oscillatory activity. More generally, our ability to track and respond to rapidly changing visual stimuli, such as a fast-moving tennis ball, indicates that the brain is capable of extrapolating the trajectory of a moving object to predict its current position, despite the delays that result from neural transmission. Specifically, the neural circuits underlying this ability can be learned through spike-timing-dependent synaptic plasticity, and these circuits emerge spontaneously and without supervision, demonstrating how the neural transmission delays can, in part, be compensated to implement the extrapolation mechanisms required to predict where a moving object is at the present moment~\citep{burkitt_predictive_2021}.

At the implementation level, a recent work proposed a bio-plausible unsupervised delay learning for extracting temporal features in spiking neural networks~\citep{nadafian_bio-plausible_2020}. The authors provided some mathematical proofs to show that their learning rule gives the ability to learn repeating spatio-temporal patterns. Applying this STDP-based rule on delays to the spiking neural network, the experimental results were validated on a simple motion detection task, but were prone to convergence issues. Another model of synaptic delay-weight plasticity integrates synaptic delay plasticity into supervised learning and proposes a novel learning method that adjusts both the synaptic delays and weights of the learning neurons to make them fire precisely timed spikes~\citep{zhang_supervised_2020}. This was also presented by~\citep{wang_delay_2019}, who proposed a supervised delay learning algorithm for spiking neurons with temporal encoding, in which both the weight and delay of a synaptic connection can be adjusted to enhance the learning performance. Other models, such as that of~\citep{hazan_memory_2022}, propose a weightless spiking neural networks that can perform a simple classification task which is applied to MNIST. In a recent paper~\citep{luo_supervised_2022}, the authors proposed a gradient descent-based learning algorithm for synaptic delays to enhance the sequential learning performance of a single spiking neuron. In this algorithm, information is encoded in the relative timing of individual neuronal spikes, and learning is performed based on the exact derivatives of the postsynaptic spike times with respect to presynaptic spike times.
In yet another computational model,~\citet{sun_learning_2016} showed that the frequently activated polychronous neural groups can be learned efficiently by readout neurons with joint weight-delay spike-timing-dependent plasticity.

%In~\citep{duffy_variation_2019}, authors present a model to ``show a way by which the nervous system maintains precise, stereotyped behavior in the face of environmental and neural changes.'' It is shown in birdsong generation that ``A precise, temporally sparse sequence from the premotor nucleus HVC is crucial to the performance of song in songbirds''~\citep{suthers_motor_2002,wild_descending_1993,yu_temporal_1996} and this model shows how one could vary HVC activity using something similar to dropout in ML. Using such controlled variability, ``behaviors are made more robust to environmental change by continually seeking subtly new ways of performing the same task.'' Not sure however how important it is that the HVC pattern should be sparse (and similar to PGs). In~\citep{agus_rapid_2010}, there are ``good'' and ``bad'' noises show that some patterns are more easy to disentangle - similar to bird songs and ecological niche. 

% Learning compositional sequences with multiple time scales through a hierarchical network of spiking neurons.
% Maes A, Barahona M, Clopath C.PLoS Comput Biol. 2021

% Characteristics of sequential activity in networks with temporally asymmetric Hebbian learning.
% Gillett M, Pereira U, Brunel N.Proc Natl Acad Sci U S A. 2020

% Unsupervised Learning of Persistent and Sequential Activity.
% Pereira U, Brunel N.Front Comput Neurosci. 2020

% From space to time: Spatial inhomogeneities lead to the emergence of spatiotemporal sequences in spiking neuronal networks.
% Spreizer S, Aertsen A, Kumar A.PLoS Comput Biol. 2019

% Fast and flexible sequence induction in spiking neural networks via rapid excitability changes.
% Pang R, Fairhall AL.Elife. 2019 May~\citep{pang_fast_2019}

% Emergence of spontaneous assembly activity in developing neural networks without afferent input.
% Triplett MA, Avitan L, Goodhill GJ.PLoS Comput Biol. 2018

% Training and Spontaneous Reinforcement of Neuronal Assemblies by Spike Timing Plasticity.
% Ocker GK, Doiron B.Cereb Cortex. 2019.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real-World Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%----------------------------%
% TODO

% 
%----------------------------%

A second shortcoming of models derived from the polychronization model is their lack of applications in real-world scenarios. Indeed, most of these theoretical models are trying to reproduce neurobiological observations, while applications to machine learning methods, such as image processing, would further prove their plausibility. For instance, in a recent work,~\citet{ghosh_spatiotemporal_2019} proposed a two-stage unsupervised--supervised system for the categorization of spatio-temporal actions from an event-based stream. The first stage learns spatio-temporal convolutional filters targeted to minimize event-removal-related changes to a local spatio-temporal spike-event pattern. The second stage takes the output of the spatio-temporal filters as an input example containing multiple feature channels, and proceeds to train a classifier for recognition of spatio-temporal activity. For testing the system, two datasets are considered: DVS gesture and a new action recognition dataset recorded for this work. Results demonstrate the ability of the system to outperform the state-of-the-art in event-based gesture recognition, along with demonstrating superior performance to other alternative ways of obtaining the first stage filters, thus showing the potential of such representation. 

There are more applications to image processing using spiking neural networks. For instance, a set of models are based on the design of micro-circuits with specific lateral interactions embedded with spatially anisotropic connections. Using this core computational unit, and extending it to computations on a topographic representation similar to that observed in the primary visual cortex of mammals, the anisotropic rules implemented a form of delayed activation. This result was based on a predictive model defined in the Bayesian framework (the so-called free-energy principle) which was able to account for temporal delays in the  system, both at the sensory and motor levels~\citep{perrinet_active_2014}, and in particular that ``the application of delay operators just means changing synaptic connection strengths to take different mixtures of generalized sensations and their prediction errors.'' Such a model was implemented at the network level and applied to various motion detection tasks. In essence, two neurons, which were selective to specific motions, were connected if the delay was coherent with the change in the position of their respective receptive fields~\citep{hogendoorn_predictive_2019}.  This was also implemented in a neural mass model, showing that such anisotropic connectivity may explain the emergence of tracking~\citep{khoei_motion-based_2013}, and further explored in a spiking neural network which reproduced the observation that neural activity was maintained during the trajectory of a smoothly moving dot, even if it was momentarily blanked~\citep{kaplan_anisotropic_2013}. This led in particular to the proposal that such delay-based computations could explain diverse perceptual mechanisms, such as the so-called flash--lag illusion~\citep{khoei_flash-lag_2017}. However, these latter models used parametric rules for defining the weights. Extending such mechanisms with the ability of learning delays in a SNN will provide a breakthrough in the efficiency of these networks, and we will explore some exemplar results from neuromorphic engineering to obtain better insights on that aspect.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications of Precise Spiking Motifs in Neuromorphic Engineering}
\label{sec:neuromorphic}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Artificial intelligence has made huge advances in the past decades. Deep learning algorithms, nowadays, outperform humans at complex tasks such as natural image recognition or abstract strategy board games. Yet, machine learning algorithms suffer from adversarial attacks or a lack of generalization \change[Antoine]{capacity but}{capacity. However,} their main weakness, compared to biological neural networks, is their poor energy efficiency. Neuromorphic engineering intends to mimic the neural bases of communication with a wide variety of technics, from strictly analog circuits to software-based neuromorphic systems, and to develop tools improving the capacities of current artificial intelligence~\citep{roy_towards_2019, javanshir_advancements_2022}. Because the reduced energy consumption of biological networks can be explained \add{in part} by the use of spikes and asynchronous responses to exchange information~\citep{maass_networks_1997}, neuromorphic chips use this parallel and event-based representation to \change{make}{perform} energy-efficient computations. Another important distinction with classical von Neumann architectures is the localized memory of this new type of chips. It can be materialized by the capacity of the physical connections between the processing units to store information~\citep{markovic_physics_2020}. An example of such a connection, directly inspired by synaptic plasticity, is the memristor~\citep{rasetto_challenges_2022} for which the resistance value can be dynamically adjusted. Using these event-based computations as a building brick, neuromorphic engineering proposes new hardware designs perfect to simulate SNNs and use the full power of asynchronous computations observed in biological systems. Even if some useful SNNs simulators run on GPUs~\citep{diesmann_nest_2003, hazan_bindsnet_2018, stimberg_brian_2019}, such event-based computing techniques show their advantages in terms of frugality and rapidity only on neuromorphic chips. 

This field of research is inspired by neuroscientific advances and a computational formalism to design innovative architectures and, by artificially reproducing such mechanisms, it is interesting to study neural circuitry. Many connections can be drawn between neuromorphic engineering and computational neuroscience to aim at solving both research and technology challenges~\citep{zenke_visualizing_2021}. In this section, we give a description of the different neuromorphic hardware that have been developed and see how they can be used to deal with precise temporal motifs. 
%
\subsection{The Emergence of Novel Computational Architectures}
%
To our knowledge, the first neuromorphic circuit is the pulsed current-source synapse proposed by Carver Mead in 1989~\citep{mead_analog_1989}. It was implemented with transistors operating in the sub-threshold domain and responded to asynchronous events, but was not capable of discriminating two different spiking sequences with the same firing rate. Indeed, the postsynaptic membrane potential was increased by a step proportional to the input current but did not decrease in time, as it can be observed in biological neurons. Then, electronics circuits became more and more bio-realistic and, two decades later, ~\citep{bartolozzi_synaptic_2007} released the Diff-Pair Integrator (DPI) synapse that could reproduce the global dynamics of the biological neurons. The DPI circuit could multiplex in time spikes from different sources and became a potential  ``silicon coincidence detector''. Today, many devices are good candidates for implementing event-based algorithms and use the \change[Antoine]{AER}{address event representation (AER)}. They can be divided intro three major categories: digital, analog and mixed analog/digital platforms. For a more complete review, the reader can refer to~\citep{schuman_survey_2017}, where we site the most popular ones. SpiNNaker~\citep{furber_overview_2013, furber_spinnaker_2020}, Loihi~\citep{davies_loihi_2018} and TrueNorth~\citep{merolla_million_2014} chips are widely used examples of digital hardware implementations. Compared to TrueNorth, which exclusively implements a LIF neuron, SpiNNaker and Loihi offer some flexibility in terms of neuron model and allow for on-chip learning. This flexibility in the implementation comes at the cost of an increased energy consumption. Mixed analog--digital systems were developed at Stanford University: Neurogrid and Braindrop~\citep{benjamin_neurogrid_2014, neckar_braindrop_2019}. They are mostly used by computational neuroscientists to model brain activity with different levels of abstraction. BrainScaleS~\citep{schemmel_wafer-scale_2010} is another mixed analog--digital system developed, just like SpiNNaker, for the Human Brain Project~\citep{markram_introducing_2011}. It is a wafer-scale neuromorphic hardware with analog components. Analog arrays (i.e., field programmable analog arrays (FPAA)) refer to the initial idea of neuromorphic hardware aiming at building strictly analog devices. The pulsed current source synapse and the DPI are examples of such devices; we can also mention the field programmable neural array~\citep{farquhar_field_2006} and the NeuroFPAA~\citep{cheng_fpaa_2009} specifically designed for neuromorphic systems. Due to their lack of generality and some issues specific to analog circuits, these fully analog devices are not yet widely used for neuromorphic computing. 
%
%----------------------------%

%----------------------------%
%
Neuromorphic sensors have also been developed with the idea to capture external stimuli more efficiently and closer to biological systems. A widely used example is the dynamic vision sensors (DVSs) which provide a stream of binary asynchronous events signaling detectable changes in luminance. These devices, also named ``silicon retinas'' (see Figure~\ref{fig:silicon_retina}), show great improvements in terms of memory allocation, or power consumption, for the recording of a visual scene. We also report other event-based sensing devices for sound~\citep{chan_aer_2007} and touch~\citep{haessig_event-based_2020} but will focus on DVS for the next subsection about the use of dynamics embedded in event-based signals. 

\begin{figure}[H]
\centering
\includegraphics[width=0.980\linewidth]{figures/event_driven_computations.png}
\caption{
 A miniature, event-based ATIS sensor.
 Contrary to a classical frame-based camera for which a full dense image representation is given at discrete, regularly spaced timings, the event-based camera provides events at the micro-second resolution. These are sparse, as they represent luminance increments or decrements (ON and OFF events, respectively). Figure courtesy of Sio-Hoi Ieng (Sorbonne Université/UPMC, Institut de la Vision).}\label{fig:silicon_retina}
\end{figure}

%Using these event-based computations as a building brick, neuromorphic engineering proposes new hardware designs perfect to simulate SNNs and use the full power of asynchronous computations observed in biological systems. This field of research is inspired by neuroscientific advances and a computational formalism to design innovative architectures and, by reproducing such mechanisms, it is interesting to study neural circuitry. Many connections can be drawn between neuromorphic engineering and computational neuroscience to aim at solving both research and technology challenges~\citep{zenke_visualizing_2021}. Just as communication between neurons can be illustrated by a raster plot, information exchanges in neuromorphic hardware are described as event streams by an Address-Event Representation (AER). In this section, we will use indifferently \textit{events} or \textit{spikes} to describe the same binary and discrete processes and we will extend the review of the use of precise spike timing to this emerging field. There exist different types of event-based sensing devices for sound~\citep{chan_aer_2007} and touch~\citep{haessig_event-based_2020} and here we focus only on neuromorphic cameras to review different methods using efficiently the dynamics of event streams. Then, we dedicate one subsection to the implementation of delay-based algorithms on neuromorphic chips.  
%
\subsection{On the Importance of Spatio-Temporal Information in Silicon Retinas}
%
With the AER specification and their sub-millisecond temporal precision, DVSs bring a new approach to the storage and processing of visual information. From their generative model, these sparse events are markers of the dynamics of the visual scene captured by the sensor. The dynamics of the event streams have to be used to make sense of the recorded information, and new algorithms are needed to solve efficiently classical computer vision~tasks. 

In~\citep{lagorce_hots_2017}, \textit{\hl{time surfaces}} were introduced as an event-driven 2D image of the delay between the last event recorded at the address of a pixel and the current time. An exponential decay is applied on this delay to obtain the analog values of the time surfaces. It gives more precision to represent recent events and offers an analogy with the LIF spiking neuron. It is a way to represent the local dynamics embedded in the event-based recordings, and with unsupervised learning on the event stream based on the cosine similarity, they can capture repeating motifs within the input signals. Learned time surfaces can be used for object recognition~\citep{lagorce_hots_2017, sironi_hats_2018, maro_event-based_2020, grimaldi_homeostatic_2021,grimaldi_robust_2022} and show that this method could be efficiently applied to state-of-the-art benchmarks.

More generally, three-dimensional convolutions in both space and time are another representation of the spiking motifs embedded in the event stream~\citep{ghosh_spatiotemporal_2019, grimaldi_learning_2022, yu_stsc-snn_2022}. With their additional temporal dimension, their kernels can capture multiple events on the same pixel address, as long as they belong to the local temporal window (see Figure~\ref{fig:icip}). This representation is only limited by the time step used for the discretization of the signal; this factor defines the temporal precision of the representation. Other methods make direct use of the precise timing of events captured by the DVS to solve optical flow and time-to-contact challenges~\citep{benosman_event-based_2014, clady_asynchronous_2014, tschechne_bio-inspired_2014}, inferring depth~\citep{hidalgo-carrio_learning_2020}, feature detection and tracking~\citep{dardelet_event-by-event_2021}, motion segmentation~\citep{stoffregen_event-based_2019} or the simultaneous localization and mapping problem~\citep{kim_real-time_2016}. This non-exhaustive list of complex task solving is not directly linked to biological processes, but shows the potential of the precise temporal resolution of neuromorphic retina-like cameras. By essence, these sensors offer a novel view of visual information processing due to the asynchronous responses of the different pixels. With this type of signal, the use of spatio-temporal motifs embedded in the event streams is essential to solve high-level visual tasks. 

\begin{figure}[H]%[t!]
%  \centering
  \includegraphics[width=0.98\linewidth]{figures/2022-09-27_MotionDetection_kernel.png}
  %\includegraphics[width=0.980\linewidth]{figures/2022-10-10_MotionDetection_kernel.png}
    \caption{
      Detecting visual motion in an event stream with heterogeneous delays.
      Extending the polychrony detection model to the spatial domain,~\citet{grimaldi_learning_2022} have applied a supervised learning scheme to the detection of motion. The models' parameters are represented by different spatio-temporal kernels, and we show three examples as pairs of rows, one targeting ON spikes, the other OFF spikes, the first column representing the corresponding motion detected. When trained on a set of natural images, it shows the emergence of localized, oriented kernels organized in a so-called push--pull organization for which weights to an ON spike are negatively proportional to that to an OFF cell~\citep{kremkow_push-pull_2016}. Global weight is globally decreasing from the lowest delay (\textbf{right}) to less recent information (\textbf{left}).
     }
  \label{fig:icip}
\end{figure}

%
\subsection{Computations with Delays in Neuromorphic Hardware}
%
For the rest of this section, we report examples of implementations of event-based algorithms using precise spatio-temporal motifs on neuromorphic hardware. 
\citep{hussain_deltron_2012} implemented a delay-learning algorithm on an analog chip. Online learning on neuromorphic chips is still a challenge today and for this work, only the detection of spiking motifs was performed on the analog architecture; training was performed digitally and based on the tempotron learning rule~\citep{gutig_tempotron_2006}. In addition to delay learning, a group at the University of West Sydney developed a neuromorphic implementation of multiple synaptic plasticity learning rules~\citep{wang_neuromorphic_2015}. They showed that STDP and spike-timing-dependent delay plasticity rules could be implemented in both a digital and an analog chip. From the network parameters and the physical limitations to store it on-chip, they proved that the digital implementation is way easier to scale up and that an external memory would be needed for a larger network. The same group presented a FPGA hardware implementation of polychronous networks in which propagation delays are learned in a supervised manner, based on the expected firing time of the post-synaptic neuron~\citep{wang_fpga_2014}. %This paper proposes a supervised delay learning algorithm for spiking neurons with temporal encoding, in which both the weight and delay of a synaptic connection can be adjusted to enhance the learning performance. 
\citet{pfeil_neuromorphic_2013} implemented STDP on a mixed analog--digital chip to simulate the sound localization processes observed on the barn owl auditory system~\citep{gerstner_neuronal_1996}. Coherence detection on input spikes coming from two sources was obtained with a 50 \hl{$ns$} precision. They claimed that this unsupervised learning denoises the input and compensates for variations between neural components. The variability of response of the analog components can be compensated by population coding for responses robust to noise, and this phenomenon is also observed in biological neural networks~\citep{boerlin_spike-based_2011}. 
A recent work performed the implementation of a sparse vector symbol architecture binding operation on the Loihi chip, delay lines and coincidence detection, used to compute the binding operation~\citep{renner_sparse_2022}. They highlighted the fact that using delays can be expensive \add[Laurent]{notably in memory bandwidth} because incoming spikes have to be stored in blocks with a temporal dimension equivalent to the maximal delay. Note that this problem is also due to the algorithm used in this paper and that the analog chips must not suffer from this type of~limitation. 

Online on-chip learning and computations with delays are still emerging in neuromorphic engineering. The technical challenges linked to the development of this type of implementation and the growing interest in delay learning make advances in this field interesting for the future of computations with precise spatio-temporal motifs. While improvements are still to be made, neuromorphic chips seem to be a good candidate to efficiently make use of these particular features. 



%
%they made a Spike-Timing Neuromorphic Architecture with a dendritic arbor to implement the delays and a transfer function for the weights which is well-suited for areas such as pattern recognition and natural language processing. In this paper, we formally describe the STPU, implement the STPU on a field programmable gate array, and show measured performance data.\citep{hill_spike-timing_2017} not accessible
%
%review on neuromorphic computations for robotic\citep{sandamirskaya_neuromorphic_2022} (not accessible for the moment)
%
%We apply delay-based reservoir computing as the information processing core, along with a novel training and labelling method. Different from the conventional ECG classification techniques, this computation model is a end-to-end dynamic system that mimics the real-time signal flow in neuromorphic hardware~\citep{liang_neuromorphic_2022} 
%
%Real-time cortical simulation on neuromorphic hardware -> https://royalsocietypublishing.org/doi/full/10.1098/rsta.2019.0160
%
% solutions usuellement utilisées - surrogate gradient
%for other section: A hypothesis for temporal coding of young and mature granule cells(https://www.frontiersin.org/articles/10.3389/fnins.2013.00075/full)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%: >>>>>>>> Laurent is here <<<<<<<<<<<<<<
In this review paper, we presented recent evidence for the role of precise spiking motifs in neuroscience. In particular, we showed that such particular motifs may play a crucial role in neurobiology, that they may be understood at the theoretical and computational levels, and that they may have numerous applications in neuromorphic engineering. In particular, we showed the following:
\begin{itemize}
  \item  The efficiency of neural systems, and in particular the visual system, imposes strong constraints on the structure of neural activity which highlights the importance of precise spike times;
  \item  Growing evidence from neurobiology proves that neural systems are more than integrators and may use synchrony detection in different forms: synfire chains, travelling waves on arbitrary spiking motifs, \add[Laurent]{and notably that an encoding based on precise spiking motifs may provide huge computational benefits};
  \item  Many theoretical models already exist, taking into account the specificity of spiking motifs, notably by using heterogeneous delays;
  \item  Using precise spiking motifs could ultimately be a key ingredient in neuromorphic systems to reach similar efficiencies as biological neural systems.
\end{itemize}

 Overall, our reviewing effort has shown that a growing community is focusing on that aspect. This community is based on solid and validated evidence, which is breaking novel grounds thanks to the current technical advances. Moreover, we also showed that this community is highly diverse, operating on {\hl{in vivo}}, {\hl{in computo}} and {\hl{in silico}} systems. As a consequence, the global effort is still largely scattered, which limits its larger acceptance in~neuroscience. 

\subsection{Limits}
Additionally, the different models of spike motif detection and learning that we have presented at these different levels (neurobiological, theoretical, and neuromorphic) individually present limitations that prevent their widespread application in neuroscience.

First, many models are based on a discretization of time. This assumption is important to allow for a useful representation of neuronal information in order to be processed in computers. This treatment amounts to transforming spike trains into a matrix form for processing in classical machine learning algorithms. This assumption therefore implies an ineffective use of the memory, as this representation transforms the sparse representation of a spike sequence into dense matrices. In addition, this representation can induce errors due to the discretization and the scale of temporal sampling. Finally, this representation encourages the use of classical methods, which are not adapted to disruptive applications, such as event-driven representations.

Moreover, the learning of patterns is often done in a supervised way. Indeed, the problem of detecting polychronous groups implies to infer both the address and the precise time of the occurrence of these motifs. Most of the models we have presented are based on the assumption that at least one of its variables is known: either the pattern, its identity, or its time of occurrence. This constraint is to be put in parallel with the way a biological nervous system works in which learning is performed autonomously, i.e., without supervision. However, we can note that some models can perform such learning, but only in the case of data for which the motifs are easily separable. More generally, to reproduce the efficiency of biological systems, one should account for the different temporal scales of adaptation, from seconds to years. For instance, the scaffolding of neural assemblies seems to follow critical periods during development~\citep{dard_rapid_2022}.

A final limitation of the models we have presented is that they consist of a single processing layer that links an input to an output. However, we saw that the neurobiological system uses processing loops within hierarchical graphs. In general, these systems are bidirectionally connected across different layers (for instance cortical areas), but also within a layer, as was for instance used by~\citet{izhikevich_polychronization_2006}. The whole system forms a dynamical model which may be considered globally during the learning phase, yet while taking account the constraints of the system, for instance, the lack of a global clock, or the cost of fully connected topologies. Moreover,  these processes have to be distinguished from judgements on timing, such as temporal order processing (judging when one event happens relative to another) or duration estimation (measuring how long an event lasts)~\citep{coull_distinction_2022}.
%
\subsection{Perspectives}
%
The limits that we have presented can be treated individually in each model, as evidenced by individual efforts, which try to overcome them. However, to propose a real breakthrough, we believe that future venues should provide with a unified, interdisciplinary approach, with applications to real-world, ecological scenarios and with open and reproducible methods. 

First, as we already noted, the effort is still largely scattered. This is in part due to the fact that interaction between neurobiology, theoretical and computational neuroscience and neuromorphic engineering are still scarce as of today. It was largely demonstrated that close, bidirectional interactions are essential to foster breakthroughs. For instance, the design of model-driven protocols has proven to be essential in modern neuroscience. Additionally, if neural networks were essential in shaping modern-day machine learning, e.g., computer vision using deep learning, spiking neural networks should prove essential in future emerging technologies.

In that perspective, it is essential that such models are tested on ecologically relevant, real-world scenarios. Indeed, classic convolutional neural networks have emerged as optimal solutions, for example, to classify static images into categories, yet they are not well-adapted for processing dynamic, multimodal sensory flows. The emerging necessity to be able to process more complex flows, such as the multiple flows of information in a car designed for autonomous driving, necessitates modifying such modelling paradigms, and in particular, to take into account that the generated actions may modify the sensory inputs. Notably, the protocols used as well in neurobiology, theory or engineering should take into account these novel levels of complexity.

Ultimately, the community should encourage the adoption of open, reproducible science. Indeed, the different models that we have displayed often come with the tools necessary to reproduce the results obtained. This is true in neurobiology~\citep{bellec_fitting_2021}, in theoretical neuroscience~\citep{izhikevich_polychronization_2006} or in engineering~\citep{grimaldi_robust_2022}. This aspect is essential to foster the emergence of interdisciplinary projects, such as model-driven neurobiological experiments or biologically inspired neuromorphic engineering. Solutions exist to optimize these collaborations~\citep{panahi_generative_2021} and suggest the emergence of a novel paradigm for scientific advances in neuroscience~\citep{tolle_fourth_2011}, i.e., by using data exploration in which the scientific models are fit to the data by learning algorithms. As such, this review aims at paving the way to openly share the variety of resources and to offer a unified view on the role of precise spiking motifs in neuroscience.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following supporting information can be downloaded at:  \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following supporting information can be downloaded at: \linksupplementary{s1}, Figure S1: title; Table S1: title; Video S1: title. A supporting video article is available at doi: link.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \authorcontributions{Conceptualization, LP; writing---original draft preparation, all authors; writing---review and editing, all authors; supervision, LP; funding acquisition, LP and JM. All authors have read and agreed to the published version of the manuscript.}

\funding{
This research was funded by the European Union ERA-NET CHIST-ERA 2018 research and innovation program under grant number ANR-19-CHR3-0008-03 (``\href{http://aprovis3d.eu/}{\hl{APROVIS3D}}''). % Agence Nationale de la Recherche %HL and 
LP, CB and AG received funding from the ANR grant number ANR-20-CE23-0021 ``\href{https://laurentperrinet.github.io/grant/anr-anr/}{\hl{AgileNeuroBot}}''.
% LP received funding 
LP received support from the french government under the France 2030 investment plan, as part of the Initiative d’Excellence d’Aix-Marseille Université – A*MIDEX number AMX-21-RID-025 ``\href{https://laurentperrinet.github.io/grant/polychronies/}{\hl{Polychronies}}''.}

%\institutionalreview{In this section, you should add the Institutional Review Board Statement and approval number, if relevant to your study. You might choose to exclude this statement if the study did not require ethical approval. Please note that the Editorial Office might ask you for further information. Please add “The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving humans. OR “The animal study protocol was approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving animals. OR “Ethical review and approval were waived for this study due to REASON (please provide a detailed justification).” OR “Not applicable” for studies not involving humans or animals.}

%\informedconsent{Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.

%Written informed consent for publication must be obtained from participating patients who can be identified (including by the patients themselves). Please state ``Written informed consent has been obtained from the patient(s) to publish this paper'' if applicable.}

\institutionalreview{Not applicable.
%In this section, you should add the Institutional Review Board Statement and approval number, if relevant to your study. You might choose to exclude this statement if the study did not require ethical approval. Please note that the Editorial Office might ask you for further information. Please add “The study was conducted in accordance with the Declaration of Helsinki, and approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving humans. OR “The animal study protocol was approved by the Institutional Review Board (or Ethics Committee) of NAME OF INSTITUTE (protocol code XXX and date of approval).” for studies involving animals. OR “Ethical review and approval were waived for this study due to REASON (please provide a detailed justification).” OR “Not applicable” for studies not involving humans or animals.
}

\informedconsent{Not applicable.
%Any research article describing a study involving humans should contain this statement. Please add ``Informed consent was obtained from all subjects involved in the study.'' OR ``Patient consent was waived due to REASON (please provide a detailed justification).'' OR ``Not applicable'' for studies not involving humans. You might also choose to exclude this statement if the study did not involve humans.
%
%Written informed consent for publication must be obtained from participating patients who can be identified (including by the patients themselves). Please state ``Written informed consent has been obtained from the patient(s) to publish this paper'' if applicable.
}

\dataavailability{This work is made reproducible using the following tools. First the manuscript's source code, and the code reproducing all figures is available on \href{https://github.com/SpikeAI/2022_polychronies-review}{GitHub}. %~\citep{Grimaldi2023review}. 
This list also links to preprints versions of this review as well as links to previous versions. Find the associated \href{https://www.zotero.org/groups/4562620/polychronies}{Zotero group} which was used to regroup relevant literature on the subject.} 

\acknowledgments{We would like to thank Hugo Ladret and Frédéric Chavane for valuable feedback while writing this review. Many thanks to Simon Thorpe for the useful discussions and references on the latency of the visual system in human and non-human primates.}

\conflictsofinterest{The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Optional
% \sampleavailability{Samples of the compounds ... are available from the authors.}

% %% Only for journal Encyclopedia
% %\entrylink{The Link to this entry published on the encyclopedia platform.}

% \abbreviations{Abbreviations}{
% The following abbreviations are used in this manuscript:\\

% \noindent 
% \begin{tabular}{@{}ll}
% MDPI & Multidisciplinary Digital Publishing Institute\\
% DOAJ & Directory of open access journals\\
% TLA & Three letter acronym\\
% LD & Linear dichroism
% \end{tabular}
% }

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %% Optional
% \appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
% \appendixstart
% \appendix
% \section[\appendixname~\thesection]{}
% \subsection[\appendixname~\thesubsection]{}
% The appendix is an optional section that can contain details and data supplemental to the main text---for example, explanations of experimental details that would disrupt the flow of the main text but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data are shown in the main text can be added here if brief, or as Supplementary Data. Mathematical proofs of results not central to the paper can be added as an appendix.

% \begin{table}[H] 
% \caption{This is a table caption.\label{tab5}}
% \newcolumntype{C}{>{\centering\arraybackslash}X}
% \begin{tabularx}{\textwidth}{CCC}
% \toprule
% \textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
% \midrule
% Entry 1		& Data			& Data\\
% Entry 2		& Data			& Data\\
% \bottomrule
% \end{tabularx}
% \end{table}

% \section[\appendixname~\thesection]{}
% All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}
%\printendnotes[custom] % Un-comment to print a list of endnotes

%%%%%%%%% REFERENCES
\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 
%\bibliographystyle{Definitions/chicago2}
\begin{thebibliography}{999}

\bibitem[Piccolino(1997)]{piccolino_luigi_1997}
Piccolino, M.
\newblock Luigi {Galvani} and animal electricity: two centuries after the
  foundation of electrophysiology.
\newblock {\em Trends Neurosci.} {\bf 1997}, {\em 20},~443--448.
\newblock {{https://doi.org/10.1016/S0166-2236(97)01101-6}}.

\bibitem[Adrian and Zotterman(1926)]{adrian_impulses_1926}
Adrian, E.D.; Zotterman, Y.
\newblock The impulses produced by sensory nerve endings.
\newblock {\em  J. Physiol.} {\bf 1926}, {\em 61},~465--483.

\bibitem[Gouras(1960)]{gouras_graded_1960}
Gouras, P.
\newblock Graded potentials of bream retina.
\newblock {\em  J. Physiol.} {\bf 1960}, {\em 152},~487--505.

\bibitem[Perkel {et~al.}(1967{\natexlab{a}})Perkel, Gerstein, and
  Moore]{perkel_neuronal_1967}
Perkel, D.H.; Gerstein, G.L.; Moore, G.P.
\newblock Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {I}.
  {The} {Single} {Spike} {Train}.
\newblock {\em Biophys. J.} {\bf 1967}, {\em 7},~391--418.
\newblock {{https://doi.org/10.1016/S0006-3495(67)86596-2}}.

\bibitem[Perkel {et~al.}(1967{\natexlab{b}})Perkel, Gerstein, and
  Moore]{perkel_neuronal_1967-1}
Perkel, D.H.; Gerstein, G.L.; Moore, G.P.
\newblock Neuronal {Spike} {Trains} and {Stochastic} {Point} {Processes}: {II}.
  {Simultaneous} {Spike} {Trains}.
\newblock {\em Biophys. J.} {\bf 1967}, {\em 7},~419--440.
\newblock {{https://doi.org/10.1016/S0006-3495(67)86597-4}}.

\bibitem[Abeles(1982)]{abeles_role_1982}
Abeles, M.
\newblock Role of the cortical neuron: integrator or coincidence detector?
\newblock {\em Isr. J. Med. Sci.} {\bf 1982}, {\em
  18},~83--92.

\bibitem[Carr(1993)]{carr_processing_1993}
Carr, C.E.
\newblock Processing of {Temporal} {Information} in the {Brain}.
\newblock {\em Annu. Rev. Neurosci.} {\bf 1993}, {\em 16},~223--243.\linebreak
\newblock {{https://doi.org/10.1146/annurev.ne.16.030193.001255}}.

\bibitem[Davis {et~al.}(2021)Davis, Benigno, Fletterman, Desbordes, Steward,
  Sejnowski, H~Reynolds, and Muller]{davis_spontaneous_2021}
Davis, Z.W.; Benigno, G.B.; Fletterman, C.; Desbordes, T.; Steward, C.;
  Sejnowski, T.J.;~Reynolds, J.H.; Muller, L.
\newblock Spontaneous traveling waves naturally emerge from horizontal fiber
  time delays and travel through locally asynchronous-irregular states.
\newblock {\em Nat. Commun.} {\bf 2021}, {\em 12},~6057.
\newblock {{https://doi.org/10.1038/s41467-021-26175-1}}.

\bibitem[Perrinet {et~al.}(2004)Perrinet, Samuelides, and
  Thorpe]{perrinet_coding_2004}
Perrinet, L.; Samuelides, M.; Thorpe, S.
\newblock Coding static natural images using spiking event times: do neurons
  cooperate?
\newblock {\em IEEE Trans. Neural Netw.} {\bf 2004}, {\em
  15},~1164--1175.
 {{https://doi.org/10.1109/TNN.2004.833303}}.

\bibitem[Gollisch and Meister(2008)]{gollisch_rapid_2008}
Gollisch, T.; Meister, M.
\newblock Rapid {Neural} {Coding} in the {Retina} with {Relative} {Spike}
  {Latencies}.
\newblock {\em Science} {\bf 2008}, {\em 319},~1108--1111.
\newblock {{https://doi.org/10.1126/science.1149639}}.

\bibitem[DeWeese and Zador(2003)]{deweese_binary_2003}
DeWeese, M.R.; Zador, A.M.
\newblock {\em Binary Coding in Auditory Cortex}; \hl{Neural Information Processing
  Systems Foundation:} %MDPI: Please add the location of the publisher (City and Country).
   2003.

\bibitem[Carr and Konishi(1990)]{carr_circuit_1990}
Carr, C.E.; Konishi, M.
\newblock A circuit for detection of interaural time differences in the brain
  stem of the barn owl.
\newblock {\em J. Neurosci.} {\bf 1990}, {\em 10},~3227--3246.
%\newblock Publisher: Society for Neuroscience Section: Articles,
  {{https://doi.org/10.1523/JNEUROSCI.10-10-03227.1990}}.

\bibitem[Bohte(2004)]{bohte_evidence_2004}
Bohte, S.M.
\newblock The evidence for neural information processing with precise
  spike-times: {A} survey.
\newblock {\em Nat. Comput.} {\bf 2004}, {\em 3},~195--206.
\newblock {{https://doi.org/10.1023/B:NACO.0000027755.02868.60}}.

\bibitem[DiLorenzo and Victor(2013)]{dilorenzo_spike_2013}
DiLorenzo, P.M.; Victor, J.D.
\newblock {\em Spike {Timing}: {Mechanisms} and {Function}}; CRC Press:  \hl{Boca Raton, FL, USA,} %newly added information, please confirm
  2013.
%\newblock Google-Books-ID: KTHUIMUpQCUC.  %MDPI: We removed Google-Books-ID: KTHUIMUpQCUC. Please confirm this revision.


\bibitem[Roy {et~al.}(2019)Roy, Jaiswal, and Panda]{roy_towards_2019}
Roy, K.; Jaiswal, A.; Panda, P.
\newblock Towards spike-based machine intelligence with neuromorphic computing.
\newblock {\em Nature} {\bf 2019}, {\em 575},~607--617.
%\newblock Number: 7784 Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/s41586-019-1677-2}}.

\bibitem[Flourens(1842)]{flourens_recherches_1842}
Flourens, M.J.P.
\newblock {\em Recherches Expérimentales sur les Propriétés et les Fonctions
  du Système Nerveux, Dans les Animaux Vertébrés}; Balliere, J.-B., Ed.;  \hl{1842.} %MDPI: Please add the name of the publisher and their location.


\bibitem[Pearce(2009)]{pearce_marie-jean-pierre_2009}
Pearce, J.
\newblock Marie-{Jean}-{Pierre} {Flourens} (1794--1867) and {Cortical}
  {Localization}.
\newblock {\em Eur. Neurol.} {\bf 2009}, {\em 61},~311--314.\linebreak
\newblock {{https://doi.org/10.1159/000206858}}.

\bibitem[Hubel and Wiesel(1968)]{hubel_receptive_1968}
Hubel, D.H.; Wiesel, T.N.
\newblock Receptive fields and functional architecture of monkey striate
  cortex.
\newblock {\em  J. Physiol.} {\bf 1968}, {\em 195},~215--243.
\newblock {{https://doi.org/10.1113/jphysiol.1968.sp008455}}.

\bibitem[Carandini and Heeger(2012)]{carandini_normalization_2012}
Carandini, M.; Heeger, D.J.
\newblock Normalization as a canonical neural computation.
\newblock {\em Nat. Rev. Neurosci.} {\bf 2012}, {\em 13},~51--62.
\newblock {{https://doi.org/10.1038/nrn3136}}.

\bibitem[Thorpe {et~al.}(1996)Thorpe, Fize, and Marlot]{thorpe_speed_1996}
Thorpe, S.; Fize, D.; Marlot, C.
\newblock Speed of processing in the human visual system.
\newblock {\em Nature} {\bf 1996}, {\em 381},~520--522.\linebreak
\newblock {{https://doi.org/10.1038/381520a0}}.

\bibitem[Kirchner and Thorpe(2006)]{kirchner_ultra-rapid_2006}
Kirchner, H.; Thorpe, S.
\newblock Ultra-rapid object detection with saccadic eye movements: {Visual}
  processing speed revisited.
\newblock {\em Vis. Res.} {\bf 2006}, {\em 46},~1762--1776.
%\newblock Loaded from an external bibliography file by Manubot.
%  source\_bibliography: manual-references.bib standard\_id: Kirchner06,
  {{https://doi.org/10.1016/j.visres.2005.10.002}}.

\bibitem[Keysers {et~al.}(2001)Keysers, Xiao, Földiák, and
  Perrett]{keysers_speed_2001}
Keysers, C.; Xiao, D.K.; Földiák, P.; Perrett, D.I.
\newblock The {Speed} of {Sight}.
\newblock {\em J. Cogn. Neurosci.} {\bf 2001}, {\em
  13},~90--101.\linebreak
\newblock {{https://doi.org/10.1162/089892901564199}}.

\bibitem[Schmolesky {et~al.}(1998)Schmolesky, Wang, Hanes, Thompson,
  Leutgeb, Schall, and Leventhal]{schmolesky_signal_1998}
Schmolesky, M.T.; Wang, Y.; Hanes, D.P.; Thompson, K.G.; Leutgeb, S.; Schall,
  J.D.; Leventhal, A.G.
\newblock Signal timing across the macaque visual system.
\newblock {\em J. Neurophysiol.} {\bf 1998}, {\em 79},~3272--3278.
\newblock {{https://doi.org/10.1152/jn.1998.79.6.3272}}.

\bibitem[Vanni {et~al.}(2001)Vanni, Tanskanen, Seppä, Uutela, and
  Hari]{vanni_coinciding_2001}
Vanni, S.; Tanskanen, T.; Seppä, M.; Uutela, K.; Hari, R.
\newblock Coinciding early activation of the human primary visual cortex and
  anteromedial cuneus.
\newblock {\em Proc. Natl. Acad. Sci.  USA} {\bf 2001}, {\em 98},~2776--2780.
\newblock {{https://doi.org/10.1073/pnas.041600898}}.

\bibitem[Lamme and Roelfsema(2000)]{lamme_distinct_2000}
Lamme, V.A.; Roelfsema, P.R.
\newblock The distinct modes of vision offered by feedforward and recurrent
  processing.
\newblock {\em Trends Neurosci.} {\bf 2000}, {\em 23},~571--579.
\newblock {{https://doi.org/10.1016/s0166-2236(00)01657-x}}.

\bibitem[Serre {et~al.}(2007)Serre, Oliva, and
  Poggio]{serre_feedforward_2007}
Serre, T.; Oliva, A.; Poggio, T.
\newblock A feedforward architecture accounts for rapid categorization.
\newblock {\em Proc. Natl. Acad. Sci.  USA} {\bf 2007}, {\em 104},~6424--6429.
%\newblock tex.bdsk-url-2: https://doi.org/10.1073/pnas.0700622104
%  tex.date-added: 2022-05-05 19:08:15 +0200 tex.date-modified: 2022-05-05
%  19:08:15 +0200, 
  {{https://doi.org/10.1073/pnas.0700622104}}.

\bibitem[Jérémie and Perrinet(2022)]{jeremie_ultrafast_2022}
Jérémie, J.N.; Perrinet, L.U.
\newblock Ultrafast image categorization in vivo and in silico. \emph{arXiv}  \textbf{2022}, 
\newblock arXiv:2205.03635.

\bibitem[Nowak and Bullier(1997)]{nowak_timing_1997}
Nowak, L.G.; Bullier, J.
\newblock The {Timing} of {Information} {Transfer} in the {Visual} {System}. In
  {\em Extrastriate {Cortex} in {Primates}}; Springer: \hl{Boston, MA, USA,} %MDPI: We added the location of the publisher. Please confirm.
  1997; pp. 205--241.

\bibitem[Thorpe and Fabre-Thorpe(2001)]{thorpe_seeking_2001}
Thorpe, S.J.; Fabre-Thorpe, M.
\newblock Seeking {Categories} in the {Brain}.
\newblock {\em Science} {\bf 2001}, {\em 291},~260--263.
\newblock {{https://doi.org/10.1126/science.1058249}}.

\bibitem[Rucci {et~al.}(2018)Rucci, Ahissar, and Burr]{rucci_temporal_2018}
Rucci, M.; Ahissar, E.; Burr, D.
\newblock Temporal {Coding} of {Visual} {Space}.
\newblock {\em Trends Cogn. Sci.} {\bf 2018}, {\em 22},~883--895.\linebreak
\newblock {{https://doi.org/10.1016/j.tics.2018.07.009}}.

\bibitem[Softky and Koch(1993)]{softky_highly_1993}
Softky, W.; Koch, C.
\newblock The highly irregular firing of cortical cells is inconsistent with
  temporal integration of random {EPSPs}.
\newblock {\em  J. Neurosci.} {\bf 1993}, {\em 13},~334--350.
\newblock {{https://doi.org/10.1523/jneurosci.13-01-00334.1993}}.

\bibitem[Bryant and Segundo(1976)]{bryant_spike_1976}
Bryant, H.L.; Segundo, J.P.
\newblock Spike initiation by transmembrane current: a white-noise analysis.
\newblock {\em  J. Physiol.} {\bf 1976}, {\em 260},~279--314.
%\newblock \_eprint:
%  https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1976.sp011516,
  {{https://doi.org/10.1113/jphysiol.1976.sp011516}}.

\bibitem[Mainen and Sejnowski(1995)]{mainen_reliability_1995}
Mainen, Z.F.; Sejnowski, T.J.
\newblock Reliability of {Spike} {Timing} in {Neocortical} {Neurons}.
\newblock {\em Science} {\bf 1995}, {\em 268},~1503--1506.\linebreak
\newblock {{https://doi.org/10.1126/science.7770778}}.

\bibitem[Ermentrout {et~al.}(2008)Ermentrout, Galán, and
  Urban]{ermentrout_reliability_2008}
Ermentrout, G.B.; Galán, R.F.; Urban, N.N.
\newblock Reliability, synchrony and noise.
\newblock {\em Trends Neurosci.} {\bf 2008}, {\em 31},~428--434.
\newblock {{https://doi.org/10.1016/j.tins.2008.06.002}}.

\bibitem[Nowak(1997)]{nowak_influence_1997}
Nowak, L.
\newblock Influence of low and high frequency inputs on spike timing in visual
  cortical neurons.
\newblock {\em Cereb. Cortex} {\bf 1997}, {\em 7},~487--501.
\newblock {{https://doi.org/10.1093/cercor/7.6.487}}.

\bibitem[Kenyon {et~al.}(2004)Kenyon, Hill, Theiler, George, and
  Marshak]{kenyon_theory_2004}
Kenyon, G.T.; Hill, D.; Theiler, J.; George, J.S.; Marshak, D.W.
\newblock A theory of the {Benham} {Top} based on center–surround
  interactions in the parvocellular pathway.
\newblock {\em Neural Netw.} {\bf 2004}, {\em 17},~773--786.
\newblock {{https://doi.org/10.1016/j.neunet.2004.05.005}}.

\bibitem[Celebrini {et~al.}(1993)Celebrini, Thorpe, Trotter, and
  Imbert]{celebrini_dynamics_1993}
Celebrini, S.; Thorpe, S.; Trotter, Y.; Imbert, M.
\newblock Dynamics of orientation coding in area {V1} of the awake primate.
\newblock {\em Vis. Neurosci.} {\bf 1993}, {\em 10},~811--825.
\newblock {{https://doi.org/10.1017/s0952523800006052}}.

\bibitem[Chase and Young(2007)]{chase_first-spike_2007}
Chase, S.M.; Young, E.D.
\newblock First-spike latency information in single neurons increases when
  referenced to population onset.
\newblock {\em Proc. Natl. Acad. Sci. USA} {\bf 2007},
  {\em 104},~5175--5180.
\newblock {{https://doi.org/10.1073/pnas.0610368104}}.

\bibitem[Safaie {et~al.}(2020)Safaie, Jurado-Parras, Sarno, Louis,
  Karoutchi, Petit, Pasquet, Eloy, and Robbe]{safaie_turning_2020}
Safaie, M.; Jurado-Parras, M.T.; Sarno, S.; Louis, J.; Karoutchi, C.; Petit,
  L.F.; Pasquet, M.O.; Eloy, C.; Robbe, D.
\newblock Turning the body into a clock: {Accurate} timing is facilitated by
  simple stereotyped interactions with the environment.
\newblock {\em Proc. Natl. Acad. Sci. USA} {\bf 2020},
  {\em 117},~13084--13093.
\newblock {{https://doi.org/10.1073/pnas.1921226117}}.

\bibitem[Gautrais and Thorpe(1998)]{gautrais_rate_1998}
Gautrais, J.; Thorpe, S.
\newblock Rate coding versus temporal order coding: a theoretical approach.
\newblock {\em Biosystems} {\bf 1998}, {\em 48},~57--65.
%\newblock Publisher: Elsevier,
  {{https://doi.org/10.1016/S0303-2647(98)00050-1}}.

\bibitem[Delorme {et~al.}(1999)Delorme, Gautrais, Van~Rullen, and
  Thorpe]{delorme_spikenet_1999}
Delorme, A.; Gautrais, J.; Van~Rullen, R.; Thorpe, S.
\newblock {SpikeNET}: {A} simulator for modeling large networks of integrate
  and fire neurons.
\newblock {\em Neurocomputing} {\bf 1999}, {\em 26},~989--996.
%\newblock Publisher: Elsevier,
  {{https://doi.org/10.1016/S0925-2312(99)00095-8}}.

\bibitem[Delorme {et~al.}(2000)Delorme, Richard, and
  Fabre-Thorpe]{delorme_ultra-rapid_2000}
Delorme, A.; Richard, G.; Fabre-Thorpe, M.
\newblock Ultra-rapid categorisation of natural scenes does not rely on colour
  cues: a study in monkeys and humans.
\newblock {\em Vis. Res.} {\bf 2000}, {\em 40},~2187--2200.
\newblock {{https://doi.org/10.1016/S0042-6989(00)00083-3}}.

\bibitem[Bonilla {et~al.}(2022)Bonilla, Gautrais, Thorpe, and
  Masquelier]{bonilla_analyzing_2022}
Bonilla, L.; Gautrais, J.; Thorpe, S.; Masquelier, T.
\newblock Analyzing time-to-first-spike coding schemes.
\newblock {\em Front. Neurosci.} {\bf 2022}, {\em 16}, 971937.
%\newblock Publisher: Frontiers,
  {{https://doi.org/10.3389/fnins.2022.971937}}.

\bibitem[Bohte {et~al.}(2002)Bohte, Kok, and
  La~Poutré]{bohte_error-backpropagation_2002}
Bohte, S.M.; Kok, J.N.; La~Poutré, H.
\newblock Error-backpropagation in temporally encoded networks of spiking
  neurons.
\newblock {\em Neurocomputing} {\bf 2002}, {\em 48},~17--37.
\newblock {{https://doi.org/10.1016/S0925-2312(01)00658-0}}.

\bibitem[Zenke and Vogels(2021)]{zenke_remarkable_2021}
Zenke, F.; Vogels, T.P.
\newblock The {Remarkable} {Robustness} of {Surrogate} {Gradient} {Learning}
  for {Instilling} {Complex} {Function} in {Spiking} {Neural} {Networks}.
\newblock {\em Neural Comput.} {\bf 2021}, {\em 33},~899--925.
\newblock {{https://doi.org/10.1162/neco\_a\_01367}}.

\bibitem[Göltz {et~al.}(2021)Göltz, Kriener, Baumbach, Billaudelle,
  Breitwieser, Cramer, Dold, Kungl, Senn, Schemmel, Meier, and
  Petrovici]{goltz_fast_2021}
Göltz, J.; Kriener, L.; Baumbach, A.; Billaudelle, S.; Breitwieser, O.;
  Cramer, B.; Dold, D.; Kungl, A.F.; Senn, W.; Schemmel, J.;  et~al.
\newblock Fast and energy-efficient neuromorphic deep learning with first-spike
  times. \emph{arXiv}  \textbf{2021}, arXiv:1912.11443.


\bibitem[Kheradpisheh {et~al.}(2018)Kheradpisheh, Ganjtabesh, Thorpe, and
  Masquelier]{kheradpisheh_stdp-based_2018}
Kheradpisheh, S.R.; Ganjtabesh, M.; Thorpe, S.J.; Masquelier, T.
\newblock {STDP}-based spiking deep convolutional neural networks for object
  recognition.
\newblock {\em Neural Netw.} {\bf 2018}, {\em 99},~56--67.
\newblock {{https://doi.org/10.1016/j.neunet.2017.12.005}}.

\bibitem[Tavanaei {et~al.}(2018)Tavanaei, Masquelier, and
  Maida]{tavanaei_representation_2018}
Tavanaei, A.; Masquelier, T.; Maida, A.
\newblock Representation learning using event-based {STDP}.
\newblock {\em Neural Netw.} {\bf 2018}, {\em 105},~294--303.
\newblock {{https://doi.org/10.1016/j.neunet.2018.05.018}}.

\bibitem[Gallego {et~al.}(2022)Gallego, Delbruck, Orchard, Bartolozzi, Taba,
  Censi, Leutenegger, Davison, Conradt, Daniilidis, and
  Scaramuzza]{gallego_event-based_2022}
Gallego, G.; Delbruck, T.; Orchard, G.; Bartolozzi, C.; Taba, B.; Censi, A.;
  Leutenegger, S.; Davison, A.J.; Conradt, J.; Daniilidis, K.;  et~al.
\newblock Event-{Based} {Vision}: {A} {Survey}.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}
  {\bf 2022}, {\em 44},~154--180.
\newblock {{https://doi.org/10.1109/TPAMI.2020.3008413}}.

\bibitem[Maunsell and Van~Essen(1983)]{maunsell_functional_1983}
Maunsell, J.H.; Van~Essen, D.C.
\newblock Functional properties of neurons in middle temporal visual area of
  the macaque monkey. {I}. {Selectivity} for stimulus direction, speed, and
  orientation.
\newblock {\em J. Neurophysiol.} {\bf 1983}, {\em 49},~1127--1147.
\newblock {{https://doi.org/10.1152/jn.1983.49.5.1127}}.

\bibitem[Montemurro {et~al.}(2008)Montemurro, Rasch, Murayama, Logothetis,
  and Panzeri]{montemurro_phase--firing_2008}
Montemurro, M.A.; Rasch, M.J.; Murayama, Y.; Logothetis, N.K.; Panzeri, S.
\newblock Phase-of-{Firing} {Coding} of {Natural} {Visual} {Stimuli} in
  {Primary} {Visual} {Cortex}.
\newblock {\em Curr. Biol.} {\bf 2008}, {\em 18},~375--380.
%\newblock Publisher: Elsevier,
  {{https://doi.org/10.1016/j.cub.2008.02.023}}.

\bibitem[deCharms and Merzenich(1996)]{decharms_primary_1996}
deCharms, R.C.; Merzenich, M.M.
\newblock Primary cortical representation of sounds by the coordination of
  action-potential timing.
\newblock {\em Nature} {\bf 1996}, {\em 381},~610--613.
%\newblock Number: 6583 Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/381610a0}}.

\bibitem[Vinje and Gallant(2000)]{vinje_sparse_2000}
Vinje, W.E.; Gallant, J.L.
\newblock Sparse {Coding} and {Decorrelation} in {Primary} {Visual} {Cortex}
  {During} {Natural} {Vision}.
\newblock {\em Science} {\bf 2000}, {\em 287}, 1273--1276.
%\newblock Publisher: American Association for the Advancement of Science,
  {{https://doi.org/10.1126/science.287.5456.1273}}.

\bibitem[Abeles(1991)]{abeles_corticonics_1991}
Abeles, M.
\newblock {\em Corticonics: Neural Circuits of the Cerebral Cortex}; Cambridge
  University Press: Cambridge, MA, USA; New York, NY, USA, 1991.

\bibitem[Paugam-Moisy and Bohte(2012)]{paugam-moisy_computing_2012}
Paugam-Moisy, H.; Bohte, S.M.
\newblock Computing with spiking neuron networks. In {\em Handbook of Natural
  Computing}; Springer:  \hl{Berlin/Heidelberg, Germany,} %newly added information, please confirm
  2012.

\bibitem[Hebb(1949)]{hebb_organization_1949}
Hebb, D.O.
\newblock {\em The Organization of Behavior: {A} Neuropsychological Theory};
  Wiley: New York, NY, USA, 1949.

\bibitem[Harris {et~al.}(2003)Harris, Csicsvari, Hirase, Dragoi, and
  Buzsáki]{harris_organization_2003}
Harris, K.D.; Csicsvari, J.; Hirase, H.; Dragoi, G.; Buzsáki, G.
\newblock Organization of cell assemblies in the hippocampus.
\newblock {\em Nature} {\bf 2003}, {\em 424},~552--556.
\newblock {{https://doi.org/10.1038/nature01834}}.

\bibitem[Singer and Gray(1995)]{singer_visual_1995}
Singer, W.; Gray, C.M.
\newblock Visual {Feature} {Integration} and the {Temporal} {Correlation}
  {Hypothesis}.
\newblock {\em Annu. Rev. Neurosci.} {\bf 1995}, {\em 18},~555--586.
\newblock {{https://doi.org/10.1146/annurev.ne.18.030195.003011}}.

\bibitem[Roelfsema {et~al.}(1997)Roelfsema, Engel, König, and
  Singer]{roelfsema_visuomotor_1997}
Roelfsema, P.R.; Engel, A.K.; König, P.; Singer, W.
\newblock Visuomotor integration is associated with zero time-lag
  synchronization among cortical areas.
\newblock {\em Nature} {\bf 1997}, {\em 385},~157--161.
\newblock {{https://doi.org/10.1038/385157a0}}.

\bibitem[Riehle {et~al.}(1997)Riehle, Grun, Diesmann, and
  Aertsen]{riehle_spike_1997}
Riehle, A.; Grun, S.; Diesmann, M.; Aertsen, A.
\newblock Spike synchronization and rate modulation differentially involved in
  motor cortical function.
\newblock {\em Science} {\bf 1997}, {\em 278},~1950--1953.
%\newblock Publisher: American Association for the Advancement of Science,
  {{https://doi.org/10.1126/science.278.5345.1950}}.

\bibitem[Kilavik {et~al.}(2009)Kilavik, Roux, Ponce-Alvarez, Confais, Grun,
  and Riehle]{kilavik_long-term_2009}
Kilavik, B.E.; Roux, S.; Ponce-Alvarez, A.; Confais, J.; Grun, S.; Riehle, A.
\newblock Long-{Term} {Modifications} in {Motor} {Cortical} {Dynamics}
  {Induced} by {Intensive} {Practice}.
\newblock {\em J. Neurosci.} {\bf 2009}, {\em 29},~12653--12663.
\newblock {{https://doi.org/10.1523/jneurosci.1554-09.2009}}.

\bibitem[Grammont and Riehle(2003)]{grammont_spike_2003}
Grammont, F.; Riehle, A.
\newblock Spike synchronization and firing rate in a population of motor
  cortical neurons in relation to movement direction and reaction time.
\newblock {\em Biol. Cybern.} {\bf 2003}, {\em 88},~360--373.
\newblock {{https://doi.org/10.1007/s00422-002-0385-3}}.

\bibitem[Denker {et~al.}(2018)Denker, Zehl, Kilavik, Diesmann, Brochier,
  Riehle, and Grün]{denker_lfp_2018}
Denker, M.; Zehl, L.; Kilavik, B.E.; Diesmann, M.; Brochier, T.; Riehle, A.;
  Grün, S.
\newblock {LFP} beta amplitude is linked to mesoscopic spatio-temporal phase
  patterns.
\newblock {\em Sci. Rep.} {\bf 2018}, {\em 8}.
\newblock {{https://doi.org/10.1038/s41598-018-22990-7}}.

\bibitem[Torre {et~al.}(2016)Torre, Canova, Denker, Gerstein, Helias, and
  Grün]{torre_asset_2016}
Torre, E.; Canova, C.; Denker, M.; Gerstein, G.; Helias, M.; Grün, S.
\newblock {ASSET}: {Analysis} of {Sequences} of {Synchronous} {Events} in
  {Massively} {Parallel} {Spike} {Trains}.
\newblock {\em PLOS Comput. Biol.} {\bf 2016}, {\em 12},~e1004939.
\newblock {{https://doi.org/10.1371/journal.pcbi.1004939}}.

\bibitem[Ben-yishai and Hansel(1997)]{ben-yishai_traveling_1997}
Ben-yishai, R.; Hansel, D.
\newblock Traveling {Waves} and the {Processing} of {Weakly} {Tuned} {Inputs}
  in a {Cortical} {Network} {Module}.  \emph{J. Comput. Neurosci.} {\bf 1997}.
\newblock {\em 77},~57--77.


\bibitem[Bruno and Sakmann(2006)]{bruno_cortex_2006}
Bruno, R.M.; Sakmann, B.
\newblock Cortex {Is} {Driven} by {Weak} but {Synchronously} {Active}
  {Thalamocortical} {Synapses}.
\newblock {\em Science} {\bf 2006}, {\em 312},~1622--1627.
\newblock {{https://doi.org/10.1126/science.1124593}}.

\bibitem[Deneve(2004)]{deneve_bayesian_2004}
Deneve, S.
\newblock Bayesian inference in spiking neurons.
\newblock In \emph{Proceedings of the Advances in {Neural} {Information} {Processing}
  {Systems}}; MIT Press:  \hl{Cambridge, MA, USA,} %newly added information, please confirm
  2004, Volume~17.

\bibitem[Ballard and Jehee(2011)]{ballard_dual_2011}
Ballard, D.; Jehee, J.
\newblock Dual {Roles} for {Spike} {Signaling} in {Cortical} {Neural}
  {Populations}.
\newblock {\em Front. Comput. Neurosci.} {\bf 2011}, {\em 5}, 22.

\bibitem[Gewaltig {et~al.}(2001)Gewaltig, Diesmann, and
  Aertsen]{gewaltig_propagation_2001}
Gewaltig, M.O.; Diesmann, M.; Aertsen, A.
\newblock Propagation of cortical synfire activity: survival probability in
  single trials and stability in the mean.
\newblock {\em Neural Netw.} {\bf 2001}, {\em 14},~657--673.
\newblock {{https://doi.org/10.1016/S0893-6080(01)00070-3}}.

\bibitem[Gerstner(1995)]{gerstner_time_1995}
Gerstner, W.
\newblock Time structure of the activity in neural network models.
\newblock {\em Phys. Rev. E} {\bf 1995}, {\em 51},~738--758.\linebreak
\newblock {{https://doi.org/10.1103/physreve.51.738}}.

\bibitem[Azouz and Gray(2008)]{azouz_stimulus-selective_2008}
Azouz, R.; Gray, C.M.
\newblock Stimulus-selective spiking is driven by the relative timing of
  synchronous excitation and disinhibition in cat striate neurons {in
  vivo}.
\newblock {\em Eur. J. Neurosci.} {\bf 2008}, {\em
  28},~1286--1300.
\newblock {{https://doi.org/10.1111/j.1460-9568.2008.06434.x}}.

\bibitem[Kremkow {et~al.}(2016)Kremkow, Perrinet, Monier, Alonso, Aertsen,
  Frégnac, and Masson]{kremkow_push-pull_2016}
Kremkow, J.; Perrinet, L.U.; Monier, C.; Alonso, J.M.; Aertsen, A.; Frégnac,
  Y.; Masson, G.S.
\newblock Push-{Pull} {Receptive} {Field} {Organization} and {Synaptic}
  {Depression}: {Mechanisms} for {Reliably} {Encoding} {Naturalistic} {Stimuli}
  in {V1}.
\newblock {\em Front. Neural Circuits} {\bf 2016}, {\em 10}, 37.
\newblock {{https://doi.org/10.3389/fncir.2016.00037}}.

\bibitem[Aviel {et~al.}(2003)Aviel, Mehring, Abeles, and
  Horn]{aviel_embedding_2003}
Aviel, Y.; Mehring, C.; Abeles, M.; Horn, D.
\newblock On {Embedding} {Synfire} {Chains} in a {Balanced} {Network}.
\newblock {\em Neural Comput.} {\bf 2003}, {\em 15},~1321--1340.
\newblock {{https://doi.org/10.1162/089976603321780290}}.

\bibitem[Kremkow {et~al.}(2010)Kremkow, Perrinet, Masson, and
  Aertsen]{kremkow_functional_2010}
Kremkow, J.; Perrinet, L.U.; Masson, G.S.; Aertsen, A.
\newblock Functional consequences of correlated excitatory and inhibitory
  conductances in cortical networks.
\newblock {\em J. Comput. Neurosci.} {\bf 2010}, {\em
  28},~579--594.
\newblock {{https://doi.org/10.1007/s10827-010-0240-9}}.

\bibitem[Davison(2008)]{davison_pynn_2008}
Davison, A.P.
\newblock {PyNN}: A common interface for neuronal network simulators.
\newblock {\em Front. Neuroinformatics} {\bf 2008}, {\em 2}, 11.
\newblock {{https://doi.org/10.3389/neuro.11.011.2008}}.

\bibitem[Pfeil {et~al.}(2013)Pfeil, Grübl, Jeltsch, Müller, Müller,
  Petrovici, Schmuker, Brüderle, Schemmel, and Meier]{pfeil_six_2013}
Pfeil, T.; Grübl, A.; Jeltsch, S.; Müller, E.; Müller, P.; Petrovici, M.A.;
  Schmuker, M.; Brüderle, D.; Schemmel, J.; Meier, K.
\newblock Six {Networks} on a {Universal} {Neuromorphic} {Computing}
  {Substrate}.
\newblock {\em Front. Neurosci.} {\bf 2013}, {\em 7}, 11.
\newblock {{https://doi.org/10.3389/fnins.2013.00011}}.

\bibitem[Schrader {et~al.}(2008)Schrader, Grün, Diesmann, and
  Gerstein]{schrader_detecting_2008}
Schrader, S.; Grün, S.; Diesmann, M.; Gerstein, G.L.
\newblock Detecting {Synfire} {Chain} {Activity} {Using} {Massively} {Parallel}
  {Spike} {Train} {Recording}.
\newblock {\em J. Neurophysiol.} {\bf 2008}, {\em 100},~2165--2176.
\newblock {{https://doi.org/10.1152/jn.01245.2007}}.

\bibitem[Grammont and Riehle(1999)]{grammont_precise_1999}
Grammont, F.; Riehle, A.
\newblock Precise spike synchronization in monkey motor cortex involved in
  preparation for movement.
\newblock {\em Exp. Brain Res.} {\bf 1999}, {\em 128},~118--122.
\newblock {{https://doi.org/10.1007/s002210050826}}.

\bibitem[Brette(2012)]{brette_computing_2012}
Brette, R.
\newblock Computing with {Neural} {Synchrony}.
\newblock {\em PLoS Comput. Biol.} {\bf 2012}, {\em 8},~e1002561.
\newblock {{https://doi.org/10.1371/journal.pcbi.1002561}}.

\bibitem[Fries(2005)]{fries_mechanism_2005}
Fries, P.
\newblock A mechanism for cognitive dynamics: neuronal communication through
  neuronal coherence.
\newblock {\em Trends Cogn. Sci.} {\bf 2005}, {\em 9},~474--480.
\newblock {{https://doi.org/10.1016/j.tics.2005.08.011}}.

\bibitem[VanRullen {et~al.}(2006)VanRullen, Reddy, and
  Koch]{vanrullen_continuous_2006}
VanRullen, R.; Reddy, L.; Koch, C.
\newblock The {Continuous} {Wagon} {Wheel} {Illusion} {Is} {Associated} with
  {Changes} in {Electroencephalogram} {Power} at 13 {Hz}.
\newblock {\em J. Neurosci.} {\bf 2006}, {\em 26},~502--507.
 {{https://doi.org/10.1523/jneurosci.4654-05.2006}}.

\bibitem[Dugue {et~al.}(2011)Dugue, Marque, and VanRullen]{dugue_phase_2011}
Dugue, L.; Marque, P.; VanRullen, R.
\newblock The {Phase} of {Ongoing} {Oscillations} {Mediates} the {Causal}
  {Relation} between {Brain} {Excitation} and {Visual} {Perception}.
\newblock {\em J. Neurosci.} {\bf 2011}, {\em 31},~11889--11893.
\newblock {{https://doi.org/10.1523/JNEUROSCI.1161-11.2011}}.

\bibitem[Bringuier {et~al.}(1999)Bringuier, Chavane, Glaeser, and
  Frégnac]{bringuier_horizontal_1999}
Bringuier, V.; Chavane, F.; Glaeser, L.; Frégnac, Y.
\newblock Horizontal {Propagation} of {Visual} {Activity} in the {Synaptic}
  {Integration} {Field} of {Area} 17 {Neurons}.
\newblock {\em Science} {\bf 1999}, {\em 283},~695--699.
%\newblock 00535 Loaded from an external bibliography file by Manubot.
%  source\_bibliography: manual-references.bib standard\_id: Bringuier99,
  {{https://doi.org/10.1126/science.283.5402.695}}.

\bibitem[Benvenuti {et~al.}(2020)Benvenuti, Chemla, Boonman, Perrinet,
  Masson, and Chavane]{benvenuti_anticipatory_2020}
Benvenuti, G.; Chemla, S.; Boonman, A.; Perrinet, L.U.; Masson, G.S.; Chavane,
  F.
\newblock Anticipatory responses along motion trajectories in awake monkey area
  {V1}.
\newblock {\em bioRxiv  Prepr. Serv. Biol.} {\bf 2020}.
\newblock {{https://doi.org/10.1101/2020.03.26.010017}}.

\bibitem[Le~Bec {et~al.}(2022)Le~Bec, Troncoso, Desbois, Passarelli, Baudot,
  Monier, Pananceau, and Frégnac]{le_bec_horizontal_2022}
Le~Bec, B.; Troncoso, X.G.; Desbois, C.; Passarelli, Y.; Baudot, P.; Monier,
  C.; Pananceau, M.; Frégnac, Y.
\newblock Horizontal connectivity in {V1}: {Prediction} of coherence in contour
  and motion integration.
\newblock {\em PLoS ONE} {\bf 2022}, {\em 17},~e0268351.
\newblock {{https://doi.org/10.1371/journal.pone.0268351}}.

\bibitem[Feller {et~al.}(1997)Feller, Butts, Aaron, Rokhsar, and
  Shatz]{feller_dynamic_1997}
Feller, M.B.; Butts, D.A.; Aaron, H.L.; Rokhsar, D.S.; Shatz, C.J.
\newblock Dynamic {Processes} {Shape} {Spatiotemporal} {Properties} of
  {Retinal} {Waves}.
\newblock {\em Neuron} {\bf 1997}, {\em 19},~293--306.
\newblock {{https://doi.org/10.1016/S0896-6273(00)80940-X}}.

\bibitem[Bienenstock(1995)]{bienenstock_model_1995}
Bienenstock, E.
\newblock A model of neocortex.
\newblock {\em Netw. Comput. Neural Syst.} {\bf 1995}, {\em
  6},~179--224.
%\newblock Publisher: Taylor \& Francis \_eprint:
%  https://doi.org/10.1088/0954-898X\_6\_2\_004,
  {{https://doi.org/10.1088/0954-898X\_6\_2\_004}}.

\bibitem[Muller {et~al.}(2014)Muller, Reynaud, Chavane, and
  Destexhe]{muller_stimulus-evoked_2014}
Muller, L.; Reynaud, A.; Chavane, F.; Destexhe, A.
\newblock The stimulus-evoked population response in visual cortex of awake
  monkey is a propagating wave.
\newblock {\em Nat. Commun.} {\bf 2014}, {\em 5},~3675.
%\newblock 00068 Loaded from an external bibliography file by Manubot.
%  source\_bibliography: manual-references.bib standard\_id: Muller14,
  {{https://doi.org/10.1038/ncomms4675}}.

\bibitem[Muller {et~al.}(2018)Muller, Chavane, Reynolds, and
  Sejnowski]{muller_cortical_2018}
Muller, L.; Chavane, F.; Reynolds, J.; Sejnowski, T.J.
\newblock Cortical travelling waves: {Mechanisms} and computational principles.
\newblock {\em Nat. Rev. Neurosci.} {\bf 2018}, \emph{19}, 255--268.
%\newblock Loaded from an external bibliography file by Manubot.
%  source\_bibliography: manual-references.bib standard\_id: Muller18,
  {{https://doi.org/10.1038/nrn.2018.20}}.

\bibitem[Lindén {et~al.}(2022)Lindén, Petersen, Vestergaard, and
  Berg]{linden_movement_2022}
Lindén, H.; Petersen, P.C.; Vestergaard, M.; Berg, R.W.
\newblock Movement is governed by rotational neural dynamics in spinal motor
  networks.
\newblock {\em Nature} {\bf 2022}, {\em 610},~526--531.
\newblock {{https://doi.org/10.1038/s41586-022-05293-w}}.

\bibitem[Chemla {et~al.}(2019)Chemla, Reynaud, diVolo, Zerlaut, Perrinet,
  Destexhe, and Chavane]{chemla_suppressive_2019}
Chemla, S.; Reynaud, A.; diVolo, M.; Zerlaut, Y.; Perrinet, L.U.; Destexhe, A.;
  Chavane, F.Y.
\newblock Suppressive waves disambiguate the representation of long-range
  apparent motion in awake monkey {V1}.
\newblock {\em J. Neurosci.} {\bf 2019}, {\em 2792},~18.
%\newblock Loaded from an external bibliography file by Manubot.
%  source\_bibliography: manual-references.bib standard\_id: Chemla19,
  {{https://doi.org/10.1523/JNEUROSCI.2792-18.2019}}.

\bibitem[Pillow {et~al.}(2008)Pillow, Shlens, Paninski, Sher, Litke,
  Chichilnisky, and Simoncelli]{pillow_spatio-temporal_2008}
Pillow, J.W.; Shlens, J.; Paninski, L.; Sher, A.; Litke, A.M.; Chichilnisky,
  E.J.; Simoncelli, E.P.
\newblock Spatio-temporal correlations and visual signalling in a complete
  neuronal population.
\newblock {\em Nature} {\bf 2008}, {\em 454},~995--999.
\newblock {{https://doi.org/10.1038/nature07140}}.

\bibitem[Schneidman {et~al.}(2006)Schneidman, Berry, Segev, and
  Bialek]{schneidman_weak_2006}
Schneidman, E.; Berry, M.J.; Segev, R.; Bialek, W.
\newblock Weak pairwise correlations imply strongly correlated network states
  in a neural population.
\newblock {\em Nature} {\bf 2006}, {\em 440},~1007--1012.
\newblock {{https://doi.org/10.1038/nature04701}}.

\bibitem[Berry(2022)]{berry_spike_2022}
Puchalla, J.; Berry, M.J.
\newblock \emph{Spike {Trains} of {Retinal} {Ganglion} {Cells} {Viewing} a {Repeated}
  {Natural} {Movie}}; \hl{Princeton University: Princeton, NJ, USA,} %MDPI: We added the name of the publisher and location. Please confirm.
  {2022}.
\newblock {{https://doi.org/10.34770/V0V4-3H52}}.

\bibitem[Miller {et~al.}(2014)Miller, Ayzenshtat, Carrillo-Reid, and
  Yuste]{miller_visual_2014}
Miller, J.e.K.; Ayzenshtat, I.; Carrillo-Reid, L.; Yuste, R.
\newblock Visual stimuli recruit intrinsically generated cortical ensembles.
\newblock {\em Proc. Natl. Acad. Sci. USA} {\bf 2014},
  {\em 111}, E4053--E4061.
\newblock {{https://doi.org/10.1073/pnas.1406077111}}.

\bibitem[Ikegaya {et~al.}(2004)Ikegaya, Aaron, Cossart, Aronov, Lampl,
  Ferster, and Yuste]{ikegaya_synfire_2004}
Ikegaya, Y.; Aaron, G.; Cossart, R.; Aronov, D.; Lampl, I.; Ferster, D.; Yuste,
  R.
\newblock Synfire {Chains} and {Cortical} {Songs}: {Temporal} {Modules} of
  {Cortical} {Activity}.
\newblock {\em Science} {\bf 2004}, {\em 304},~559--564.
\newblock {{https://doi.org/10.1126/science.1093173}}.

\bibitem[Luczak {et~al.}(2007)Luczak, Barthó, Marguet, Buzsáki, and
  Harris]{luczak_sequential_2007}
Luczak, A.; Barthó, P.; Marguet, S.L.; Buzsáki, G.; Harris, K.D.
\newblock Sequential structure of neocortical spontaneous activity in vivo.
\newblock {\em Proc. Natl. Acad. Sci. USA} {\bf 2007},
  {\em 104},~347--352.
\newblock {{https://doi.org/10.1073/pnas.0605643104}}.

\bibitem[Pastalkova {et~al.}(2008)Pastalkova, Itskov, Amarasingham, and
  Buzsáki]{pastalkova_internally_2008}
Pastalkova, E.; Itskov, V.; Amarasingham, A.; Buzsáki, G.
\newblock Internally {Generated} {Cell} {Assembly} {Sequences} in the {Rat}
  {Hippocampus}.
\newblock {\em Science} {\bf 2008}, {\em 321},~1322--1327.
\newblock {{https://doi.org/10.1126/science.1159775}}.

\bibitem[Villette {et~al.}(2015)Villette, Malvache, Tressard, Dupuy, and
  Cossart]{villette_internally_2015}
Villette, V.; Malvache, A.; Tressard, T.; Dupuy, N.; Cossart, R.
\newblock Internally {Recurring} {Hippocampal} {Sequences} as a {Population}
  {Template} of {Spatiotemporal} {Information}.
\newblock {\em Neuron} {\bf 2015}, {\em 88},~357--366.
 {{https://doi.org/10.1016/j.neuron.2015.09.052}}.

\bibitem[Branco {et~al.}(2010)Branco, Clark, and
  Häusser]{branco_dendritic_2010}
Branco, T.; Clark, B.A.; Häusser, M.
\newblock Dendritic {Discrimination} of {Temporal} {Input} {Sequences} in
  {Cortical} {Neurons}.
\newblock {\em Science} {\bf 2010}, {\em 329},~1671--1675.
\newblock {{https://doi.org/10.1126/science.1189664}}.

\bibitem[Luczak {et~al.}(2015)Luczak, McNaughton, and
  Harris]{luczak_packet-based_2015}
Luczak, A.; McNaughton, B.L.; Harris, K.D.
\newblock Packet-based communication in the cortex.
\newblock {\em Nat. Rev. Neurosci.} {\bf 2015}, {\em 16},~745--755.
\newblock {{https://doi.org/10.1038/nrn4026}}.

\bibitem[Meister {et~al.}(1995)Meister, Lagnado, and
  Baylor]{meister_concerted_1995}
Meister, M.; Lagnado, L.; Baylor, D.A.
\newblock Concerted {Signaling} by {Retinal} {Ganglion} {Cells}.
\newblock {\em Science} {\bf 1995}, {\em 270},~1207--1210.
\newblock {{https://doi.org/10.1126/science.270.5239.1207}}.

\bibitem[Cleland(2014)]{cleland_construction_2014}
Cleland, T.A.
\newblock Construction of {Odor} {Representations} by {Olfactory} {Bulb}
  {Microcircuits}. In {\em Progress in {Brain} {Research}}; Elsevier:  \hl{Amsterdam, The Netherlands,} %newly added information, please confirm
  2014;
  Volume 208, pp. 177--203.
\newblock {{https://doi.org/10.1016/B978-0-444-63350-7.00007-3}}.

\bibitem[Kashiwadani {et~al.}(1999)Kashiwadani, Sasaki, Uchida, and
  Mori]{kashiwadani_synchronized_1999}
Kashiwadani, H.; Sasaki, Y.F.; Uchida, N.; Mori, K.
\newblock Synchronized {Oscillatory} {Discharges} of {Mitral}/{Tufted} {Cells}
  {With} {Different} {Molecular} {Receptive} {Ranges} in the {Rabbit}
  {Olfactory} {Bulb}.
\newblock {\em J. Neurophysiol.} {\bf 1999}, {\em 82},~1786--1792.\linebreak
%\newblock Publisher: American Physiological Society,
  {{https://doi.org/10.1152/jn.1999.82.4.1786}}.

\bibitem[Rinberg {et~al.}(2006)Rinberg, Koulakov, and
  Gelperin]{rinberg_speed-accuracy_2006}
Rinberg, D.; Koulakov, A.; Gelperin, A.
\newblock Speed-{Accuracy} {Tradeoff} in {Olfaction}.
\newblock {\em Neuron} {\bf 2006}, {\em 51},~351--358.\linebreak
\newblock {{https://doi.org/10.1016/j.neuron.2006.07.013}}.

\bibitem[Johansson and Birznieks(2004)]{johansson_first_2004}
Johansson, R.S.; Birznieks, I.
\newblock First spikes in ensembles of human tactile afferents code complex
  spatial fingertip events.
\newblock {\em Nat. Neurosci.} {\bf 2004}, {\em 7},~170--177.
\newblock {{https://doi.org/10.1038/nn1177}}.

\bibitem[Buzsáki and Tingley(2018)]{buzsaki_space_2018}
Buzsáki, G.; Tingley, D.
\newblock Space and {Time}: {The} {Hippocampus} as a {Sequence} {Generator}.
\newblock {\em Trends Cogn. Sci.} {\bf 2018}, {\em 22},~853--869.
\newblock {{https://doi.org/10.1016/j.tics.2018.07.006}}.

\bibitem[Malvache {et~al.}(2016)Malvache, Reichinnek, Villette, Haimerl, and
  Cossart]{malvache_awake_2016}
Malvache, A.; Reichinnek, S.; Villette, V.; Haimerl, C.; Cossart, R.
\newblock Awake hippocampal reactivations project onto orthogonal neuronal
  assemblies.
\newblock {\em Science} {\bf 2016}, {\em 353},~1280--1283.
 {{https://doi.org/10.1126/science.aaf3319}}.

\bibitem[Haimerl {et~al.}(2019)Haimerl, Angulo-Garcia, Villette, Reichinnek,
  Torcini, Cossart, and Malvache]{haimerl_internal_2019}
Haimerl, C.; Angulo-Garcia, D.; Villette, V.; Reichinnek, S.; Torcini, A.;
  Cossart, R.; Malvache, A.
\newblock Internal representation of hippocampal neuronal population spans a
  time-distance continuum.
\newblock {\em Proc. Natl. Acad. Sci. USA} {\bf 2019},
  {\em 116},~7477--7482.
\newblock {{https://doi.org/10.1073/pnas.1718518116}}.

\bibitem[Agus {et~al.}(2010)Agus, Thorpe, and Pressnitzer]{agus_rapid_2010}
Agus, T.R.; Thorpe, S.J.; Pressnitzer, D.
\newblock Rapid {Formation} of {Robust} {Auditory} {Memories}: {Insights} from
  {Noise}.
\newblock {\em Neuron} {\bf 2010}, {\em 66},~610--618.
\newblock {{https://doi.org/10.1016/j.neuron.2010.04.014}}.

\bibitem[Izhikevich(2006)]{izhikevich_polychronization_2006}
Izhikevich, E.M.
\newblock Polychronization: {Computation} with {Spikes}.
\newblock {\em Neural Comput.} {\bf 2006}, {\em 18},~245--282.\linebreak
 {{https://doi.org/10.1162/089976606775093882}}.

\bibitem[Simoncelli {et~al.}(2003)Simoncelli, Paninski, Pillow, and
  Schwartz]{simoncelli_characterization_2003}
Simoncelli, E.P.; Paninski, L.; Pillow, J.; Schwartz, O.
\newblock Characterization of {Neural} {Responses} with {Stochastic} {Stimuli}. In \emph{\hl{The New Cognitive Neurosciences}}, \hl{3rd ed.; Gazzaniga, M., Ed.; MIT Press: Cambridge, MA, USA,} %MDPI: Newly added  information. Please confirm.
  {2003}.

\bibitem[Jazayeri and Movshon(2006)]{jazayeri_optimal_2006}
Jazayeri, M.; Movshon, J.A.
\newblock Optimal representation of sensory information by neural populations.
\newblock {\em Nat. Neurosci.} {\bf 2006}, {\em 9},~690--696.
%\newblock Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/nn1691}}.

\bibitem[Berens {et~al.}(2012)Berens, Ecker, Cotton, Ma, Bethge, and
  Tolias]{berens_fast_2012}
Berens, P.; Ecker, A.S.; Cotton, R.J.; Ma, W.J.; Bethge, M.; Tolias, A.S.
\newblock A {Fast} and {Simple} {Population} {Code} for {Orientation} in
  {Primate} {V1}.
\newblock {\em J. Neurosci.} {\bf 2012}, {\em 32},~10618--10626.
\newblock {{https://doi.org/10.1523/jneurosci.1335-12.2012}}.

\bibitem[Bellec {et~al.}(2021)Bellec, Wang, Modirshanechi, Brea, and
  Gerstner]{bellec_fitting_2021}
Bellec, G.; Wang, S.; Modirshanechi, A.; Brea, J.; Gerstner, W.
\newblock Fitting summary statistics of neural data with a differentiable
  spiking network simulator. \emph{arXiv} \textbf{2021},  	arXiv:2106.10064.
%\newblock Technical Report 2106.10064, arXiv,  .

\bibitem[Kohn and Smith(2016)]{kohn_utah_2016}
Kohn, A.; Smith, M.
\newblock Utah Array Extracellular Recordings of Spontaneous and Visually
  Evoked Activity from Anesthetized Macaque Primary Visual Cortex ({V1}). 
  2016.   
Available online: {\url{https://doi.org/10.6080/K0NC5Z4X}}  \hl{(accessed on date month year).} %MDPI: Please add the access date (format: Date Month Year), e.g., accessed on 1 January 2020.


\bibitem[Warner {et~al.}(2022)Warner, Ruda, and
  Sommer]{warner_probabilistic_2022}
Warner, C.; Ruda, K.; Sommer, F.T.
\newblock A probabilistic latent variable model for detecting structure in
  binary data. \emph{arXiv} \textbf{2022}, arXiv:2201.11108.
%\newblock {\em arXiv:2201.11108 [cs, q-bio, stat]} {\bf }.
%\newblock arXiv: 2201.11108.

\bibitem[Victor and Purpura(1996)]{victor_nature_1996}
Victor, J.D.; Purpura, K.P.
\newblock Nature and precision of temporal coding in visual cortex: A
  metric-space analysis.
\newblock {\em J. Neurophysiol.} {\bf 1996}, {\em 76},~1310--1326.
\newblock {{https://doi.org/10.1152/jn.1996.76.2.1310}}.

\bibitem[van Rossum(2001)]{van_rossum_novel_2001}
van Rossum, M.C.
\newblock A novel spike distance.
\newblock {\em Neural Comput.} {\bf 2001}, {\em 13},~751--763.
\newblock {{https://doi.org/10.1162/089976601300014321}}.

\bibitem[Kreuz {et~al.}(2007)Kreuz, Haas, Morelli, Abarbanel, and
  Politi]{kreuz_measuring_2007}
Kreuz, T.; Haas, J.S.; Morelli, A.; Abarbanel, H.D.I.; Politi, A.
\newblock Measuring spike train synchrony.
\newblock {\em J. Neurosci. Methods} {\bf 2007}, {\em
  165},~151--161.
\newblock {{https://doi.org/10.1016/j.jneumeth.2007.05.031}}.

\bibitem[Moser and Natschlager(2014)]{moser_stability_2014}
Moser, B.A.; Natschlager, T.
\newblock On {Stability} of {Distance} {Measures} for {Event} {Sequences}
  {Induced} by {Level}-{Crossing} {Sampling}.
\newblock {\em IEEE Trans. Signal Process.} {\bf 2014}, {\em
  62},~1987--1999.
\newblock {{https://doi.org/10.1109/tsp.2014.2305642}}.

\bibitem[Weyl(1916)]{weyl_ueber_1916}
Weyl, H.
\newblock Ueber die {Gleichverteilung} von {Zahlen} mod. {Eins}.
\newblock {\em Math. Ann.} {\bf 1916}, {\em 77},~313--352.
\newblock {{https://doi.org/10.1007/bf01475864}}.

\bibitem[Aronov and Victor(2004)]{aronov_non-euclidean_2004}
Aronov, D.; Victor, J.D.
\newblock Non-{Euclidean} properties of spike train metric spaces.
\newblock {\em Phys. Rev. E} {\bf 2004}, {\em 69},~061905.
%\newblock Publisher: American Physical Society,
  {{https://doi.org/10.1103/PhysRevE.69.061905}}.

\bibitem[Levakova {et~al.}(2015)Levakova, Tamborrino, Ditlevsen, and
  Lansky]{levakova_review_2015}
Levakova, M.; Tamborrino, M.; Ditlevsen, S.; Lansky, P.
\newblock A review of the methods for neuronal response latency estimation.
\newblock {\em Biosystems} {\bf 2015}, {\em 136},~23--34.
\newblock {{https://doi.org/10.1016/j.biosystems.2015.04.008}}.

\bibitem[Grün {et~al.}(2002)Grün, Diesmann, and
  Aertsen]{grun_unitary_2002-1}
Grün, S.; Diesmann, M.; Aertsen, A.
\newblock Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking}
  {Activity}: {II}. {Nonstationary} {Data}.
\newblock {\em Neural Comput.} {\bf 2002}, {\em 14},~81--119.
\newblock {{https://doi.org/10.1162/089976602753284464}}.

\bibitem[Grün {et~al.}(2010)Grün, Diesmann, and
  Aertsen]{grun_unitary_2010}
Grün, S.; Diesmann, M.; Aertsen, A.
\newblock Unitary {Event} {Analysis}. In {\em Analysis of {Parallel} {Spike}
  {Trains}}; Springer: \hl{Boston, MA, USA,} %MDPI: We added the location of the publisher. Please confirm.
  2010; pp. 191--220.

\bibitem[Grün {et~al.}(2002)Grün, Diesmann, and
  Aertsen]{grun_unitary_2002}
Grün, S.; Diesmann, M.; Aertsen, A.
\newblock Unitary {Events} in {Multiple} {Single}-{Neuron} {Spiking}
  {Activity}: {I}. {Detection} and {Significance}.
\newblock {\em Neural Comput.} {\bf 2002}, {\em 14},~43--80.
\newblock {{https://doi.org/10.1162/089976602753284455}}.

\bibitem[Torre {et~al.}(2013)Torre, Picado-Muiño, Denker, Borgelt, and
  Grün]{torre_statistical_2013}
Torre, E.; Picado-Muiño, D.; Denker, M.; Borgelt, C.; Grün, S.
\newblock Statistical evaluation of synchronous spike patterns extracted by
  frequent item set mining.
\newblock {\em Front. Comput. Neurosci.} {\bf 2013}, {\em 7}, 132.
\newblock {{https://doi.org/10.3389/fncom.2013.00132}}.

\bibitem[Quaglio {et~al.}(2018)Quaglio, Rostami, Torre, and
  Grün]{quaglio_methods_2018}
Quaglio, P.; Rostami, V.; Torre, E.; Grün, S.
\newblock Methods for identification of spike patterns in massively parallel
  spike trains.
\newblock {\em Biol. Cybern.} {\bf 2018}, {\em 112},~57--80.
\newblock {{https://doi.org/10.1007/s00422-018-0755-0}}.

\bibitem[Stella {et~al.}(2019)Stella, Quaglio, Torre, and
  Grün]{stella_3d-spade_2019}
Stella, A.; Quaglio, P.; Torre, E.; Grün, S.
\newblock 3d-{SPADE}: {Significance} evaluation of spatio-temporal patterns of
  various temporal extents.
\newblock {\em Biosystems} {\bf 2019}, {\em 185},~104022.
\newblock {{https://doi.org/10.1016/j.biosystems.2019.104022}}.

\bibitem[Stella {et~al.}(2022)Stella, Bouss, Palm, and
  Grün]{stella_comparing_2022}
Stella, A.; Bouss, P.; Palm, G.; Grün, S.
\newblock Comparing {Surrogates} to {Evaluate} {Precisely} {Timed}
  {Higher}-{Order} {Spike} {Correlations}.
\newblock {\em eNeuro} {\bf 2022}, {\em 9},~ENEURO.0505--21.2022.
\newblock {{https://doi.org/10.1523/eneuro.0505-21.2022}}.

\bibitem[Grossberger {et~al.}(2018)Grossberger, Battaglia, and
  Vinck]{grossberger_unsupervised_2018}
Grossberger, L.; Battaglia, F.P.; Vinck, M.
\newblock Unsupervised clustering of temporal patterns in high-dimensional
  neuronal ensembles using a novel dissimilarity measure.
\newblock {\em PLoS Comput. Biol.} {\bf 2018}, {\em 14},~e1006283.
\newblock {{https://doi.org/10.1371/journal.pcbi.1006283}}.

\bibitem[Nádasdy {et~al.}(1999)Nádasdy, Hirase, Czurkó, Csicsvari, and
  Buzsáki]{nadasdy_replay_1999}
Nádasdy, Z.; Hirase, H.; Czurkó, A.; Csicsvari, J.; Buzsáki, G.
\newblock Replay and {Time} {Compression} of {Recurring} {Spike} {Sequences} in
  the {Hippocampus}.
\newblock {\em J. Neurosci.} {\bf 1999}, {\em 19},~9497--9507.
%\newblock Publisher: Society for Neuroscience Section: ARTICLE,
  {{https://doi.org/10.1523/JNEUROSCI.19-21-09497.1999}}.

\bibitem[Lee and Wilson(2004)]{lee_combinatorial_2004}
Lee, A.K.; Wilson, M.A.
\newblock A {Combinatorial} {Method} for {Analyzing} {Sequential} {Firing}
  {Patterns} {Involving} an {Arbitrary} {Number} of {Neurons} {Based} on
  {Relative} {Time} {Order}.
\newblock {\em J. Neurophysiol.} {\bf 2004}, {\em 92},~2555--2573.
\newblock {{https://doi.org/10.1152/jn.01030.2003}}.

\bibitem[Sotomayor-Gómez {et~al.}(2021)Sotomayor-Gómez, Battaglia, and
  Vinck]{sotomayor-gomez_spikeship_2021}
Sotomayor-Gómez, B.; Battaglia, F.P.; Vinck, M.
\newblock {SpikeShip}: {A} method for fast, unsupervised discovery of
  high-dimensional neural spiking patterns.
\newblock {\em bioRxiv  Prepr. Serv. Biol.} {\bf 2021}.
%\newblock Publisher: Cold Spring Harbor Laboratory,
  {{https://doi.org/10.1101/2020.06.03.131573}}.

\bibitem[Pachitariu {et~al.}(2018)Pachitariu, Stringer, and
  Harris]{pachitariu_robustness_2018}
Pachitariu, M.; Stringer, C.; Harris, K.D.
\newblock Robustness of {Spike} {Deconvolution} for {Neuronal} {Calcium}
  {Imaging}.
\newblock {\em  J. Neurosci.} {\bf 2018}, {\em 38},~7976--7985.
\newblock {{https://doi.org/10.1523/jneurosci.3339-17.2018}}.

\bibitem[Stringer(2020)]{stringer_mouselandrastermap_2020}
Stringer, C.
\newblock {MouseLand}/Rastermap: {A} Multi-Dimensional Embedding Algorithm.
  \hl{2020}. %MDPI: Please add website and access date. Please check.


\bibitem[Stringer {et~al.}(2019)Stringer, Pachitariu, Steinmetz, Reddy,
  Carandini, and Harris]{stringer_spontaneous_2019}
Stringer, C.; Pachitariu, M.; Steinmetz, N.; Reddy, C.B.; Carandini, M.;
  Harris, K.D.
\newblock Spontaneous behaviors drive multidimensional, brainwide activity.
\newblock {\em Science} {\bf 2019}, {\em 364}, 255.
\newblock {{https://doi.org/10.1126/science.aav7893}}.

\bibitem[Stringer {et~al.}(2021)Stringer, Michaelos, Tsyboulski, Lindo, and
  Pachitariu]{stringer_high-precision_2021}
Stringer, C.; Michaelos, M.; Tsyboulski, D.; Lindo, S.E.; Pachitariu, M.
\newblock High-precision coding in visual cortex.
\newblock {\em Cell} {\bf 2021}, {\em 184},~2767--2778.e15.
\newblock {{https://doi.org/10.1016/j.cell.2021.03.042}}.

\bibitem[Russo and Durstewitz(2017)]{russo_cell_2017}
Russo, E.; Durstewitz, D.
\newblock Cell assemblies at multiple time scales with arbitrary lag
  constellations.
\newblock {\em eLife} {\bf 2017}, {\em 6}, e19428.
\newblock {{https://doi.org/10.7554/elife.19428}}.

\bibitem[Pipa {et~al.}(2008)Pipa, Wheeler, Singer, and
  Nikolić]{pipa_neuroxidence_2008}
Pipa, G.; Wheeler, D.W.; Singer, W.; Nikolić, D.
\newblock {NeuroXidence}: Reliable and efficient analysis of an excess or
  deficiency of joint-spike events.
\newblock {\em J. Comput. Neurosci.} {\bf 2008}, {\em
  25},~64--88.
\newblock {{https://doi.org/10.1007/s10827-007-0065-3}}.

\bibitem[Torre {et~al.}(2016)Torre, Quaglio, Denker, Brochier, Riehle, and
  Grun]{torre_synchronous_2016}
Torre, E.; Quaglio, P.; Denker, M.; Brochier, T.; Riehle, A.; Grun, S.
\newblock Synchronous {Spike} {Patterns} in {Macaque} {Motor} {Cortex} during
  an {Instructed}-{Delay} {Reach}-to-{Grasp} {Task}.
\newblock {\em J. Neurosci.} {\bf 2016}, {\em 36},~8329--8340.
\newblock {{https://doi.org/10.1523/jneurosci.4375-15.2016}}.

\bibitem[Williams {et~al.}(2020)Williams, Degleris, Wang, and
  Linderman]{williams_point_2020}
Williams, A.H.; Degleris, A.; Wang, Y.; Linderman, S.W.
\newblock Point process models for sequence detection in high-dimensional
  neural spike trains. \emph{arXiv}  \textbf{2020},  	arXiv:2010.04875
%\newblock Technical Report 2010.04875, .

\bibitem[Kass {et~al.}(2005)Kass, Ventura, and Brown]{kass_statistical_2005}
Kass, R.E.; Ventura, V.; Brown, E.N.
\newblock Statistical issues in the analysis of neuronal data.
\newblock {\em J. Neurophysiol.} {\bf 2005}, {\em 94},~8--25.
%\newblock Publisher: American Physiological Society,
  {{https://doi.org/10.1152/jn.00648.2004}}.

\bibitem[van Kempen {et~al.}(2021)van Kempen, Gieselmann, Boyd, Steinmetz,
  Moore, Engel, and Thiele]{van_kempen_top-down_2021}
van Kempen, J.; Gieselmann, M.A.; Boyd, M.; Steinmetz, N.A.; Moore, T.; Engel,
  T.A.; Thiele, A.
\newblock Top-down coordination of local cortical state during selective
  attention.
\newblock {\em Neuron} {\bf 2021}, {\em 109},~894--904.e8.
\newblock {{https://doi.org/10.1016/j.neuron.2020.12.013}}.

\bibitem[Pasturel {et~al.}(2020)Pasturel, Montagnini, and
  Perrinet]{pasturel_humans_2020}
Pasturel, C.; Montagnini, A.; Perrinet, L.U.
\newblock Humans adapt their anticipatory eye movements to the volatility of
  visual motion properties.
\newblock {\em PLoS Comput. Biol.} {\bf 2020}, \emph{16}, e1007438.
\newblock {{https://doi.org/10.1371/journal.pcbi.1007438}}.

\bibitem[Von~Helmholz(1850)]{von_helmholz_messungen_1850}
Von~Helmholz, H.
\newblock Messungen über den zeitlichen {Verlauf} der {Zuckung} animalischer
  {Muskeln} und die {Fortpflanzungsgeschwindigkeit} der {Reizung} in den
  {Nerven}.
\newblock {\em Arch. Anat. Physiol. Wiss. Med.}
  {\bf 1850}, {\em 17},~176--364.

\bibitem[Peyrard(2020)]{peyrard_how_2020}
Peyrard, M.
\newblock How is information transmitted in a nerve?
\newblock {\em J. Biol. Phys.} {\bf 2020}, {\em 46},~327--341.
\newblock {{https://doi.org/10.1007/s10867-020-09557-2}}.

\bibitem[Young(1938)]{young_functioning_1938}
Young, J.Z.
\newblock The {Functioning} of the {Giant} {Nerve} {Fibres} of the {Squid}.
\newblock {\em J. Exp. Biol.} {\bf 1938}, {\em 15},~170--185.\linebreak
\newblock {{https://doi.org/10.1242/jeb.15.2.170}}.

\bibitem[Madadi~Asl {et~al.}(2018)Madadi~Asl, Valizadeh, and
  Tass]{madadi_asl_dendritic_2018}
Madadi~Asl, M.; Valizadeh, A.; Tass, P.A.
\newblock Dendritic and {Axonal} {Propagation} {Delays} {May} {Shape}
  {Neuronal} {Networks} {With} {Plastic} {Synapses}.
\newblock {\em Front. Physiol.} {\bf 2018}, {\em 9},~1849.
\newblock {{https://doi.org/10.3389/fphys.2018.01849}}.

\bibitem[Stetson {et~al.}(1992)Stetson, Albers, Silverstein, and
  Wolfe]{stetson_effects_1992}
Stetson, D.S.; Albers, J.W.; Silverstein, B.A.; Wolfe, R.A.
\newblock Effects of age, sex, and anthropometric factors on nerve conduction
  measures.
\newblock {\em Muscle Nerve} {\bf 1992}, {\em 15},~1095--1104.
%\newblock \_eprint:
%  https://onlinelibrary.wiley.com/doi/pdf/10.1002/mus.880151007,
  {{https://doi.org/10.1002/mus.880151007}}.

\bibitem[Jeffress(1948)]{jeffress_place_1948}
Jeffress, L.A.
\newblock A place theory of sound localization.
\newblock {\em J. Comp. Physiol. Psychol.} {\bf 1948},
  {\em 41},~35--39.
%\newblock Place: US Publisher: American Psychological Association,
  {{https://doi.org/10.1037/h0061495}}.

\bibitem[Konishi(2003)]{konishi_coding_2003}
Konishi, M.
\newblock Coding of auditory space.
\newblock {\em Annu. Rev. Neurosci.} {\bf 2003}, {\em 26},~31--55.\linebreak
\newblock {{https://doi.org/10.1146/annurev.neuro.26.041002.131123}}.

\bibitem[Gerstner {et~al.}(1996)Gerstner, Kempter, van Hemmen, and
  Wagner]{gerstner_neuronal_1996}
Gerstner, W.; Kempter, R.; van Hemmen, J.L.; Wagner, H.
\newblock A neuronal learning rule for sub-millisecond temporal coding.
\newblock {\em Nature} {\bf 1996}, {\em 383},~76--78.
%\newblock Number: 6595 Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/383076a0}}.

\bibitem[Seidl {et~al.}(2010)Seidl, Rubel, and
  Harris]{seidl_mechanisms_2010}
Seidl, A.H.; Rubel, E.W.; Harris, D.M.
\newblock Mechanisms for adjusting interaural time differences to achieve
  binaural coincidence detection.
\newblock {\em  J. Neurosci. Off. J. Soc. Neurosci.} {\bf 2010}, {\em 30},~70--80.
\newblock {{https://doi.org/10.1523/JNEUROSCI.3464-09.2010}}.

\bibitem[Camon {et~al.}(2019)Camon, Hugues, Erlandson, Robbe, Lagoun,
  Marouane, and Bureau]{camon_timing_2019}
Camon, J.; Hugues, S.; Erlandson, M.A.; Robbe, D.; Lagoun, S.; Marouane, E.;
  Bureau, I.
\newblock The {Timing} of {Sensory}-{Guided} {Behavioral} {Response} is
  {Represented} in the {Mouse} {Primary} {Somatosensory} {Cortex}.
\newblock {\em Cereb. Cortex} {\bf 2019}, {\em 29},~3034--3047.
\newblock {{https://doi.org/10.1093/cercor/bhy169}}.

\bibitem[Gasser and Grundfest(1939)]{gasser_axon_1939}
Gasser, H.S.; Grundfest, H.
\newblock {AXON} {DIAMETERS} {IN} {RELATION} {TO} {THE} {SPIKE} {DIMENSIONS}
  {AND} {THE} {CONDUCTION} {VELOCITY} {IN} {MAMMALIAN} {A} {FIBERS}.
\newblock {\em Am. J.-Physiol.-Leg. Content} {\bf 1939}. %MDPI: Please add the volume and page number.
%\newblock Publisher: American Physiological Society,
  {{https://doi.org/10.1152/ajplegacy.1939.127.2.393}}.

\bibitem[Brill {et~al.}(1977)Brill, Waxman, Moore, and
  Joyner]{brill_conduction_1977}
Brill, M.H.; Waxman, S.G.; Moore, J.W.; Joyner, R.W.
\newblock Conduction velocity and spike configuration in myelinated fibres:
  computed dependence on internode distance.
\newblock {\em J. Neurol. Neurosurg. Psychiatry} {\bf 1977},
  {\em 40},~769--774.

\bibitem[Pérez-Cerdá {et~al.}(2015)Pérez-Cerdá, Sánchez-Gómez, and
  Matute]{perez-cerda_pio_2015}
Pérez-Cerdá, F.; Sánchez-Gómez, M.V.; Matute, C.
\newblock Pío del {Río} {Hortega} and the discovery of the oligodendrocytes.
\newblock {\em Front. Neuroanat.} {\bf 2015}, {\em 9},~92.
\newblock {{https://doi.org/10.3389/fnana.2015.00092}}.

\bibitem[Schmitt and Bear(1939)]{schmitt_ultrastructure_1939}
Schmitt, F.O.; Bear, R.S.
\newblock The {Ultrastructure} of the {Nerve} {Axon} {Sheath}.
\newblock {\em Biol. Rev.} {\bf 1939}, {\em 14},~27--50.
%\newblock \_eprint:
%  https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-185X.1939.tb00922.x,
  {{https://doi.org/10.1111/j.1469-185X.1939.tb00922.x}}.

\bibitem[Simons and Nave(2016)]{simons_oligodendrocytes_2016}
Simons, M.; Nave, K.A.
\newblock Oligodendrocytes: {Myelination} and {Axonal} {Support}.
\newblock {\em Cold Spring Harb. Perspect. Biol.} {\bf 2016}, {\em
  8},~a020479.
\newblock {{https://doi.org/10.1101/cshperspect.a020479}}.

\bibitem[Duncan {et~al.}(2021)Duncan, Simkins, and
  Emery]{duncan_neuron-oligodendrocyte_2021}
Duncan, G.J.; Simkins, T.J.; Emery, B.
\newblock Neuron-{Oligodendrocyte} {Interactions} in the {Structure} and
  {Integrity} of {Axons}.
\newblock {\em Front. Cell Dev. Biol.} {\bf 2021}, {\em
  9}, 653101.
\newblock {{https://doi.org/10.3389/fcell.2021.653101}}.

\bibitem[Fields(2015)]{fields_new_2015}
Fields, R.D.
\newblock A new mechanism of nervous system plasticity: activity-dependent
  myelination.
\newblock {\em Nat. Rev. Neurosci.} {\bf 2015}, {\em 16},~756--767.
%\newblock Number: 12 Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/nrn4023}}.

\bibitem[Fields and Bukalo(2020)]{fields_myelin_2020}
Fields, R.D.; Bukalo, O.
\newblock Myelin makes memories.
\newblock {\em Nat. Neurosci.} {\bf 2020}, {\em 23},~469--470.
\newblock {{https://doi.org/10.1038/s41593-020-0606-x}}.

\bibitem[Reynolds and Slater(1928)]{reynolds_study_1928}
Reynolds, F.E.; Slater, J.K.
\newblock A {Study} of the {Structure} and {Function} of the {Interstitial}
  {Tissue} of the {Central} {Nervous} {System}.
\newblock {\em Edinb. Med. J.} {\bf 1928}, {\em 35},~49--57.

\bibitem[Steadman {et~al.}(2020)Steadman, Xia, Ahmed, Mocle, Penning,
  Geraghty, Steenland, Monje, Josselyn, and
  Frankland]{steadman_disruption_2020}
Steadman, P.E.; Xia, F.; Ahmed, M.; Mocle, A.J.; Penning, A.R.; Geraghty, A.C.;
  Steenland, H.W.; Monje, M.; Josselyn, S.A.; Frankland, P.W.
\newblock Disruption of {Oligodendrogenesis} {Impairs} {Memory} {Consolidation}
  in {Adult} {Mice}.
\newblock {\em Neuron} {\bf 2020}, {\em 105},~150--164.e6.
\newblock {{https://doi.org/10.1016/j.neuron.2019.10.013}}.

\bibitem[Pan {et~al.}(2020)Pan, Mayoral, Choi, Chan, and
  Kheirbek]{pan_preservation_2020}
Pan, S.; Mayoral, S.R.; Choi, H.S.; Chan, J.R.; Kheirbek, M.A.
\newblock Preservation of a remote fear memory requires new myelin formation.
\newblock {\em Nat. Neurosci.} {\bf 2020}, {\em 23},~487--499.
\newblock {{https://doi.org/10.1038/s41593-019-0582-1}}.

\bibitem[Wan {et~al.}(2020)Wan, Cheli, Santiago-González, Rosenblum, Wan,
  and Paez]{wan_impaired_2020}
Wan, R.; Cheli, V.T.; Santiago-González, D.A.; Rosenblum, S.L.; Wan, Q.; Paez,
  P.M.
\newblock Impaired {Postnatal} {Myelination} in a {Conditional} {Knockout}
  {Mouse} for the {Ferritin} {Heavy} {Chain} in {Oligodendroglial} {Cells}.
\newblock {\em  J. Neurosci.} {\bf 2020}, {\em 40},~7609--7624.
\newblock {{https://doi.org/10.1523/JNEUROSCI.1281-20.2020}}.

\bibitem[Xue {et~al.}(2021)Xue, Zhu, Liu, Lin, Li, Li, and
  Zhuo]{xue_demyelination_2021}
Xue, J.; Zhu, Y.; Liu, Z.; Lin, J.; Li, Y.; Li, Y.; Zhuo, Y.
\newblock Demyelination of the {Optic} {Nerve}: {An} {Underlying} {Factor} in
  {Glaucoma}?
\newblock {\em Front. Aging Neurosci.} {\bf 2021}, {\em 13},~701322.
\newblock {{https://doi.org/10.3389/fnagi.2021.701322}}.

\bibitem[Kuhn {et~al.}(2019)Kuhn, Gritti, Crooks, and
  Dombrowski]{kuhn_oligodendrocytes_2019}
Kuhn, S.; Gritti, L.; Crooks, D.; Dombrowski, Y.
\newblock Oligodendrocytes in {Development}, {Myelin} {Generation} and
  {Beyond}.
\newblock {\em Cells} {\bf 2019}, {\em 8},~1424.
\newblock {{https://doi.org/10.3390/cells8111424}}.

\bibitem[Baraban {et~al.}(2018)Baraban, Koudelka, and
  Lyons]{baraban_ca2_2018}
Baraban, M.; Koudelka, S.; Lyons, D.A.
\newblock Ca$^{2+}$ activity signatures of myelin sheath formation and growth in
  vivo.
\newblock {\em Nat. Neurosci.} {\bf 2018}, {\em 21},~19--23.
\newblock {{https://doi.org/10.1038/s41593-017-0040-x}}.

\bibitem[Nave and Salzer(2006)]{nave_axonal_2006}
Nave, K.A.; Salzer, J.L.
\newblock Axonal regulation of myelination by neuregulin 1.
\newblock {\em Curr. Opin. Neurobiol.} {\bf 2006}, {\em 16},~492--500.
\newblock {{https://doi.org/10.1016/j.conb.2006.08.008}}.

\bibitem[Cullen {et~al.}(2021)Cullen, Pepper, Clutterbuck, Pitman, Oorschot,
  Auderset, Tang, Ramm, Emery, Rodger, Jolivet, and
  Young]{cullen_periaxonal_2021}
Cullen, C.L.; Pepper, R.E.; Clutterbuck, M.T.; Pitman, K.A.; Oorschot, V.;
  Auderset, L.; Tang, A.D.; Ramm, G.; Emery, B.; Rodger, J.;  et~al.
\newblock Periaxonal and nodal plasticities modulate action potential
  conduction in the adult mouse brain.
\newblock {\em Cell Rep.} {\bf 2021}, {\em 34}, 108641.
%\newblock Publisher: Elsevier,
  {{https://doi.org/10.1016/j.celrep.2020.108641}}.

\bibitem[Gibson {et~al.}(2014)Gibson, Purger, Mount, Goldstein, Lin, Wood,
  Inema, Miller, Bieri, Zuchero, Barres, Woo, Vogel, and
  Monje]{gibson_neuronal_2014}
Gibson, E.M.; Purger, D.; Mount, C.W.; Goldstein, A.K.; Lin, G.L.; Wood, L.S.;
  Inema, I.; Miller, S.E.; Bieri, G.; Zuchero, J.B.;  et~al.
\newblock Neuronal {Activity} {Promotes} {Oligodendrogenesis} and {Adaptive}
  {Myelination} in the {Mammalian} {Brain}.
\newblock {\em Science} {\bf 2014}, {\em 344},~1252304.
\newblock {{https://doi.org/10.1126/science.1252304}}.

\bibitem[Spencer {et~al.}(2018)Spencer, Meffin, Burkitt, and
  Grayden]{spencer_compensation_2018}
Spencer, M.J.; Meffin, H.; Burkitt, A.N.; Grayden, D.B.
\newblock Compensation for {Traveling} {Wave} {Delay} {Through} {Selection} of
  {Dendritic} {Delays} {Using} {Spike}-{Timing}-{Dependent} {Plasticity} in a
  {Model} of the {Auditory} {Brainstem}.
\newblock {\em Front. Comput. Neurosci.} {\bf 2018}, {\em
  12},~36.
\newblock {{https://doi.org/10.3389/fncom.2018.00036}}.

\bibitem[Mel {et~al.}(2017)Mel, Schiller, and Poirazi]{mel_synaptic_2017}
Mel, B.W.; Schiller, J.; Poirazi, P.
\newblock Synaptic plasticity in dendrites: complications and coping
  strategies.
\newblock {\em Curr. Opin. Neurobiol.} {\bf 2017}, {\em 43},~177--186.
\newblock {{https://doi.org/10.1016/j.conb.2017.03.012}}.

\bibitem[Golding {et~al.}(2002)Golding, Staff, and
  Spruston]{golding_dendritic_2002}
Golding, N.L.; Staff, N.P.; Spruston, N.
\newblock Dendritic spikes as a mechanism for cooperative long-term
  potentiation.
\newblock {\em Nature} {\bf 2002}, {\em 418},~326--331.
%\newblock Number: 6895 Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/nature00854}}.

\bibitem[Maass(1997)]{maass_networks_1997}
Maass, W.
\newblock Networks of spiking neurons: {The} third generation of neural network
  models.
\newblock {\em Neural Netw.} {\bf 1997}, {\em 10},~1659--1671.

\bibitem[Neftci {et~al.}(2019)Neftci, Mostafa, and
  Zenke]{neftci_surrogate_2019}
Neftci, E.O.; Mostafa, H.; Zenke, F.
\newblock Surrogate {Gradient} {Learning} in {Spiking} {Neural} {Networks}:
  {Bringing} the {Power} of {Gradient}-{Based} {Optimization} to {Spiking}
  {Neural} {Networks}.
\newblock {\em IEEE Signal Process. Mag.} {\bf 2019}, {\em 36},~51--63.\linebreak
\newblock {{https://doi.org/10.1109/MSP.2019.2931595}}.

\bibitem[Rueckauer {et~al.}(2017)Rueckauer, Lungu, Hu, Pfeiffer, and
  Liu]{rueckauer_conversion_2017}
Rueckauer, B.; Lungu, I.A.; Hu, Y.; Pfeiffer, M.; Liu, S.C.
\newblock Conversion of {Continuous}-{Valued} {Deep} {Networks} to {Efficient}
  {Event}-{Driven} {Networks} for {Image} {Classification}.
\newblock {\em Front. Neurosci.} {\bf 2017}, {\em 11}.
\newblock {{https://doi.org/10.3389/fnins.2017.00682}}.

\bibitem[Susi {et~al.}(2021)Susi, Antón-Toro, Maestú, Pereda, and
  Mirasso]{susi_nmnsd-spiking_2021}
Susi, G.; Antón-Toro, L.F.; Maestú, F.; Pereda, E.; Mirasso, C.
\newblock {nMNSD}-{A} {Spiking} {Neuron}-{Based} {Classifier} {That} {Combines}
  {Weight}-{Adjustment} and {Delay}-{Shift}.
\newblock {\em Front. Neurosci.} {\bf 2021}, {\em 15},~582608.
\newblock {{https://doi.org/10.3389/fnins.2021.582608}}.

\bibitem[Davies {et~al.}(2018)Davies, Srinivasa, Lin, Chinya, Cao, Choday,
  Dimou, Joshi, Imam, Jain, Liao, Lin, Lines, Liu, Mathaikutty, McCoy, Paul,
  Tse, Venkataramanan, Weng, Wild, Yang, and Wang]{davies_loihi_2018}
Davies, M.; Srinivasa, N.; Lin, T.H.; Chinya, G.; Cao, Y.; Choday, S.H.; Dimou,
  G.; Joshi, P.; Imam, N.; Jain, S.;  et~al.
\newblock Loihi: {A} {Neuromorphic} {Manycore} {Processor} with {On}-{Chip}
  {Learning}.
\newblock {\em IEEE Micro} {\bf 2018}, {\em 38},~82--99.
\newblock {{https://doi.org/10.1109/MM.2018.112130359}}.

\bibitem[Lazar(2004)]{lazar_time_2004}
Lazar, A.A.
\newblock Time encoding with an integrate-and-fire neuron with a refractory
  period.
\newblock {\em Neurocomputing} {\bf 2004}, {\em 58--60},~53--58.
\newblock {{https://doi.org/10.1016/j.neucom.2004.01.022}}.

\bibitem[Markram {et~al.}(1997)Markram, Lübke, Frotscher, and
  Sakmann]{markram_regulation_1997}
Markram, H.; Lübke, J.; Frotscher, M.; Sakmann, B.
\newblock Regulation of {Synaptic} {Efficacy} by {Coincidence} of
  {Postsynaptic} {APs} and {EPSPs}.
\newblock {\em Science} {\bf 1997}, {\em 275},~213--215.
\newblock {{https://doi.org/10.1126/science.275.5297.213}}.

\bibitem[Caporale and Dan(2008)]{caporale_spike_2008}
Caporale, N.; Dan, Y.
\newblock Spike {Timing}–{Dependent} {Plasticity}: {A} {Hebbian} {Learning}
  {Rule}.
\newblock {\em Annu. Rev. Neurosci.} {\bf 2008}, {\em 31},~25--46.
\newblock {{https://doi.org/10.1146/annurev.neuro.31.060407.125639}}.

\bibitem[Hüning {et~al.}(1998)Hüning, Glünder, and
  Palm]{huning_synaptic_1998}
Hüning, H.; Glünder, H.; Palm, G.
\newblock Synaptic {Delay} {Learning} in {Pulse}-{Coupled} {Neurons}.
\newblock {\em Neural Comput.} {\bf 1998}, {\em 10},~555--565.
\newblock {{https://doi.org/10.1162/089976698300017665}}.

\bibitem[Eurich {et~al.}(1999)Eurich, Pawelzik, Ernst, Cowan, and
  Milton]{eurich_dynamics_1999}
Eurich, C.W.; Pawelzik, K.; Ernst, U.; Cowan, J.D.; Milton, J.G.
\newblock Dynamics of {Self}-{Organized} {Delay} {Adaptation}.
\newblock {\em Phys. Rev. Lett.} {\bf 1999}, {\em 82},~1594--1597.
%\newblock 00082 Publisher: American Physical Society,
  {{https://doi.org/10.1103/PhysRevLett.82.1594}}.

\bibitem[Gütig and Sompolinsky(2006)]{gutig_tempotron_2006}
Gütig, R.; Sompolinsky, H.
\newblock The tempotron: A neuron that learns spike timing–based decisions.
\newblock {\em Nat. Neurosci.} {\bf 2006}, {\em 9},~420--428.
\newblock {{https://doi.org/10.1038/nn1643}}.

\bibitem[Gütig(2014)]{gutig_spike_2014}
Gütig, R.
\newblock To spike, or when to spike?
\newblock {\em Curr. Opin. Neurobiol.} {\bf 2014}, {\em 25},~134--139.
\newblock {{https://doi.org/10.1016/j.conb.2014.01.004}}.

\bibitem[Pauli {et~al.}(2018)Pauli, Weidel, Kunkel, and
  Morrison]{pauli_reproducing_2018}
Pauli, R.; Weidel, P.; Kunkel, S.; Morrison, A.
\newblock Reproducing {Polychronization}: {A} {Guide} to {Maximizing} the
  {Reproducibility} of {Spiking} {Network} {Models}.
\newblock {\em Front. Neuroinformatics} {\bf 2018}, {\em 12},~46.
 {{https://doi.org/10.3389/fninf.2018.00046}}.

\bibitem[Guise {et~al.}(2014)Guise, Knott, and
  Benuskova]{guise_bayesian_2014}
Guise, M.; Knott, A.; Benuskova, L.
\newblock A {Bayesian} {Model} of {Polychronicity}.
\newblock {\em Neural Comput.} {\bf 2014}, {\em 26},~2052--2073.
\newblock {{https://doi.org/10.1162/neco\_a\_00620}}.

\bibitem[Zhang {et~al.}(2020)Zhang, Wu, Belatreche, Pan, Xie, Chua, Li, Qu,
  and Li]{zhang_supervised_2020}
Zhang, M.; Wu, J.; Belatreche, A.; Pan, Z.; Xie, X.; Chua, Y.; Li, G.; Qu, H.;
  Li, H.
\newblock Supervised learning in spiking neural networks with synaptic
  delay-weight plasticity.
\newblock {\em Neurocomputing} {\bf 2020}, {\em 409},~103--118.
\newblock {{https://doi.org/10.1016/j.neucom.2020.03.079}}.

\bibitem[Ghosh {et~al.}(2021)Ghosh, Frasca, Rizzo, Majhi, Rakshit,
  Alfaro-Bittner, and Boccaletti]{ghosh_synchronization_2021}
Ghosh, D.; Frasca, M.; Rizzo, A.; Majhi, S.; Rakshit, S.; Alfaro-Bittner, K.;
  Boccaletti, S.
\newblock Synchronization in time-varying networks. \emph{arXiv} {\bf 2021}, arXiv:2109.07618
%\newblock {\em arXiv:2109.07618 [physics]} .
%\newblock 00000 arXiv: 2109.07618.

\bibitem[Ghosh {et~al.}(2022)Ghosh, Frasca, Rizzo, Majhi, Rakshit,
  Alfaro-Bittner, and Boccaletti]{ghosh_synchronized_2022}
Ghosh, D.; Frasca, M.; Rizzo, A.; Majhi, S.; Rakshit, S.; Alfaro-Bittner, K.;
  Boccaletti, S.
\newblock The synchronized dynamics of time-varying networks.
\newblock {\em Phys. Rep.} {\bf 2022}, {\em 949},~1--63.
%\newblock arXiv:2109.07618 [physics],
  {{https://doi.org/10.1016/j.physrep.2021.10.006}}.

\bibitem[Izhikevich and Hoppensteadt(2009)]{izhikevich_polychronous_2009}
Izhikevich, E.M.; Hoppensteadt, F.C.
\newblock Polychronous {Wavefront} {Computations}.
\newblock {\em Int. J. Bifurc. Chaos} {\bf 2009}, {\em
  19},~1733--1739.
\newblock {{https://doi.org/10.1142/S0218127409023809}}.

\bibitem[Grimaldi and Perrinet(2022)]{grimaldi_learning_2022}
Grimaldi, A.; Perrinet, L.U.
\newblock Learning hetero-synaptic delays for motion detection in a single
  layer of spiking neurons.
\newblock In Proceedings of the 2022 {IEEE} {International} {Conference} on
  {Image} {Processing} ({ICIP}), \hl{Bordeaux, France, 16--19 October} %MDPI: We added the location and date of the conference. Please confirm.
 2022; pp.~3591--3595.
%\newblock ISSN: 2381-8549,
  {{https://doi.org/10.1109/ICIP46576.2022.9897394}}.

\bibitem[Madadi~Asl and
  Ramezani~Akbarabadi(2022)]{madadi_asl_delay-dependent_2022}
Madadi~Asl, M.; Ramezani~Akbarabadi, S.
\newblock Delay-dependent transitions of phase synchronization and coupling
  symmetry between neurons shaped by spike-timing-dependent plasticity.
\newblock {\em Cogn. Neurodynamics} {\bf \hl{2022} %MDPI: Please add the volume.
}, 1--14.
\newblock {{https://doi.org/10.1007/s11571-022-09850-x}}.

\bibitem[Perrinet and Samuelides(2002)]{perrinet_coherence_2002}
Perrinet, L.; Samuelides, M.
\newblock Coherence detection in a spiking neuron via {Hebbian} learning.
\newblock {\em Neurocomputing} {\bf 2002}, {\em 44-46},~133--139.
\newblock {{https://doi.org/10.1016/s0925-2312(02)00374-0}}.

\bibitem[Perrinet {et~al.}(2001)Perrinet, Delorme, Samuelides, and
  Thorpe]{perrinet_networks_2001}
Perrinet, L.; Delorme, A.; Samuelides, M.; Thorpe, S.
\newblock Networks of integrate-and-fire neuron using rank order coding {A}:
  {How} to implement spike time dependent {Hebbian} plasticity.
\newblock {\em Neurocomputing} {\bf 2001}, {\em 38--40},~817--822.
\newblock {{https://doi.org/10.1016/s0925-2312(01)00460-x}}.

\bibitem[Gilson(2010)]{gilson_stdp_2010}
Gilson, M.
\newblock {STDP} in recurrent neuronal networks.
\newblock {\em Front. Comput. Neurosci.} {\bf 2010}, {\em 4}, 23.
\newblock {{https://doi.org/10.3389/fncom.2010.00023}}.

\bibitem[Datadien {et~al.}(2011)Datadien, Haselager, and
  Sprinkhuizen-Kuyper]{datadien_right_2011}
Datadien, A.; Haselager, P.; Sprinkhuizen-Kuyper, I.
\newblock {\em The {Right} {Delay}---{Detecting} {Specific} {Spike} {Patterns}
  with {STDP} and {Axonal} {Conduction} {Delays}}; \hl{Springer:  Berlin/Heidelberg, Germany,} %MDPI: We added the name of the publisher and location. Please confirm.
 2011;
\newblock pp. 90--99.

\bibitem[Kerr {et~al.}(2013)Kerr, Burkitt, Thomas, Gilson, and
  Grayden]{kerr_delay_2013}
Kerr, R.R.; Burkitt, A.N.; Thomas, D.A.; Gilson, M.; Grayden, D.B.
\newblock Delay {Selection} by {Spike}-{Timing}-{Dependent} {Plasticity} in
  {Recurrent} {Networks} of {Spiking} {Neurons} {Receiving} {Oscillatory}
  {Inputs}.
\newblock {\em PLoS Comput. Biol.} {\bf 2013}, {\em 9},~e1002897.
\newblock {{https://doi.org/10.1371/journal.pcbi.1002897}}.

\bibitem[Burkitt and Hogendoorn(2021)]{burkitt_predictive_2021}
Burkitt, A.N.; Hogendoorn, H.
\newblock Predictive {Visual} {Motion} {Extrapolation} {Emerges}
  {Spontaneously} and without {Supervision} at {Each} {Layer} of a
  {Hierarchical} {Neural} {Network} with {Spike}-{Timing}-{Dependent}
  {Plasticity}.
\newblock {\em  J. Neurosci.} {\bf 2021}, {\em 41},~4428--4438.
\newblock {{https://doi.org/10.1523/jneurosci.2017-20.2021}}.

\bibitem[Nadafian and Ganjtabesh(2020)]{nadafian_bio-plausible_2020}
Nadafian, A.; Ganjtabesh, M.
\newblock Bio-plausible {Unsupervised} {Delay} {Learning} for {Extracting}
  {Temporal} {Features} in {Spiking} {Neural} {Networks}. \emph{arXiv}  \textbf{2020},  	arXiv:2011.09380.

\bibitem[Wang {et~al.}(2019)Wang, Lin, and Dang]{wang_delay_2019}
Wang, X.; Lin, X.; Dang, X.
\newblock A {Delay} {Learning} {Algorithm} {Based} on {Spike} {Train} {Kernels}
  for {Spiking} {Neurons}.
\newblock {\em Front. Neurosci.} {\bf 2019}, {\em 13}, 252.

\bibitem[Hazan {et~al.}(2022)Hazan, Caby, Earl, Siegelmann, and
  Levin]{hazan_memory_2022}
Hazan, H.; Caby, S.; Earl, C.; Siegelmann, H.; Levin, M.
\newblock Memory via {Temporal} {Delays} in weightless {Spiking} {Neural}
  {Network}.  \emph{arXiv} \textbf{2022}, arXiv:2202.07132.
%\newblock arXiv:2202.07132 [cs, q-bio, stat].

\bibitem[Luo {et~al.}(2022)Luo, Qu, Wang, Yi, Zhang, and
  Zhang]{luo_supervised_2022}
Luo, X.; Qu, H.; Wang, Y.; Yi, Z.; Zhang, J.; Zhang, M.
\newblock Supervised {Learning} in {Multilayer} {Spiking} {Neural} {Networks}
  {With} {Spike} {Temporal} {Error} {Backpropagation}.
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.} {\bf
  \hl{2022} %MDPI: Please add the volume.
}, 1--13.
\newblock {{https://doi.org/10.1109/tnnls.2022.3164930}}.

\bibitem[Sun {et~al.}(2016)Sun, Sourina, and Huang]{sun_learning_2016}
Sun, H.; Sourina, O.; Huang, G.B.
\newblock Learning polychronous neuronal groups using joint weight-delay
  spike-timing-dependent plasticity.
\newblock {\em Neural Comput.} {\bf 2016}, {\em 28},~2181--2212.
%\newblock tex.eprint:
%  https://direct.mit.edu/neco/article-pdf/28/10/2181/972099/neco{\textbackslash}\_a{\textbackslash}\_00879.pdf,
  {{https://doi.org/10.1162/NECO\_a\_00879}}.

\bibitem[Ghosh {et~al.}(2019)Ghosh, Gupta, Silva, Soares, and
  Thakor]{ghosh_spatiotemporal_2019}
Ghosh, R.; Gupta, A.; Silva, A.N.; Soares, A.; Thakor, N.V.
\newblock Spatiotemporal filtering for event-based action recognition. \emph{arXiv}  \textbf{2019},  	arXiv:1903.07067.

\bibitem[Perrinet {et~al.}(2014)Perrinet, Adams, and
  Friston]{perrinet_active_2014}
Perrinet, L.U.; Adams, R.A.; Friston, K.J.
\newblock Active inference, eye movements and oculomotor delays.
\newblock {\em Biol. Cybern.} {\bf 2014}, {\em 108},~777--801.
\newblock {{https://doi.org/10.1007/s00422-014-0620-8}}.

\bibitem[Hogendoorn and Burkitt(2019)]{hogendoorn_predictive_2019}
Hogendoorn, H.; Burkitt, A.N.
\newblock Predictive {Coding} with {Neural} {Transmission} {Delays}: {A}
  {Real}-{Time} {Temporal} {Alignment} {Hypothesis}.
\newblock {\em eNeuro} {\bf 2019}, {\em 6},~ENEURO.0412--18.2019.
\newblock {{https://doi.org/10.1523/eneuro.0412-18.2019}}.

\bibitem[Khoei {et~al.}(2013)Khoei, Masson, and
  Perrinet]{khoei_motion-based_2013}
Khoei, M.A.; Masson, G.S.; Perrinet, L.U.
\newblock Motion-based prediction explains the role of tracking in motion
  extrapolation.
\newblock {\em J. Physiol.-Paris} {\bf 2013}, {\em 107},~409--420.
\newblock {{https://doi.org/10.1016/j.jphysparis.2013.08.001}}.

\bibitem[Kaplan {et~al.}(2013)Kaplan, Lansner, Masson, and
  Perrinet]{kaplan_anisotropic_2013}
Kaplan, B.A.; Lansner, A.; Masson, G.S.; Perrinet, L.U.
\newblock Anisotropic connectivity implements motion-based prediction in a
  spiking neural network.
\newblock {\em Front. Comput. Neurosci.} {\bf 2013}, {\em 7}, 112.
%\newblock Loaded from an external bibliography file by Manubot.
%  source\_bibliography: manual-references.bib standard\_id: Kaplan13,
  {{https://doi.org/10.3389/fncom.2013.00112}}.

\bibitem[Khoei {et~al.}(2017)Khoei, Masson, and
  Perrinet]{khoei_flash-lag_2017}
Khoei, M.A.; Masson, G.S.; Perrinet, L.U.
\newblock The {Flash}-{Lag} {Effect} as a {Motion}-{Based} {Predictive}
  {Shift}.
\newblock {\em PLoS Comput. Biol.} {\bf 2017}, {\em 13},~e1005068.
%\newblock Publisher: Public Library of Science,
  {{https://doi.org/10.1371/journal.pcbi.1005068}}.

\bibitem[Javanshir {et~al.}(2022)Javanshir, Nguyen, Mahmud, and
  Kouzani]{javanshir_advancements_2022}
Javanshir, A.; Nguyen, T.T.; Mahmud, M.A.P.; Kouzani, A.Z.
\newblock Advancements in {Algorithms} and {Neuromorphic} {Hardware} for
  {Spiking} {Neural} {Networks}.
\newblock {\em Neural Comput.} {\bf 2022}, {\em 34},~1289--1328.
\newblock {{https://doi.org/10.1162/neco\_a\_01499}}.

\bibitem[Marković {et~al.}(2020)Marković, Mizrahi, Querlioz, and
  Grollier]{markovic_physics_2020}
Marković, D.; Mizrahi, A.; Querlioz, D.; Grollier, J.
\newblock Physics for neuromorphic computing.
\newblock {\em Nat. Rev. Phys.} {\bf 2020}, {\em 2},~499--510.
%\newblock Number: 9 Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/s42254-020-0208-2}}.

\bibitem[Rasetto {et~al.}(2022)Rasetto, Wan, Akolkar, Shi, Xiong, and
  Benosman]{rasetto_challenges_2022}
Rasetto, M.; Wan, Q.; Akolkar, H.; Shi, B.; Xiong, F.; Benosman, R.
\newblock The {Challenges} {Ahead} for {Bio}-inspired {Neuromorphic} {Event}
  {Processors}: {How} {Memristors} {Dynamic} {Properties} {Could}
  {Revolutionize} {Machine} {Learning}. \emph{arXiv}  \textbf{2022},  	arXiv:2201.12673.
%\newblock Technical Report 2201.12673, .

\bibitem[Diesmann and Gewaltig(2003)]{diesmann_nest_2003}
Diesmann, M.; Gewaltig, M.O.
\newblock {NEST}: {An} {Environment} for {Neural} {Systems} {Simulations}.
\newblock {\em GWDG-Bericht Nr. 58 Theo Plesser, Volker Macho (Hrsg.)}; {
  \hl{2003}}; %MDPI: Please add the name of the publisher and their location.
 p.~29.

\bibitem[Hazan {et~al.}(2018)Hazan, Saunders, Khan, Patel, Sanghavi,
  Siegelmann, and Kozma]{hazan_bindsnet_2018}
Hazan, H.; Saunders, D.J.; Khan, H.; Patel, D.; Sanghavi, D.T.; Siegelmann,
  H.T.; Kozma, R.
\newblock \textls[-15]{{BindsNET}: {A} {Machine} {Learning}-{Oriented} {Spiking} {Neural}
  {Networks} {Library} in {Python}.}
\newblock {\em Front. Neuroinformatics} {\bf 2018}, {\em 12}, 89.
\newblock {{https://doi.org/10.3389/fninf.2018.00089}}.

\bibitem[Stimberg {et~al.}(2019)Stimberg, Brette, and
  Goodman]{stimberg_brian_2019}
Stimberg, M.; Brette, R.; Goodman, D.F.
\newblock Brian 2, an intuitive and efficient neural simulator.
\newblock {\em eLife} {\bf 2019}, {\em 8},~e47314.
\newblock {{https://doi.org/10.7554/eLife.47314}}.

\bibitem[Zenke {et~al.}(2021)Zenke, Bohté, Clopath, Comşa, Göltz, Maass,
  Masquelier, Naud, Neftci, Petrovici, Scherr, and
  Goodman]{zenke_visualizing_2021}
Zenke, F.; Bohté, S.M.; Clopath, C.; Comşa, I.M.; Göltz, J.; Maass, W.;
  Masquelier, T.; Naud, R.; Neftci, E.O.; Petrovici, M.A.;  et~al.
\newblock Visualizing a joint future of neuroscience and neuromorphic
  engineering.
\newblock {\em Neuron} {\bf 2021}, {\em 109},~571--575.
\newblock {{https://doi.org/10.1016/j.neuron.2021.01.009}}.

\bibitem[Mead and Ismail(1989)]{mead_analog_1989}
Mead, C.; Ismail, M.
\newblock {\em Analog {VLSI} {Implementation} of {Neural} {Systems}}; Springer
  Science \& Business Media:  \hl{Berlin/Heidelberg, Germany,} %newly added information, please confirm
  1989.
%\newblock Google-Books-ID: 9e29dOiXeiMC.

\bibitem[Bartolozzi and Indiveri(2007)]{bartolozzi_synaptic_2007}
Bartolozzi, C.; Indiveri, G.
\newblock Synaptic {Dynamics} in {Analog} {VLSI}.
\newblock {\em Neural Comput.} {\bf 2007}, {\em 19},~2581--2603.\linebreak
\newblock {{https://doi.org/10.1162/neco.2007.19.10.2581}}.

\bibitem[Schuman {et~al.}(2017)Schuman, Potok, Patton, Birdwell, Dean, Rose,
  and Plank]{schuman_survey_2017}
Schuman, C.D.; Potok, T.E.; Patton, R.M.; Birdwell, J.D.; Dean, M.E.; Rose,
  G.S.; Plank, J.S.
\newblock A {Survey} of {Neuromorphic} {Computing} and {Neural} {Networks} in
  {Hardware}. \emph{arXiv} {\bf 2017}, arXiv:1705.06963
%\newblock {\em  [cs]} .
%\newblock arXiv: 1705.06963.

\bibitem[Furber {et~al.}(2013)Furber, Lester, Plana, Garside, Painkras,
  Temple, and Brown]{furber_overview_2013}
Furber, S.B.; Lester, D.R.; Plana, L.A.; Garside, J.D.; Painkras, E.; Temple,
  S.; Brown, A.D.
\newblock Overview of the {SpiNNaker} {System} {Architecture}.
\newblock {\em IEEE Trans. Comput.} {\bf 2013}, {\em
  62},~2454--2467.
\newblock {{https://doi.org/10.1109/TC.2012.142}}.

\bibitem[Furber and Bogdan(2020)]{furber_spinnaker_2020}
Furber, S.; Bogdan, P., Eds.
\newblock {\em {SpiNNaker}: {A} {Spiking} {Neural} {Network} {Architecture}};
  Now Publishers: \hl{Norwell, MA, USA,} %MDPI: We added the location of the publisher. Please confirm.
  2020.
\newblock {{https://doi.org/10.1561/9781680836523}}.

\bibitem[Merolla {et~al.}(2014)Merolla, Arthur, Alvarez-Icaza, Cassidy,
  Sawada, Akopyan, Jackson, Imam, Guo, Nakamura, Brezzo, Vo, Esser, Appuswamy,
  Taba, Amir, Flickner, Risk, Manohar, and Modha]{merolla_million_2014}
Merolla, P.A.; Arthur, J.V.; Alvarez-Icaza, R.; Cassidy, A.S.; Sawada, J.;
  Akopyan, F.; Jackson, B.L.; Imam, N.; Guo, C.; Nakamura, Y.;  et~al.
\newblock A million spiking-neuron integrated circuit with a scalable
  communication network and interface.
\newblock {\em Science} {\bf 2014}, {\em 345},~668--673.
\newblock {{https://doi.org/10.1126/science.1254642}}.

\bibitem[Benjamin {et~al.}(2014)Benjamin, Gao, McQuinn, Choudhary,
  Chandrasekaran, Bussat, Alvarez-Icaza, Arthur, Merolla, and
  Boahen]{benjamin_neurogrid_2014}
Benjamin, B.V.; Gao, P.; McQuinn, E.; Choudhary, S.; Chandrasekaran, A.R.;
  Bussat, J.M.; Alvarez-Icaza, R.; Arthur, J.V.; Merolla, P.A.; Boahen, K.
\newblock Neurogrid: {A} {Mixed}-{Analog}-{Digital} {Multichip} {System} for
  {Large}-{Scale} {Neural} {Simulations}.
\newblock {\em Proc.  IEEE} {\bf 2014}, {\em 102},~699--716.
%\newblock Conference Name: Proceedings of the IEEE,
  {{https://doi.org/10.1109/JPROC.2014.2313565}}.

\bibitem[Neckar {et~al.}(2019)Neckar, Fok, Benjamin, Stewart, Oza, Voelker,
  Eliasmith, Manohar, and Boahen]{neckar_braindrop_2019}
Neckar, A.; Fok, S.; Benjamin, B.V.; Stewart, T.C.; Oza, N.N.; Voelker, A.R.;
  Eliasmith, C.; Manohar, R.; Boahen, K.
\newblock Braindrop: {A} mixed-signal neuromorphic architecture with a
  dynamical systems-based programming model.
\newblock {\em Proc.  IEEE} {\bf 2019}, {\em 107},~144--164.
\newblock {{https://doi.org/10.1109/JPROC.2018.2881432}}.

\bibitem[Schemmel {et~al.}(2010)Schemmel, Brüderle, Grübl, Hock, Meier,
  and Millner]{schemmel_wafer-scale_2010}
Schemmel, J.; Brüderle, D.; Grübl, A.; Hock, M.; Meier, K.; Millner, S.
\newblock A wafer-scale neuromorphic hardware system for large-scale neural
  modeling.
\newblock In Proceedings of the 2010 {IEEE} {International} {Symposium} on
  {Circuits} and {Systems} ({ISCAS}), \hl{Paris, France, 30 May--2 June} %MDPI: We added the location and date of the conference. Please confirm.
 2010; pp. 1947--1950.
{{https://doi.org/10.1109/ISCAS.2010.5536970}}.

\bibitem[Markram {et~al.}(2011)Markram, Meier, Lippert, Grillner,
  Frackowiak, Dehaene, Knoll, Sompolinsky, Verstreken, DeFelipe, Grant,
  Changeux, and Saria]{markram_introducing_2011}
Markram, H.; Meier, K.; Lippert, T.; Grillner, S.; Frackowiak, R.; Dehaene, S.;
  Knoll, A.; Sompolinsky, H.; Verstreken, K.; DeFelipe, J.;  et~al.
\newblock Introducing the {Human} {Brain} {Project}.
\newblock {\em Procedia Comput. Sci.} {\bf 2011}, {\em 7},~39--42.
\newblock {{https://doi.org/10.1016/j.procs.2011.12.015}}.

\bibitem[Farquhar {et~al.}(2006)Farquhar, Gordon, and
  Hasler]{farquhar_field_2006}
Farquhar, E.; Gordon, C.; Hasler, P.
\newblock A {Field} {Programmable} {Neural} {Array}.
\newblock In Proceedings of the 2006 {IEEE} {International} {Symposium} on
  {Circuits} and {Systems}, \hl{Kos, Greece, 21--24 May} %MDPI: We added the location and date of the conference. Please confirm.
 2006; pp. 4114--4117.
\newblock {{https://doi.org/10.1109/ISCAS.2006.1693534}}.

\bibitem[Liu {et~al.}(2009)Liu, Yu, and Wang]{cheng_fpaa_2009}
Liu, M.; Yu, H.; Wang, W.
\newblock {FPAA} {Based} on {Integration} of {CMOS} and {Nanojunction}
  {Devices} for {Neuromorphic} {Applications}. In {\em Nano-{Net}}; Cheng, M.,
  Ed.; Springer: Berlin/Heidelberg, Germany,  2009; Volume~3, pp.
  44--48.

\bibitem[Chan {et~al.}(2007)Chan, Liu, and van Schaik]{chan_aer_2007}
Chan, V.; Liu, S.C.; van Schaik, A.
\newblock {AER} {EAR}: {A} {Matched} {Silicon} {Cochlea} {Pair} {With}
  {Address} {Event} {Representation} {Interface}.
\newblock {\em IEEE Trans. Circuits Syst. I Regul. Pap.}
  {\bf 2007}, {\em 54},~48--59.
%\newblock Conference Name: IEEE Transactions on Circuits and Systems I: Regular
%  Papers,
  {{https://doi.org/10.1109/TCSI.2006.887979}}.

\bibitem[Haessig {et~al.}(2020)Haessig, Milde, Aceituno, Oubari, Knight, van
  Schaik, Benosman, and Indiveri]{haessig_event-based_2020}
Haessig, G.; Milde, M.B.; Aceituno, P.V.; Oubari, O.; Knight, J.C.; van Schaik,
  A.; Benosman, R.B.; Indiveri, G.
\newblock Event-{Based} {Computation} for {Touch} {Localization} {Based} on
  {Precise} {Spike} {Timing}.
\newblock {\em Front. Neurosci.} {\bf 2020}, {\em 14},~420.
\newblock {{https://doi.org/10.3389/fnins.2020.00420}}.

\bibitem[Lagorce {et~al.}(2017)Lagorce, Orchard, Galluppi, Shi, and
  Benosman]{lagorce_hots_2017}
Lagorce, X.; Orchard, G.; Galluppi, F.; Shi, B.E.; Benosman, R.B.
\newblock {HOTS}: {A} {Hierarchy} of {Event}-{Based} {Time}-{Surfaces} for
  {Pattern} {Recognition}.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}
  {\bf 2017}, {\em 39},~1346--1359.
\newblock {{https://doi.org/10.1109/TPAMI.2016.2574707}}.

\bibitem[Sironi {et~al.}(2018)Sironi, Brambilla, Bourdis, Lagorce, and
  Benosman]{sironi_hats_2018}
Sironi, A.; Brambilla, M.; Bourdis, N.; Lagorce, X.; Benosman, R.
\newblock {HATS}: {Histograms} of {Averaged} {Time} {Surfaces} for {Robust}
  {Event}-{Based} {Object} {Classification}.
\newblock In Proceedings of the 2018 {IEEE}/{CVF} {Conference} on {Computer}
  {Vision} and {Pattern} {Recognition}, Salt Lake City, UT, USA, \hl{18--23 June} %MDPI: We added the  date of the conference. Please confirm.
  2018;
  pp. 1731--1740.
\newblock {{https://doi.org/10.1109/CVPR.2018.00186}}.

\bibitem[Maro {et~al.}(2020)Maro, Ieng, and Benosman]{maro_event-based_2020}
Maro, J.M.; Ieng, S.H.; Benosman, R.
\newblock Event-{Based} {Gesture} {Recognition} {With} {Dynamic} {Background}
  {Suppression} {Using} {Smartphone} {Computational} {Capabilities}.
\newblock {\em Front. Neurosci.} {\bf 2020}, {\em 14}, 275.
%\newblock Publisher: Frontiers,
  {{https://doi.org/10.3389/fnins.2020.00275}}.

\bibitem[Grimaldi {et~al.}(2021)Grimaldi, Boutin, Perrinet, Ieng, and
  Benosman]{grimaldi_homeostatic_2021}
Grimaldi, A.; Boutin, V.; Perrinet, L.; Ieng, S.H.; Benosman, R.
\newblock A homeostatic gain control mechanism to improve event-driven object
  recognition.
\newblock In Proceedings of the 2021 {International} {Conference} on
  {Content}-{Based} {Multimedia} {Indexing} ({CBMI}),  \hl{Lille, France,  28--30 June} %MDPI: We added the location and date of the conference. Please confirm.
  2021.
\newblock {{https://doi.org/10.1109/cbmi50038.2021.9461901}}.

\bibitem[Grimaldi {et~al.}(2022)Grimaldi, Boutin, Ieng, Benosman, and
  Perrinet]{grimaldi_robust_2022}
Grimaldi, A.; Boutin, V.; Ieng, S.H.; Benosman, R.; Perrinet, L.U.
\newblock A robust event-driven approach to always-on object recognition.
\newblock {\em TechRxiv} {\bf 2022}.
\newblock {{https://doi.org/10.36227/techrxiv.18003077.v1}}.

\bibitem[Yu {et~al.}(2022)Yu, Gu, Li, Wang, Wang, and Li]{yu_stsc-snn_2022}
Yu, C.; Gu, Z.; Li, D.; Wang, G.; Wang, A.; Li, E.
\newblock {STSC}-{SNN}: {Spatio}-{Temporal} {Synaptic} {Connection} with
  {Temporal} {Convolution} and {Attention} for {Spiking} {Neural} {Networks}.  	\emph{arXiv}
  2022.
\newblock arXiv:2210.05241.

\bibitem[Benosman {et~al.}(2014)Benosman, Clercq, Lagorce, {Sio-Hoi Ieng},
  and Bartolozzi]{benosman_event-based_2014}
Benosman, R.; Clercq, C.; Lagorce, X.; Ieng, {S.-H}.; Bartolozzi, C.
\newblock Event-{Based} {Visual} {Flow}.
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.} {\bf
  2014}, {\em 25},~407--417.
\newblock {{https://doi.org/10.1109/tnnls.2013.2273537}}.

\bibitem[Clady {et~al.}(2014)Clady, Clercq, Ieng, Houseini, Randazzo,
  Natale, Bartolozzi, and Benosman]{clady_asynchronous_2014}
Clady, X.; Clercq, C.; Ieng, S.H.; Houseini, F.; Randazzo, M.; Natale, L.;
  Bartolozzi, C.; Benosman, R.B.
\newblock Asynchronous visual event-based time-to-contact.
\newblock {\em Front. Neurosci.} {\bf 2014}, {\em 8}, 9.
%\newblock 00000 Publisher: Frontiers,
  {{https://doi.org/10.3389/fnins.2014.00009}}.

\bibitem[Tschechne {et~al.}(2014)Tschechne, Sailer, and
  Neumann]{tschechne_bio-inspired_2014}
Tschechne, S.; Sailer, R.; Neumann, H.
\newblock Bio-{Inspired} {Optic} {Flow} from {Event}-{Based} {Neuromorphic}
  {Sensor} {Input}.
\newblock In \emph{Proceedings of the Artificial {Neural} {Networks} in {Pattern}
  {Recognition}}; El~Gayar, N.; Schwenker, F.; Suen, C., Eds.; Springer
  International Publishing: Cham, Switzerland, 2014; pp. 171--182.
\newblock {{https://doi.org/10.1007/978-3-319-11656-3\_16}}.

\bibitem[Hidalgo-Carrió {et~al.}(2020)Hidalgo-Carrió, Gehrig, and
  Scaramuzza]{hidalgo-carrio_learning_2020}
Hidalgo-Carrió, J.; Gehrig, D.; Scaramuzza, D.
\newblock Learning {Monocular} {Dense} {Depth} from {Events}.  \emph{arXiv}, {\bf 2020}, arXiv:2010.08350.
%\newblock {\em arXiv:2010.08350 [cs]} .
%\newblock arXiv: 2010.08350.

\bibitem[Dardelet {et~al.}(2021)Dardelet, Benosman, and
  Ieng]{dardelet_event-by-event_2021}
Dardelet, L.; Benosman, R.; Ieng, S.H.
\newblock An {Event}-by-{Event} {Feature} {Detection} and {Tracking}
  {Invariant} to {Motion} {Direction} and {Velocity}. \emph{TechRxiv} {\bf 2021}.
\newblock {{https://doi.org/10.36227/techrxiv.17013824.v1}}.

\bibitem[Stoffregen {et~al.}(2019)Stoffregen, Gallego, Drummond, Kleeman,
  and Scaramuzza]{stoffregen_event-based_2019}
Stoffregen, T.; Gallego, G.; Drummond, T.; Kleeman, L.; Scaramuzza, D.
\newblock Event-{Based} {Motion} {Segmentation} by {Motion} {Compensation}.
\newblock In Proceedings of the 2019 {IEEE}/{CVF} {International} {Conference}
  on {Computer} {Vision} ({ICCV}), \hl{Seoul, Republic of Korea, 27 February} %MDPI: We added the location and date of the conference. Please confirm.
 2019; pp. 7243--7252.
 {{https://doi.org/10.1109/ICCV.2019.00734}}.

\bibitem[Kim {et~al.}(2016)Kim, Leutenegger, and
  Davison]{kim_real-time_2016}
Kim, H.; Leutenegger, S.; Davison, A.J.
\newblock Real-{Time} {3D} {Reconstruction} and 6-{DoF} {Tracking} with an
  {Event} {Camera}.
\newblock In \emph{Proceedings of the Computer {Vision}---{ECCV} 2016}; Leibe, B.;
  Matas, J.; Sebe, N.; Welling, M., Eds.; Springer International Publishing:
  Cham,  Switzerland, 2016;  pp. 349--364.
\newblock {{https://doi.org/10.1007/978-3-319-46466-4\_21}}.

\bibitem[Hussain {et~al.}(2012)Hussain, Basu, Wang, and
  Hamilton]{hussain_deltron_2012}
Hussain, S.; Basu, A.; Wang, M.; Hamilton, T.J.
\newblock {DELTRON}: {Neuromorphic} architectures for delay based learning.
\newblock In Proceedings of the 2012 {IEEE} {Asia} {Pacific} {Conference} on
  {Circuits} and {Systems},  \hl{Kaohsiung, Taiwan, 2--5 December} %MDPI: We added the location and date of the conference. Please confirm.
 2012; pp. 304--307.
\newblock {{https://doi.org/10.1109/APCCAS.2012.6419032}}.

\bibitem[Wang {et~al.}(2015)Wang, Hamilton, Tapson, and van
  Schaik]{wang_neuromorphic_2015}
Wang, R.M.; Hamilton, T.J.; Tapson, J.C.; van Schaik, A.
\newblock A neuromorphic implementation of multiple spike-timing synaptic
  plasticity rules for large-scale neural networks.
\newblock {\em Front. Neurosci.} {\bf 2015}, {\em 9}, 180.
\newblock {{https://doi.org/10.3389/fnins.2015.00180}}.

\bibitem[Wang {et~al.}(2014)Wang, Hamilton, Tapson, and van
  Schaik]{wang_fpga_2014}
Wang, R.; Hamilton, T.J.; Tapson, J.; van Schaik, A.
\newblock An {FPGA} design framework for large-scale spiking neural networks.
\newblock In Proceedings of the 2014 {IEEE} {International} {Symposium} on
  {Circuits} and {Systems} ({ISCAS}), \hl{Melbourne, VIC, Australia, 1--5 June} %MDPI: We added the location and date of the conference. Please confirm.
 2014; pp. 457--460.
 {{https://doi.org/10.1109/ISCAS.2014.6865169}}.

\bibitem[Pfeil {et~al.}(2013)Pfeil, Scherzer, Schemmel, and
  Meier]{pfeil_neuromorphic_2013}
Pfeil, T.; Scherzer, A.C.; Schemmel, J.; Meier, K.
\newblock Neuromorphic learning towards nano second precision.
\newblock In Proceedings of the The 2013 {International} {Joint} {Conference}
  on {Neural} {Networks} ({IJCNN}), \hl{Dallas, TX, USA, 4--9 August} %MDPI: We added the location and date of the conference. Please confirm.
 2013; pp. 1--5.
 {{https://doi.org/10.1109/IJCNN.2013.6706828}}.

\bibitem[Boerlin and Denève(2011)]{boerlin_spike-based_2011}
Boerlin, M.; Denève, S.
\newblock Spike-{Based} {Population} {Coding} and {Working} {Memory}.
\newblock {\em PLoS Comput. Biol.} {\bf 2011}, {\em 7},~e1001080.
\newblock {{https://doi.org/10.1371/journal.pcbi.1001080}}.

\bibitem[Renner {et~al.}(2022)Renner, Sandamirskaya, Sommer, and
  Frady]{renner_sparse_2022}
Renner, A.; Sandamirskaya, Y.; Sommer, F.T.; Frady, E.P.
\newblock Sparse {Vector} {Binding} on {Spiking} {Neuromorphic} {Hardware}
  {Using} {Synaptic} {Delays}.
\newblock {\em Proceedings of the International Conference on Neuromorphic
  Systems};
%\newblock Conference Name: ICONS 2022: International Conference on Neuromorphic
%  Systems 2022 Meeting Name: ICONS 2022: International Conference on
%  Neuromorphic Systems 2022 Place:  
  Publisher: ACM Digital
  library: Knoxville, TN, USA, 2022. {{https://doi.org/10.1145/3546790.3546820}}.

\bibitem[Dard {et~al.}(2022)Dard, Leprince, Denis, Rao~Balappa, Suchkov,
  Boyce, Lopez, Giorgi-Kurz, Szwagier, Dumont, Rouault, Minlebaev, Baude,
  Cossart, and Picardo]{dard_rapid_2022}
Dard, R.F.; Leprince, E.; Denis, J.; Rao~Balappa, S.; Suchkov, D.; Boyce, R.;
  Lopez, C.; Giorgi-Kurz, M.; Szwagier, T.; Dumont, T.;  et~al.
\newblock The rapid developmental rise of somatic inhibition disengages
  hippocampal dynamics from self-motion.
\newblock {\em eLife} {\bf 2022}, {\em 11},~e78116.
\newblock {{https://doi.org/10.7554/eLife.78116}}.

\bibitem[Coull and Giersch(2022)]{coull_distinction_2022}
Coull, J.T.; Giersch, A.
\newblock The distinction between temporal order and duration processing, and
  implications for schizophrenia.
\newblock {\em Nat. Rev. Psychol.} {\bf 2022}, {\em 1},~257--271.
%\newblock Publisher: Nature Publishing Group,
  {{https://doi.org/10.1038/s44159-022-00038-y}}.

\bibitem[Panahi {et~al.}(2021)Panahi, Abrevaya, Gagnon-Audet, Voleti, Rish,
  and Dumas]{panahi_generative_2021}
Panahi, M.R.; Abrevaya, G.; Gagnon-Audet, J.C.; Voleti, V.; Rish, I.; Dumas, G.
\newblock Generative {Models} of {Brain} {Dynamics}---{A} review. \emph{arXiv} \textbf{2021},
\newblock arXiv:2112.12147.

\bibitem[Tolle {et~al.}(2011)Tolle, Tansley, and Hey]{tolle_fourth_2011}
Tolle, K.M.; Tansley, D.S.W.; Hey, A.J.G.
\newblock The {Fourth} {Paradigm}: {Data}-{Intensive} {Scientific} {Discovery}
  [{Point} of {View}].
\newblock {\em Proc.  IEEE} {\bf 2011}, {\em 99},~1334--1337.
\newblock {{https://doi.org/10.1109/jproc.2011.2155130}}.

\end{thebibliography}


%\begin{thebibliography}{00}
%\printbibliography[heading=none]
%\end{thebibliography}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author:~\citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a},~\citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages:~\citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328;~\citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)
\PublishersNote{}
\end{adjustwidth}
\end{document}



%
%The introduction should briefly place the study in a broad context and highlight why it is important. It should define the purpose of the work and its significance. The current state of the research field should be reviewed carefully and key publications cited. Please highlight controversial and diverging hypotheses when necessary. Finally, briefly mention the main aim of the work and highlight the principal conclusions. As far as possible, please keep the introduction comprehensible to scientists outside your particular field of research. Citing a journal paper~\citep{ref-journal}. Now citing a book reference~\citep{ref-book1,ref-book2} or other reference types~\citep{ref-unpublish,ref-communication,ref-proceeding}. Please use the command~\citep{ref-thesis,ref-url} for the following MDPI journals, which use author--date citation: Administrative Sciences, Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, Journal of Intelligence, Journalism and Media, JRFM, Languages, Laws, Religions, Risks, Social Sciences, Literature.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Materials and Methods}
%
%Materials and Methods should be described with sufficient details to allow others to replicate and build on published results. Please note that publication of your manuscript implicates that you must make all materials, data, computer code, and protocols associated with the publication available to readers. Please disclose at the submission stage any restrictions on the availability of materials or information. New methods and protocols should be described in detail while well-established methods can be briefly described and appropriately cited.
%
%Research manuscripts reporting large datasets that are deposited in a publicly avail-able database should specify where the data have been deposited and provide the relevant accession numbers. If the accession numbers have not yet been obtained at the time of submission, please state that they will be provided during review. They must be provided prior to publication.
%
%Interventionary studies involving animals or humans, and other studies require ethical approval must list the authority that provided approval and the corresponding ethical approval code.
%\begin{quote}
%This is an example of a quote.
%\end{quote}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Results}
%
%This section may be divided by subheadings. It should provide a concise and precise description of the experimental results, their interpretation as well as the experimental conclusions that can be drawn.
%\section{Subsection}
%\subsection{Subsubsection}
%
%Bulleted lists look like this:
%\begin{itemize}
%\item	First bullet;
%\item	Second bullet;
%\item	Third bullet.
%\end{itemize}
%
%Numbered lists can be added as follows:
%\begin{enumerate}
%\item	First item; 
%\item	Second item;
%\item	Third item.
%\end{enumerate}
%
%The text continues here. 
%
%\section{Figures, Tables and Schemes}
%
%All figures and tables should be cited in the main text as Figure~\ref{fig1}, Table~\ref{tab1}, Table~\ref{tab2}, etc.
%
%\begin{figure}[H]
%\includegraphics[width=10.5 cm]{Definitions/logo-mdpi}
%\caption{This is a figure. Schemes follow the same formatting. If there are multiple panels, they should be listed as: (\textbf{a}) Description of what is contained in the first panel. (\textbf{b}) Description of what is contained in the second panel. Figures should be placed in the main text near to the first time they are cited. A caption on a single line should be centered.\label{fig1}}
%\end{figure}   
%\unskip
%
%\begin{table}[H] 
%\caption{This is a table caption. Tables should be placed in the main text near to the first time they are~cited.\label{tab1}}
%\newcolumntype{C}{>{\centering\arraybackslash}X}
%\begin{tabularx}{\textwidth}{CCC}
%\toprule
%\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
%\midrule
%Entry 1		& Data			& Data\\
%Entry 2		& Data			& Data\\
%\bottomrule
%\end{tabularx}
%\end{table}
%\unskip
%
%\begin{table}[H]
%\caption{This is a wide table.\label{tab2}}
%	\begin{adjustwidth}{-\extralength}{0cm}
%		\newcolumntype{C}{>{\centering\arraybackslash}X}
%		\begin{tabularx}{\fulllength}{CCCC}
%			\toprule
%			\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}     & \textbf{Title 4}\\
%			\midrule
%			Entry 1		& Data			& Data			& Data\\
%			Entry 2		& Data			& Data			& Data \textsuperscript{1}\\
%			\bottomrule
%		\end{tabularx}
%	\end{adjustwidth}
%	\noindent{\footnotesize{\textsuperscript{1} This is a table footnote.}}
%\end{table}
%
%%\begin{listing}[H]
%%\caption{Title of the listing}
%%\rule{\columnwidth}{1pt}
%%\raggedright Text of the listing. In font size footnotesize, small, or normalsize. Preferred format: left aligned and single spaced. Preferred border format: top border line and bottom border line.
%%\rule{\columnwidth}{1pt}
%%\end{listing}
%
%Text.
%
%Text.
%
%\section{Formatting of Mathematical Components}
%
%This is the example 1 of equation:
%\begin{linenomath}
%\begin{equation}
%a = 1,
%\end{equation}
%\end{linenomath}
%the text following an equation need not be a new paragraph. Please punctuate equations as regular text.
%%% If the documentclass option "submit" is chosen, please insert a blank line before and after any math environment (equation and eqnarray environments). This ensures correct linenumbering. The blank line should be removed when the documentclass option is changed to "accept" because the text following an equation should not be a new paragraph.
%
%This is the example 2 of equation:
%\begin{adjustwidth}{-\extralength}{0cm}
%\begin{equation}
%a = b + c + d + e + f + g + h + i + j + k + l + m + n + o + p + q + r + s + t + u + v + w + x + y + z
%\end{equation}
%\end{adjustwidth}
%
%% Example of a page in landscape format (with table and table footnote).
%%\startlandscape
%%\begin{table}[H] %% Table in wide page
%%\caption{This is a very wide table.\label{tab3}}
%%	\begin{tabularx}{\textwidth}{CCCC}
%%		\toprule
%%		\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}	& \textbf{Title 4}\\
%%		\midrule
%%		Entry 1		& Data			& Data			& This cell has some longer content that runs over two lines.\\
%%		Entry 2		& Data			& Data			& Data\textsuperscript{1}\\
%%		\bottomrule
%%	\end{tabularx}
%%	\begin{adjustwidth}{+\extralength}{0cm}
%%		\noindent\footnotesize{\textsuperscript{1} This is a table footnote.}
%%	\end{adjustwidth}
%%\end{table}
%%\finishlandscape
%
%% Example of a figure that spans the whole page width. The same concept works for tables, too.
%\begin{figure}[H]
%\begin{adjustwidth}{-\extralength}{0cm}
%\centering
%\includegraphics[width=13.5cm]{Definitions/logo-mdpi}
%\end{adjustwidth}
%\caption{This is a wide figure.\label{fig2}}
%\end{figure}  
%
%Please punctuate equations as regular text. Theorem-type environments (including propositions, lemmas, corollaries etc.) can be formatted as follows:
%%% Example of a theorem:
%\begin{Theorem}
%Example text of a theorem.
%\end{Theorem}
%
%The text continues here. Proofs must be formatted as follows:
%
%%% Example of a proof:
%\begin{proof}[Proof of Theorem 1]
%Text of the proof. Note that the phrase ``of Theorem 1'' is optional if it is clear which theorem is being referred to.
%\end{proof}
%The text continues here.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Discussion}
%
%Authors should discuss the results and how they can be interpreted from the perspective of previous studies and of the working hypotheses. The findings and their implications should be discussed in the broadest context possible. Future research directions may also be highlighted.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Conclusions}
%
%This section is not mandatory, but can be added to the manuscript if the discussion is unusually long or complex.
